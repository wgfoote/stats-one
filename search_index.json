[["index.html", "Probabilistic Reasoning: from an elementary point of view Prologomena for a Future Statistics Why this book Premises So many questions and too little time Dont we know everything we need to know? What we desire Frequentist or probabilistic? A work in progress", " Probabilistic Reasoning: from an elementary point of view William G. Foote 2021-07-16 Prologomena for a Future Statistics This book is a compilation of many years of experience with replying to clients (CFO, CEO, business unit leader, project manager, etc., board member) and their exclamations and desire for the number. The number might be the next quarters earnings per share after a failed product launch, customer switching cost and churn with the entry of a new competitor, stock return volatility and attracting managerial talent, fuel switch timing in a fleet of power plants, fastest route to ship soy from Sao Paolo to Lianyungang, launch date for refractive surgery services, relocation for end state renal care facilities, and so on, and so much so forth. Here is highly redacted scene of a segment of a board meeting of a medical devices company in the not so distant past. All of the characters are of course fictions of a vivid imagination. The scene and the company are all too real. Sara, a board member, has the floor. Sara: Bill, just give us the number! Bill is the consultant that the CFO hired to provide some insight into a business issue: how many product units can be shipped in 45 days from a port on the Gulf of Mexico, USA to a port in Jiangsu Provice, PRC. Bill: Sara, the most plausible number is 42 units through the Canal in 30 days. However,  Sara interjects. Sara: Finally! We have a numbe  Harrumphing a bit, but quietly and thinking Not so fast!, Bill: Yes, 42 is plausible, but so are 36 and 45, in fact the odds are that these results, given the data and assumptions you provided our team, as well as your own beliefs about the possibilities in the first place, are even. You could bet on any of them and be consistent with the data. Jeri the CFO grins, and says to George the CEO in a loud enough whisper so that other board members perked up from their trances, Jeri, CFO: Thats why we hired Bill and his team. He is giving us the range of the plausible. Now its up to us to pull the trigger. Sara winces, yields the floor to Jeri, who gently explains to the board, Jeri: Thank you Bill and give our regards to your team. Excellent and extensive work in a very short time frame!. Bill leaves the board room, thinking Phew!, made through that gauntlet, again. Jeri: Now our work really begins. George, please lead us in our deliberations. Pan through the closed board room door outside to the waiting room and watch Bill, wiping the sweat from his brow, prepare the invoice on his laptop. A watchful administrative assistant nods his approval. Why this book This book is an analytical palimpsest which attempts to recast and reimagine the Bayesian Stan-based analytics presented by Richard (???) for the data analytics that help business managers and executives make decisions. Richard McElreaths vision and examples, though reworked, are nearly immediately evident throughout. But again this work is a palimpset with its own quirks, examples, but surely built on McElreaths analytical architecture. We wrangle the data and build the models through spreadsheets. Lets state this again: we will recast introductory portions of Richard McElreaths text with the aid of purely Microsoft Excel workbooks, aided in small part by calls to some Visual Basic for Applications (VBA) to automate simulations. On rare occasion we will use the Solver add-in, packaged with the Excel installation. The concepts, even the zeitgeist of a Danielle Navarro might be evident along with examples from her work on learning and reasoning in the behavioral sciences. And if there was ever a pool of behavior to dip into it is in the business domain. Premises The one premise of this book is that Learning is inference By inference we mean reaching a conclusion. Conclusions can be either true or false. They are reached by a process of reflection on what is understood and by what is experienced. Lets clarify these terms in this journey courtesy of Potter (1994): Ignorance is simply lack of knowledge. Opinion is a very tentative, if not also hesitant, subject to change, assent to a conclusion. Belief is a firm conviction. Knowledge is belief with sufficient evidence to justify assent. Doubt suspends assent when evidence is lacking or just too week. Learning can only occur with doubt or ignorance or even opinion; learning generates new knowledge. While the book is vividly, and outspokenly in this regard, Bayesian, I prefer the nomer probabalistic as evinced by Edwin T. Jaynes (2004), and (???), (???), and (???), before him, reaching back into the inverse probability tradition at least back to (???). We will not get into a debate about subjective versus objective yet: lets hold that (those) thought(s). There is a very strong and cogent reason for this tradition and it is the underlying heuristic structure of human knowing itself. Yes, philosophical principles provide guideposts for the operations, authenticity, products and methodology of probabilistic reasoning and the techne in this book and the works which ground this book. (???) explicates the heuristic structure of human knowing through three successive movements from experience, through understanding, onto to reflection about what is and is not (plausibly of course!), The terms of each movement wind their way into the methodology of any excursus that implements human knowing about anything, including the knower, whom we often tag the analyst or decision maker. The knower observes and experiences, yes, with a bias. The course will boil down to the statistics (minimum, maximum, mean, quantiles, deviations, skewness, kurtosis) and the probability that the evidence we have to support any proposition(s) we claim. The evidence is the strength (for example in decibels, base 10) of our hypothesis or claim. The measure of evidence is the measure of surprise and its complement informativeness of the data, current and underlying, inherent in the claim. In the interests of putting the bottom line up front , here is the formula for our measure of evidence \\(e\\) of a hypothesis \\(H\\) comes from Edwin T. Jaynes (2004). \\[ e(H|DX) = e(H|X) + 10\\,\\,log_{10}\\left[\\frac{P(D|HX)}{P({D|\\overline{H}X})} \\right] \\] Lets dissect this. First \\(H\\) is the claim, say, the typical null hypothesis \\(H_0\\) that the status quo is true. If \\(H=H_0\\) then \\(\\overline{H}\\) must be the alternative hypothesis logically so that \\(\\overline{H}=H_1\\). \\(D\\) is the data at hand we observe. \\(X\\) is the data, including data about beliefs, we already have at our disposable. Now we look at the combinations. \\(DX\\) is the logical conjunction of the two data sets. This conjunction represents the proposition that both the new data and the old data exist. \\(HX\\) is the phrase both the claim is true and the available data \\(X\\) are true. \\(\\overline{H}X\\) is the phrase both the claim is false and the available data \\(X\\) are true. Here are the conditions of the contract. We look for evidence \\(e(H|DX)\\) that \\(H\\) is true given the existence both of new data \\(D\\) and available data \\(X\\), that is \\(H|DX\\) where the | is the conditional binary operator. This compound (\\(DX\\)) evidence depends on the evidence \\(e(H|X)\\) that \\(H\\) is true given knowledge of available data \\(X\\). _ The evidence impounded in the odds of finding data \\(D\\) when the hypothesis is true \\(H\\) relative to when the hypothesis is not true \\(\\overline{H}\\). Everything is in \\(log_{10}\\) measures to allow us the luxury of simply adding up evidence scores. What is \\(H\\)? These are the hypotheses, ideas, explanations, parameters (like intercept and slope, means, standard deviations, you name it) we think when we work through answers to questions. So many questions and too little time Where in the business domain do questions arise? The usual suspects are the components of any organizations, stakeholders, value chain. This chain extends from questions about the who, when, where, how, how long, how much, how often, how many, for whom, for what, and why of each component. There are decisions in each link. In planning an organizations strategy the decisions might be to examine new markets or not, invest in assets or retire them, fund innovation with shareholder equity or stand fast. In each of these decisions are questions about the size and composition of markets and competitors, the generation of cash flow from operations, the capture of value for stakeholders. It is this font from which questions will arise and analysis will attempt to guide. Dont we know everything we need to know? We dont seem to know everything all of the time, although we certainly have many doubts and opinions. These hurdles are often managed through the collection of data against hypotheses, explanations, theories, and ideas. This syllogism and its sibling will guide us through a lot of the dark places we might travel in this book. ## Warning: package &#39;kableExtra&#39; was built under R version 4.0.3 Table 0.1: modus ponens plausibility major if A is true, then B is true minor B is true conclusion therefore, A becomes more plausible There is no guarantee here, just plausible, but justified, belief. We will call plausibility a measure of belief, also known as probability. Here is the sibling. Table 0.2: modus tollens plausibility major if A is true, then B is true minor A is false conclusion therefore, B becomes less plausible The first chapter will detail the inner workings and cognitional operations at work in these inferences. But let us remember that learning is inference. What we desire These two ways of inferring plausible, justified, true belief will be the bedrock of this book. They imply three desiderata of our methods: Include both the data of sensible observation and the data of assumptions and beliefs. Condition ideas, hypotheses, theories, explanations with the data of experience and belief. Measure the impact of data on hypotheses using a measure of plausibility. Herein we presents plausibility as the compound of a menagerie of conditional probabilities. A conditional probability is just a number between 0 and 1, that makes some sort of sense, given the truth of some ensemble of propositions. For example, we might observe lots of people in running gear running across a parking lot. We know, from experience that people running together in running gear often run in a race. The probability that the people we see in the parking lot, given they are running together in running gear, are running in a race is fairly plausible. It makes some sort of common sense. We might compute a probability of 0.25 as well. We just made a statement of conditional probability. Insofar as this is sensible, we might also say it is rational. What we will call rational will be the consistency, the compatibility, of data with hypotheses measured by plausibility, often labeled posterior probability. The data are runners in running gear. The hypothesis is they are running in a race. The data of beliefs, of maximum leakage of information, will be contained in what many call prior probability. We might find reasonable the hypothesis that people are running a race. We might also, for objectivitys sake, and to play the dialectical advocate, they are not. The two hypotheses are mutually exclusive logically. We might find them to be equally plausible, at least for the moment. The data of beliefs include our assumptions about the distribution of hypotheses. The conditioning of hypotheses with data is another conditional probability, what many will call likelihood. Ultimately what we call uncertainty will be the range and plausibility of the impact of data on hypotheses. What we will solve for is the most plausible explanation given data and belief. Frequentist or probabilistic? Both. The moving parts of our reasoning demand at least a probabilistic approach if we are to summarily, if not completely ever, deal with uncertainty in our descriptions and explanations of reality. For the business decision maker this mindset becomes critical as the way in which any decision must be made if it is to be compatible with the data of experience, understanding and reflection. Practically speaking the probabilistic approach directly aligns with the heuristic structure of human knowing: experience, understanding, reflection. All are fulfilled virtually (we dont know everything through bias, omission, ignorance, malice) as we enter the decision-making arena. Because our understanding of the future is imperfect we insert the word and idea of plausibility into every statement. Or perhaps its many synonyms, qualifiers and modalities. Frequentism is a quasi-objective approach to analysis. It is objective in that it focuses on only the data. It is quasi-objective in that the data must be collected by human knowers and doers as a sample of what might occur more generally. An assumption of the analysis is always that the data are equally likely to occur. These beings do not always have access to or even want to collect all or the right data for further deliberation. That is an interesting historical, and a priori, fact of human existence. The collection itself rails against the time, space, and resources needed for collection and thus the epithet of garbage-in and garbage-out plagues all analysis. However, frequentist approaches contradict their supposed objectivism in that both the data selected, collected, and deliberated up are subject (yes the prior substrate of experience begins to enter) to, and conditional upon the collector and the deliberator. Both frequentist and probabilistic reasoning intersect when prior knowledge is all but unknown (the uninformative or diffuse prior) or might as well assign equally plausible weights (probabilities) to any hypothesis the analyst might propose about the value of a moment of a distribution. They all but diverge when uneven, lumpy, step function priors on the values of the supposed estimators as hypotheses collide with the likelihood that the data is compatible with any of these hypotheses. Such divergence is not the destruction of objectivity, rather the inclusion into a complete objective description and explanation of a phenomenon when we allow the analysts assumptions, pre-existing conditions, the world as it has been known, into the deliberation of a posterior results.1 A work in progress This book will expand and contract over the days ahead. Email me will comments, errors, omissions, etc. The books content is rendered using Yihuis bookdown package (thank you!), served on GitHub pages with source code. When I figure out how to build an issues facility in Github that will be the font of all wisdom from readers and syndics. References "],["part-one-the-basics.html", "Part One  The Basics", " Part One  The Basics Learning how to count Probability for real people, and some animals "],["counting-the-ways.html", "Chapter 1 Counting the Ways 1.1 Plausibility, probability and information 1.2 Some Surprise 1.3 How many ways? 1.4 Back to data 1.5 Checking our grip on reality", " Chapter 1 Counting the Ways 1.1 Plausibility, probability and information According to Aristotle, if two claims are well-founded, their truth values can be ascertained. A plate is either green, or it is not. When thrown, the plate will land without a chip or it will break. If I claim that when I throw the plate, it will land with out a chip and you disagree, I can simply throw the plate to find out who was correct. One part of this is a mind game, a thought experiment a potential outcome. The other is a reality of actually throwing the plate and observing its status on landing. We will eventually call the mind-game a hypothesis and the reality a datum. It is in disagreement that logical deduction might (plausibly) break down. There is no guarantee that the plate will break, or, for that matter, that it will chip. We must simply experiment with plate(s), green, blue or otherwise, to support the claim (or not). These claims arise in everyday life. For example, despite my poor performance in plate throwing in the past, there is no cogent reason to believe that it is absolutely, positively false that the plate I throw would land without a chip. There is a degree of acceeptance, of plausibility, in one side of the claim, and on the other as well. Certainly it is not as false as the claim that \\(2+2=5\\) in base 10 arithmetic, or the patently spurious claim that true is false or my cat is a dog. Claims about things that are neither definitely true nor definitely false arise in matters both mundane and consequential: producing weather reports, catching the bus, predicting the outcomes of elections, interpreting experimental vaccine results, and betting on sports games, throwing the plate, to name just a few. So we would benefit from a method of comparing claims in these situations  which atmospheric model produces better predictions? What is the best source for predicting elections? Should I blow three times on my lucky dice, or is this all just a figment of my denial based imagination? 1.2 Some Surprise The goal, in all the cases above, is to guess about something that we dont or cant know directly, like the future, or the fundamental structure of the economy, or reasons why customer preferences change, on the basis of things we do know, like the present and the past, or the results of an existing or past experiment. Mostly we guess. Some us try to systematically consider and attempt to support with evidence the guess. Lacking precision, and sometimes even accuracy, we try to avoid bad surprises. Goods ones are often welcome. If I use the NOAA weather application on my smart phone it might not surprise me to see rain pelting down at 1630 this afternoon. After all the app indicated as much. In advance of the rain I brought in chair pads and anything else that might get ruined with rain. An airline pilot knows all the defects of her aircraft. That knowledge saves lives. Our mortal inferences, clever or dumb as they are, must have a surprise somewhere between totally expected, or zero surprises and thus certain 100% of the ways to make the statement, and totally surprising and 0% chance of anticipation. We will generally be making statements like: it will probably rain tomorrow, or nine times out of ten, the team with a better defense wins. This motivates us to express our surprise in terms of plausibility and we hanker for more precision with probability. 1.3 How many ways? Lets use a simple example. We have four voters in an upcoming election. They may be red or blue voters. Three of us go out and talk to three voters at random, that is, indiscriminately. One of us happens to come upon a blue voter, another of us, independently, happens to find a red voter, and the other separately finds a blue voter. This is the very definition of a random sample. Each of the finders does not know what the other is doing, all three do know that there are four voters out there and they happened to have independently talked to two blue and one red voter. How many red voters and how many blue voters are there? Here are all of the possible conjectures we can make for \\(blue = {\\color{blue}\\Box}\\) and \\(red = {\\color{red}\\Box}\\) voters. Table 1.1: voter conjectures 1 2 3 4 \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) Reading this we see that there are 4 voters and 5 different voter compositions ranging from all red to all blue. Our sample is 2 blue and 1 red voter, so we can very safely eliminate the first and fifth conjectures from our analysis, but for the moment just keep them for completeness sake. For each of the three remaining conjectures we may ask how many ways is the conjecture consistent with the collected data. For this task a tree is very helpful. Lets take the first realistic conjecture the \\({\\color{blue}\\Box}\\), \\({\\color{red}\\Box}\\), \\({\\color{red}\\Box}\\), \\({\\color{red}\\Box}\\) hypothesis and check if, when we sample all of the four voters, what are all of the ways this conjecture fans out. So here we go. We sampled a \\({\\color{blue}\\Box}\\) first. How many \\({\\color{blue}\\Box}\\)s are in this version of the composition of voters? only 1. We then sampled independently a \\({\\color{red}\\Box}\\). How many \\({\\color{red}\\Box}\\)s are in this conjecture? Quite a few, 3. Finally we sampled a \\({\\color{blue}\\Box}\\) at random. We know there is only one \\({\\color{blue}\\Box}\\) in this version of the truth. So, it is just counting the ways: 1 \\({\\color{blue}\\Box}\\) way \\(\\times\\) 3 \\({\\color{red}\\Box}\\) ways \\(\\times\\) 1 \\({\\color{blue}\\Box}\\) way = \\(1 \\times 3 \\times 1 = 3\\) ways altogether. When asked, many surmise that the 2 blue and 3 red conjecture is the right one. Are they right? Here is a table of the ways each conjecture pans out. We then in a separate column compute the contribution of each conjecture to the total number of ways across the conjectures, which is 3 + 8 + 9 = 20 ways. Also each of the conjecture propose a proportion \\(p\\) of the successes, that is, the blue voters in this context. Table 1.2: ways voter conjectures turn out 1 2 3 4 proportion ways plausibility \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) 0.00 0 x 4 x 0 = 0 0.00 \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) 0.25 1 x 3 x 1 = 3 0.15 \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) \\({\\color{red}\\Box}\\) 0.50 2 x 2 x 2 = 8 0.40 \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{red}\\Box}\\) 0.75 3 x 1 x 3 = 9 0.45 \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) \\({\\color{blue}\\Box}\\) 1.00 4 x 0 x 4 = 0 0.00 We cannot help but note that the proportion of ways for each conjecture can range from 0, perhaps to 1, since the proportions add up to 1. The number of ways also expresses the number of true consistencies of the data with the conjecture, an enumeration of the quality of the logical compatibility of conjectures with what we observe. We might now revise our commonsense surmise that 2 blue and 2 red is the better conjecture. However, if we use the criterion that the conjecture with the most ways consistent with the data is the best choice for a conjecture, then clearly here we would say that there are 3 blues and 1 red. Perhaps we have a better criterion that would choose our equinanimous choice of 2 blues and 2 reds? It does not appear to be so. Ways are the count, the frequencies of logical occurrence of a hypothesis given the data. The data includes the knowledge that there are possibly blues and reds, that there are 4 voters, and that we sampled 2 blues and 1 red. The relative frequency of the ways in which our conjectures are consistent with the data is what we will finally call probability. The plausibility is a measure between and including 0 and 1. The sum of all plausibilities is 1. The plausibility must be compatible with common sense. That last requirement, in turn, requires a sketch of why it is important. Lets suppose the plausibility of a proposition A (Jaynie is a runner, for example) is changed by another proposition B (Jaynie participated in a marathon). Lets mix this up with the plausibility that proposition C (Jaynie is a business analyst, for example) has nothing to do with proposition B. Then common sense (as opposed to common non-sense) dictates that the plausibility of A and C together (Jaynie is a business analyst runner) is also increased by proposition B. We have just quantified the plausibility of logical truth values. We have also found a very compelling criterion for the choice of a conjecture given the data and circumstances surrounding our inquiry. 1.4 Back to data The location and scale parameters \\(\\mu\\) and \\(\\sigma\\) (e.g., mean and standard deviation) we often calculate are not the mean and standard deviation of data \\(x\\). They are not properties of the physical representation of events called observed data. These measures use counts of events, measures of, perhaps physical properties like heat, size, time, but in the end they are abstractions and summarizations about the events. These parameters do carry information about the probability distribution of the representation of physical reality in data. To say \\(\\mu\\) is the mean of the data is to ascribe a minds eye idea to the physical reality, to invest in physical, objective reality, a property that only exists in the mind, but is in fact unobserved data. This is an example of the mind projection or reification fallacy much in vogue in the circles of fake, or better yet, chop logic. In the same way, probabilities only exist in our minds: there is no physical reality that is a probability, just a mental construct that helps us think through potential outcomes. Is information just such a fiction? 1.5 Checking our grip on reality Suppose there are 5 voters of unknown affiliation. What is the probability that 60% are red affiliates? So one of 5 moved out and we are back to 4 voters. What is the most plausible proportion of blue voters if we sample the district and find 2 blue and 3 red voters? "],["probability-for-real-people.html", "Chapter 2 Probability for Real People 2.1 Can we rationally reason? 2.2 Whats next? 2.3 Try this out, if this is reasonable 2.4 Endnotes", " Chapter 2 Probability for Real People 2.1 Can we rationally reason? Some might wonder if this section header even makes sense!2 We reason a lot. But sometimes our reasons are not founded in any data that can be observed either by ourselves or by others. Our reasoning can be founded on falsified data, delusions, unfounded opinions, beliefs with no authoritative grounds. Rationality here at least means that we, as decision makers, would tend to act based on the consistency of observed reality with imagined and through ideas about the world in which data are collected. We attempt to infer claims about the world based on our beliefs about the world. When confronting ideas, imbued with beliefs, with ovbserved reality we might find ourselves in the position to update our beliefs, even those, and sometimes especially those, we so dearly hold. In our thinking about anything we would venture candidate hypotheses \\(h\\) about the world, say the world of voters in voting districts, consumers of smart phones in zip codes, even virus testing results. Of course the whole point is that we do not know which hypothesis is more plausible, or not. We then collect some data \\(d\\). When we perform this task, we move from the mental realm of the possibiity of hyptheses, theories, surmises, and model to the realm of observed reality. We may well have to revise our original beliefs about the data. To implement our maintained hypothesis of rationality, we begin our search for potential consistencies of the collected data with our hypotheses that are fed by the data. In our quest we might find that some one of the hypotheses has more ways of being consistent with the data than others. When the data is consistent with a hypothesis, that is, when the hypothesis is reasonable logically, then our belief in that hypothesis strengthens,3 and becomes more plausible. If the data is less consistent with the hypothesis, our belief in that hypothesis weakens. So far we have performed this set of tasks with conjectures about virus testing and voter alliance in zip codes. Lets switch up our program and consider the following very simplified question about the weather. We see people carrying snow shovels. Will it snow? What is the data \\(d\\)? We have recorded a simple observation about the state of the weather so that single piece of data (\\(d =\\) We see people carrying snow shovels). Her is where our beliefs enter. We have two hypotheses, \\(h\\): either it snows today or it does not. Lets figure out how to solve this problem? We have three desiderata: We should include our experiences with snow in our analysis. We should collect data about carrying snow shovels in January as well. We prefer more consistency of data with hypotheses to less consistency. Here we go, lets strap ourselves in. 2.1.1 Priors: what we think might happen Our observation is about the weather: clouds, wind, cold. But we want to know about the snow! That is our objective and we have definite ideas about whether (dont pardon the pun!) it will snow or not. We will identify our beliefs, ever before we make our observations, about snow. The analytical profession and custom is to label these beliefs as a priori,4 and thus the ellipsis prior, contentions we hold when we walk into the data story we create with the question of will it snow? Our prior contentions are just the plausibilities of each hypothesis whatever data we eventually collect. After all we have to admit to everyone what we believe to be true as the antecedent to the consequent of observations and the plausibility of snow. This move allows us to learn, to revise, to update our dearly held beliefs. We thus can grow and develop. This is in a phrase a sine qua non, a categorial imperative, a virtually unconditioned requirement for change. What might we believe about whether it will snow (today)? If you come from Malone, New York, north of the Adirondack mountains, you will have a different belief than if you come from Daytona, Florida, on the matter of how many ways snow might happen in a given month. So lets take as our benchmark Albany, the capital of the state of New York. We will refer to some data to form hypotheses and their plausibility.using this weather statistics site. The site reports the average number of days of snowfall in January, when there is at least a 0.25 cm accumulation in a day. It is 10.3 days. These are the number of ways (days) in January, in Albany, NY, that it is true, on average and thus some notion of expected, or believed to be, that it snows. The total number of ways snow could possibly fall in any January (defined by calendar standards) is 31. While the formation of the hypotheses snowy and nice days is informed by data, we are asking a question about snow because we have yet to observe if will snow. We cannot observe something that has not yet happened. We can thus characterize hypotheses and conjectures as unobserved data. Thus we might conclude that we believe that is it plausible (probable) that snow can fall \\(10.3 / 31 = 30\\%\\) of the different ways snow can fall. Note very well we will talk about priors as potentials and conjectures and hypotheticals, and thus used the modal verbs can or might. Thus we believe it might not snow, because it is possible, with plausibility \\(1-0.30 = 0.70\\), or, multiplying by 100, 70%, according to the law of total probability of all supposed (hypothesized) events. We only have two such events: snow and not snow. Probabilities must, by definition, add up to 1 and must, again by definition be a number between 0 and 1. Table 2.1: Priors by hypotheses hypotheses priors snowy day 0.3 nice day 0.7 Nice ideas, nice beliefs, are our as yet to be observed, but projected notions of a snowy day. But how real, how plausible, how rational, that is, how consistent are they with any observed data? Is there any observed data we can use to help us project which of our unobserved data, our hypotheses, is more or less reasonable? 2.1.2 Likelihoods: thinking about the data Life in the Northeast United States in January much revolves around the number of snow days, also known as days off from school. A prediction of snow meets with overtime for snow plow drivers, school shut downs, kids at home when they normally are in school. On some snowy days we see people carrying snow shovels, on others we dont. On some nice days we see people with snow shovels, on others we dont. Confusing? Confounding? A bit. Now we link our observations of shovels with our unobserved, but through about and hypothesized, prediction of snow. We then suppose we observe that people carry snow shovels about 7 of the 10 snowy days in January or about 70%. On nice days we observe that people carry shovels at most 2 days in the 21 nice days or about 10%. This table records our thinking using data we observe in Januaries about weather conditions. Table 2.2: data meets hypotheses hypotheses shovels hands snow day 0.7 0.3 nice day 0.1 0.9 First of all these probabilities register yet another set of beliefs, this time about whether we see shovels or not, given, conditioned by, the truth of each hypothesis \\(h\\). We write the conditional probability \\(\\operatorname{Pr}(d|h)\\), which you can read as the probability of \\(d\\) given \\(h\\). Also here we will follow the convention that this set of results of our assessment of the relationship of shovels to snowy days as a likelihood .5 2.1.3 Altogether now Do we have everything to fulfill our desiderata? Lets check where we are now. We should include our experiences with snow in our analysis. Yes! We put our best beliefs forward. We even (sometimes this is a courageous analytical step) quantified teh ways in which snow and not snow would occur, we believe, in Albany NY in an average January.6 We should collect data about carrying snow shovels in January as well. Yes we did! Again we elicited yet another opinion, belief, whatever we want to colloguially call it. That belief if what we register and docuement based on observation of shovels and just hands in the presence of snowy and nice days in a January. We prefer more consistency of data with hypotheses to less consistency. Not yet! We will impose our definition of rationality here. Lets start out with one of the rules of probability theory. The rule in question is the one that talks about the probability that two things are true. In our example, we will calculate the probability that today is snowy (i.e., hypothesis \\(h\\) is true) and people carry shovels (i.e., data \\(d\\) is observed). The joint probability of the hypothesis and the data is written \\(\\operatorname{Pr}(d,h)-\\operatorname{Pr}(d \\wedge h\\), and you can calculate it by multiplying the prior \\(\\operatorname{Pr}(h)\\) by the likelihood \\(\\operatorname{Pr}(d|h)\\). The conjunction is a both-and statement. We express conjunctions using the wedge \\(\\wedge\\) symbol. Logically, when the statement that both \\(d\\) and \\(h\\) is true, then the plausibility, now grown into probability is: \\[ \\operatorname{Pr}(d \\wedge h) = \\operatorname{Pr}(d|h) \\operatorname{Pr}(h) \\] When we divide both sides by \\(\\operatorname{Pr}(h)\\) we get the definition, some say derivation, of condition probability. If we count \\(#()\\) the ways \\(d \\wedge h\\) are true and the ways that \\(h\\) are true then \\[ \\#(d|h) = \\frac{\\#(d \\wedge h)}{\\#(h)} \\] Then the number of ways the data \\(d\\) are true, given \\(h\\) is true, equals the total number of ways that \\(d\\) and \\(h\\) per each way that \\(h\\) is true. We have thus normed our approach to understanding a conditional statement like if \\(h\\), then \\(d\\). Even more so, when we combine the law of conditional probability with the law of total probability we get Bayes Theorem. This allows us to recognize the dialectical principle that, yes, we recognize \\(h = snowy\\), but we also know that every cloud has its silver lining and that there is a non-snowy day and thus a \\[ not\\,\\,h = \\lnot h = nice \\] lurking in our analysis. Here it in in all its glory. \\[ \\begin{align} \\operatorname{Pr}(h\\mid d) &amp;= \\frac{\\operatorname{Pr}(d\\mid h)\\,\\operatorname{Pr}(h)}{\\operatorname{Pr}(d\\mid h)\\,\\operatorname{Pr}(h)+\\operatorname{Pr}(d\\mid \\lnot h)\\,\\operatorname{Pr}(\\lnot h)} \\\\ &amp;= = \\frac{\\operatorname{Pr}(d \\wedge h)}{\\operatorname{Pr}(d\\mid h)\\,\\operatorname{Pr}(h)+\\operatorname{Pr}(d\\mid \\lnot h)\\,\\operatorname{Pr}(\\lnot h)} \\end{align} \\] The numerator is the same as the conjunction both \\(d\\) and $h. The denominator is the probability that either both \\(d\\) and \\(h\\) or both \\(d\\) and \\(h\\) are true. While the build up to this point is both instructive, and thus may at first be confusing, it is useful as it will highlight the roles these probabilities perform in the drama that is our analysis. We had better get back to the data or get lost in the weeds of the maths. So, what is the probability it is true that today is a snowy day and we observed people to bring a shovel? Lets see what we already have. Our prior tells us that the probability of a snowy day in any January is about 30%. Thus \\(\\operatorname{Pr}(h) = 0.30\\). The probability that we observe people carrying shovels is true given it is a snowy day is 70%. So the probability that both of these things are true is calculated by multiplying the two to get 0.21. We can make this \\[ \\begin{array}{l} \\operatorname{Pr}(snowy,\\,shovels) &amp; = &amp; \\operatorname{Pr}(shovels \\, | \\, snowy) \\times \\operatorname{Pr}( snowy ) \\\\ &amp; = &amp; 0.70 \\times 0.30 \\\\ &amp; = &amp; 0.21 \\end{array} \\] This is an interesting result, something odds makers intuitively know when punters put skin in the game. There will be a 21% chance of a snowy day when we see shovels in peoples hands. However, there are of course four possible pairings of hypotheses and data that could happen. We then repeatthis calculation for all four possibilities. We then have the following table. Table 2.3: both data and hypotheses hypotheses shovels hands sum snow day 0.21 0.09 0.3 nice day 0.07 0.63 0.7 sum 0.28 0.72 1.0 Just to put this into perspective, we have for the 31 days in a January this table. Table 2.4: both data and hypotheses in days in January hypotheses shovels hands sum snowy day 6.5 2.8 9.3 nice day 2.2 19.5 21.7 sum 8.7 22.3 31.0 We have four logical possibilities for the interaction of observed data and unobserved hypotheses. We arrange these possibilities in two stacked rows. We recall that visualizatiton is everything, even in tables! Here is the first row. Snowy and shovels \\[ \\begin{array}{l} \\operatorname{Pr}(snowy,\\,shovels) &amp; = &amp; \\operatorname{Pr}(shovels \\, | \\, snowy) \\times \\operatorname{Pr}( snowy ) \\\\ &amp; = &amp; 0.70 \\times 0.30 \\\\ &amp; = &amp; 0.21 \\end{array} \\] Snowy and just hands \\[ \\begin{array}{l} \\operatorname{Pr}(snowy,\\,hands) &amp; = &amp; \\operatorname{Pr}(hands \\, | \\, snowy) \\times \\operatorname{Pr}( snowy ) \\\\ &amp; = &amp; 0.30 \\times 0.30 \\\\ &amp; = &amp; 0.09 \\end{array} \\] In this row the prior probability about snow is 0.30. Here is the second row. Nice and shovels \\[ \\begin{array}{l} \\operatorname{Pr}(nice,\\,shovels) &amp; = &amp; \\operatorname{Pr}(shovels \\, | \\, nice) \\times \\operatorname{Pr}( nice ) \\\\ &amp; = &amp; 0.10 \\times 0.70 \\\\ &amp; = &amp; 0.07 \\end{array} \\] Nice and just hands \\[ \\begin{array}{l} \\operatorname{Pr}(nice,\\,hands) &amp; = &amp; \\operatorname{Pr}(hands \\, | \\, nice) \\times \\operatorname{Pr}( nice ) \\\\ &amp; = &amp; 0.90 \\times 0.70 \\\\ &amp; = &amp; 0.63 \\end{array} \\] In this row the prior probability about nice days is 0.70. A great exercise is to carry these calculations from the number of ways snow with and without shovels occurs given we think we know something about snow. The same with the number of ways a nice day might occur with and without shovels, given what we think about nice days. Lets put one calculatin together with a not so surprising requirement. When we conjoin snow with shovels, how many possible ways can these logical statements occur? It is just the 31 days. We now have all of the derived information to carry our investigation further. We also total the rows and, of course, the columns. We will see why very soon. The row sums just tell us as a check that we got all of the ways in which snow occurs in 31 days. What is brand new are the column sums. They add up the ways that data occurs across the two ways we hypothesize that data can occur: snow, no snow (nice day). They tell us the probability of carrying a shovel or not, across the two hypotheses. Another way of thinking about the $p(d)4 column sums is that they are the expectation of finding snow or hands in the data. The consistency of all of these calculations is that column sums equal row sums, 100%. All regular, all present and correct, probability-wise. 2.1.4 Updating beliefs The table lays out each of the four logically possible combinations of data and hypotheses. So what happens to our beliefs when they confront data? In the problem, we are told that we really see shovels, just like the picture from Albany, NY at the turn of the 20th century. Is surprising? Not necessarily in Albany and in January, so you might expect this behavior out of habit during a rough Winter. The point is that whatever our beliefs have been about shovel behavior, we should still subject them to the possibility of accomodating the fact of seeing shovels in hands in Albany in January, a winter month in the Northern Hemisphere. We should recall this formula about the probability of seeing both an hypothesis and data: \\[ \\operatorname{Pr}(h \\mid d) = \\frac{\\operatorname{Pr}(d \\wedge h)}{\\operatorname{Pr}(d)}=\\frac{\\operatorname{Pr}(d \\mid h) \\operatorname{Pr}(h)}{\\operatorname{Pr}(d)} \\] Now we can trawl through about our intuitions and some arithmetic. We worked out that the joint probability of both snowy day and shovel is 21%, a rate reasonable given the circumstances. In our formula, this is the product of the likelihood \\(\\operatorname{Pr}(d=shovels \\mid h=snow)=0.70\\) and the prior probability we registered that snow might occur \\(\\operatorname{Pr}(h=snow)=0.30\\). Relative to the product of the likelihood of shovels given a nice day and the chance that snow might occur is the the joint probability of both nice day and shovel at 10%, or \\(\\operatorname{Pr}(d=shovels \\mid h = nice)\\operatorname{Pr}(h=nice)=0.10\\times 0.70=0.07\\), again a reasonable idea, since we plausibly wouldnt see much shovel handling on that nice day in January.. Both of these estimates are consistent with actually seeing shovels in peoples hands. But what are the chances of just seeing shovels at all? This is an either or question. We see shovels 21% of the time on snowy days or we see shovels 7% of the total days in January on nice days. We then add them up to get 28% of the time we see shovels in all of January, whether it snows or not. So back to the question: if we do see shovels in the hands of those folk, will it snow? The hypothesis is \\(h=snow\\) and the data is \\(d=shovels\\). The joint probability of both snow and shovels is \\(\\operatorname{Pr}(d, h)=0.21\\). But just focusing on the data we just observed, namely that we see shovels, we now know that the chances of seeing shovels on any day in January in Albany, NY is \\(\\operatorname{Pr}(d)=0.27\\). Out of all of the ways that shovels can be seen in January then we would anticipate that the probability of snow, upon seeing shovels, must be \\(\\operatorname{Pr}(h \\mid d)=\\operatorname{Pr}(d,h)/\\operatorname{Pr}(d)=0.21/0.28=0.75\\). What is the chance of a nice day given we see shovels? It would be again likelihood times prior or \\(0.10\\times0.7=0.07\\) divided by the probability of seeing shovels any day in January 28%. We then calculate \\(0.07/0.28=0.25\\). We now have the posterior distribution of the two hypotheses, snow or nice, in the face of data, shovels. So what are the odds in favor of snow when we see shovels? \\[ \\begin{align} OR(h \\mid d) &amp;=\\frac{\\operatorname{Pr}(h=snow \\mid d=shovels)}{\\operatorname{Pr}(h=nice \\mid d=shovels)} \\\\ &amp;=\\frac{0.75}{0.25} \\\\ &amp;=3 \\end{align} \\] We can read this as: when we see people with shovels in January in Albany, NY, then it is 3 times more plausible to have a snowy day than a nice day. The ratio of two posteriors gives us some notion of the plausible divergence in likely outcomes of snowy versus nice days. Again we must append the circumstances of time and place: in a January and in Albany, NY. Here is table that summarizes all of our work to date. Table 2.5: unobserved belief tempered by observed data = posteriors hypotheses shovels hands priors posterior shovels posterior hands snow day 0.7 0.3 0.3 0.75 0.13 nice day 0.1 0.9 0.7 0.25 0.88 sum 0.8 0.2 1.0 1.00 1.00 2.2 Whats next? We have travelled through the complete model of probabilistic reasoning. We started with a question. The question at least bifurcates into the dialectical is it? or is it not?. We then began to think about beliefs inherent in the question for each of the hypotheses buried in the question. We then collected data that is relevant to attempting an answer to the question relative to each hypothesis. Then we conditioned the data with the hypotheses inside the question. It is always about the question! Finally we derived plausible answers to the question. What is next? We continue to use this recurring scheme of heuristic thinking, sometimes using algorithms to count more efficiently, applied to questions of ever greater comnplexity. In the end our goal will be to learn, and learning is inference. 2.3 Try this out, if this is reasonable Start with a question for analysis using a indicative-interrogative statement format, for example We observe X. Will Y occur? Based on this statement identify the unobserved data of the hypothesis and the observed data. Use binary hypotheses and observations as we did in the section above. Rework the Albany NY example using your hometown or city. Develop initial distribution of hypotheses, distributions of data given a hypothesis, joint distributions of hypotheses and data. Find the probability that a particular hypothesis might occur given a specific piece of data. 2.4 Endnotes References "],["part-two-the-fantastic-four.html", "Part Two  The Fantastic Four", " Part Two  The Fantastic Four Counting on Steroids Binomial stuff Poisson raptors Gaussian robots "],["algorithmics-1-counting-made-easy.html", "Chapter 3 Algorithmics 1: counting made easy 3.1 Whats an algorithm? 3.2 Our first job: unobserved hypotheses 3.3 Possibilities abound 3.4 Observed data 3.5 Is anything really plausible? 3.6 Interpretation 3.7 10 locales? 3.8 Next 3.9 References and endnotes", " Chapter 3 Algorithmics 1: counting made easy 3.1 Whats an algorithm? An algorithm is a set of rules used to perform ordered tasks to achieve a desired result. It is precise, effective in that it produces what it advertises it will do, and sometimes, also efficient in the number of steps to achieve a desired result. we often contrast the notion of an algorithm with a heuristic. A heuristic structure attempts to anticipate that which is unknown. It might use algorithms, rules of thumb, received traditions, all to help get to a solution. In computing parlance and recent practice, this description of a heuristic structure deprecates to a convenient approximation to a solution, even if the result is not optimal.7 We will start by unpeeling the structure of a specific context: a decision making process for a disaster relief organization whose co-workers deliver services in locales known to have disease. We search for the most plausible answer to the question will our co-workers be safe if exposed to pathogens? In the next section We use this working example of our organization which provides disaster relief services on location. 3.2 Our first job: unobserved hypotheses We currently deliver aid to victims in four locales. Severe weather compromises water supplies, sewage containment, communications, transportation, healthcare, food and energy. We have obligations to keep co-workers reasonably safe and healthy as they deliver their services. One protocol we follow is frequent testing for pathogens in the blood and respiratory systems of each co-worker. If co-workers test positive for known pathogens in a locale, affected co-workers will be removed to a safe site for appropriate care, and a further decision will be forthcoming for continued deployment to the locale. We see co-workers who are exposed to known pathogens. Will they test negative for disease and be safe?8 In our heuristic process, before we begin to construct an algorithm, we begin thinking about the question first. Will they remain healthy? At this stage we consider only what is possible for testing in the four locales. We will consider only a strict positive or negative test forIn considering this question for co-workers across four locales the answer to the question provisionally is binary: yes co-workers in a locale test positive, or, no they do not. We will eventually color code a positive test with red and a negative test with green. 3.3 Possibilities abound In this thought experiment we imagine that all four locales have co-workers that might test positive or they might test negative. We do not expect anything yet, just express hope that all are negative. These are two conjectures, considerations, theories even, we will call hypotheses. How many ways can four locales test positive and / or negative? A discussion might reveal that is is possible that one locale tests positive while the other three are negative. Or again, that two test positive while the other two are negative. And, again, that three test positive, while only one tests negative. At the boundaries of our consideration we have the possibility that either none or all of the locales will test positive. What have we arrived at? There are 5 hypotheses across 4 locales. We visualize the 5 hypotheses here in an Excel worksheet. We index each hypothesis with integers from 1 to 5. We also notice that with 4 locales we have \\(4+1=5\\) hypotheses. We notice that hypothesis 1 conjectures 0 green tests; hypothesis 2 with 1 green test (3 red), and so on. We can now calculate the number of green and red tests for each conjecture. Start withe 0 green at hypothesis 1. Then the number of reds is \\(locales - green = 4 - 0 = 0\\). For hypothesis 2 the number of greens is simply increased by 1 so is the hypothesis 1 0 greens plus 1, which is just 1. The reds are still \\(locales - green = 4 - 1 = 3\\). Simple counting. We could even simplify this further by also noticing the number of greens is just \\(hypothesis - 1\\). Thus for hypothesis 4 we have \\(hypothesis - 1 = 4 - 1 = 3\\) greens. We arrive at a simpler, easier to compute way to get at the number of greens and reds for each hypothesis built of the number of locales plus 1 to get the hypotheses. We then take the hypothesis and reduce by 1 to obtain the number of greens. Now that we determined the number of greens and reds for each hypothesis, we can also calculate the proportion of greens, a statistic, a metric, that can also represent the particular hypothesis. That is what the next panel shows. Excel formula calculations appear below each row. We also notice that the green and red calculations run in opposite directions. We now hypothesize that proportions of observed data in the population of the 4 locales can range from 0.00 to 1.00 in steps of 0.25. This is a grid of hypotheses. 3.4 Observed data We spent quite a bit of quality time with hypothesis formation. This is a subject we can and should consider very carefully. First, we framed a question informed by exposures to a pathogen and in the context of a policy. Second, we included all the possible hypotheses. Third, we quantified each and arrived at a measurable metric. We still do not which hypothesis, which proportion, represents the 4 locales. We turn to that task next. We start with observed data. We label the hypotheses as unobserved data. Everything is fair game: its data. We take three samples from any locale. The samples dont care what locale they come from. They are samples of tests pure and simple without locality attribution. This is our little world. We observe 2 green tests (negatives) and 1 red test (positive). If we let only the observed data talk, we say that 0.67 of the tests are negative, green. But is this representative of the 4 locales? Not at all necessarily. The observed data has no intrinsic relationship with the fact that there are locales. Next we allow the data to interact with each hypothesis. This table details that conversation. We add two columns, K and L. we ask the question of how many ways can a green occur when we consider a hypothesis. Lets look at the calculation for hypothesis 4. There are 3 green ways in that hypothesis that matter when we observe a green in the sample. But there are 2 greens in the sample. This is a both-and joint arrangement. Both we hypothesize 3 green ways and we observe 2 greens. For each observation we have 3 green ways in hypothesis 4. For both observations there are \\(3 \\times 3 = 3^2 = 9\\) ways. Both for one observation of a red and only 1 way left in hypothesis 4 reserved for a red there are \\(1^1 = 1\\) way for this observation to be consistent with this hypothesis. 3.5 Is anything really plausible? Just asking this question seems to say yes. It is our final algorithmic task to deduce plausibility of the unobserved data of each hypothesis in the face of observed data. This panel reveals the finale. In this last scene, we calculate for each unobserved hypothesis the number of ways it is consistent with the observed data. We already calculated the number of consistent ways for the two greens and one red. Now we strap them together in another both-and logical calculation in column M. We simply multiply columns K and L together. Again we lean on hypothesis 4 for help. There are 9 ways that 2 greens are consistent with a hypothesized proportion of 0.75 greens in a population of greens and reds. Both 9 green-consistent ways and 1 red-consistent way yield 9 jointly consistent ways that the sample and the hypothesis are logically compatible with one another. We calculate the ways that each unobserved hypothesis are consistent with all of the data sampled. Now we calculate the contribution of the ways obvserved data are consistent with unobserved hypotheses. First we add up all of those ways. There are 20 total consistent ways. We calculate the fraction each hypothesis is consistent out of the total 20 ways. 3.6 Interpretation Each of these fractions is between 0 and 1. The fractions add up to 1. They can now also be called probabilities. We have just deduced the probability of an unobserved hypothesis about the proportion of greens given a sample of observed data, \\(Pr( h \\mid d)\\). For example \\(Pr( h = 3 \\,greens, \\,\\, 1\\, red \\mid d = 2 \\,greens, \\,\\, 1 \\,red ) = 0.75\\), the highest rated hypothesis. We would therefore report that, given this data, hypothesis 4 is the most probably. 3.7 10 locales? Suppose we needed to sample from 10 locales? Does that mean more hypotheses possible? Heres the view from the grid. Eventually we run out of rows or columns or, good grief, patience. But the ideas around our algorithm, as inspired as they are by this grid, remain the same. We still have to only count the possible number of greens and reds for each possible, and exhaustive list of unobserved hyptheses. We still have to sample co-workers in locales, as many as we have time and money to do so. We still must determine deductively the hypothesis with the highest plausibility, credibility, consistency with the observed data. Now let us sample 12 locales. Here is the grid approximation to the probability of hypotheses given data. We lost the green and red staircase. But look at those numbers of ways! The analysis boils down to a most likely hypothesis 8 (out of 11, one more than the number of locales) with a green proportion of 0.70 in the population of 10 locales. The unwieldliness of these large ways numbers will soon outrun our computerss ability to count. We will need to fix the algorithm very soon. We should notice the use of two named ranges: locales and sum. This begins to represent prevailing recommended practice for the construction of spreadsheet models. Added here as well are two important notes about the probability results. The first is the interpretation of the \\(Pr( h \\mid d )\\) as a relative frequency, that is, the count of ways in column I relative to the sum of ways across all unobserved hypotheses. The second is the addition of column J to store our calculation of the cumulative relative frequency. This calculates the relative frequency increment to the previous accumulation of the relative frequency. It is the area under the relative frequency curve. We interpret this area as the probability of all hypotheses \\(h\\) less than a possible proportion \\(p\\) occurring. A plot will help with the tortured description of the concepts. Here is the Excel setup for the plot. Prevailing plotting practices recommend (strongly!) to store plot titles for the main chart and axes in editable cells. We can follow these directions to create dynamic chart titles including axis titles for primary and secondary y-axes. The abline connects two points in x-y coordinates. The line visualizes the location of the maximum \\(Pr(h \\mid d)\\) point on the relative frequency and cumulative relative frequency curves and the identification of the most plausible hypothesis 0.80 derived from 8 of the 10 locales. Cell O7 uses the excel =MAX() function to retrieve the maximum \\(Pr(h \\mid d)\\) from vector J4:J14. In cell O8 the =MATCH() function locates the row number of the \\(max \\, Pr(h \\mid d)\\) in the vector J4:J14. The INDEX() retrieves the value of the cell in the \\(p\\) vector F4:F14 at the row located with the =MATCH() function. With 10 locales we must expand the number of unobserved potential hypotheses that the population of locales might occur. With this expansion comes the finer delineation of hypothesized proportions of the population. While the proportions of the population are continuous real numbers, the discrete nature of the hypotheses in terms of 10 distinct locales maps directly to a discrete set of possible proportions. 3.8 Next Algorithmics 2 will expand this model to its rightful place in applied probability (also known as statistics). This place is called the binomial distribution. The binomial distribution will serve as our observational model. We will reduce the simple counting model of this note to a much more robust distribution. 3.9 References and endnotes We define a heuristic notion as the concept of an as-yet-unknown content or solution, something to be discovered (Gr. heurisko). A heuristic structure is then an ordered set of heuristic notions, according to Lonergan (1970, p.392), that is, a method, insofar as structure is some sort of order. As such it is the procedure or method, not the content of the procedure or method. A solution for a heuristic structure can derive from approximations according to a criterion. Algorithms provide a quantification of some heuristics. Algorithms then form a subset of heuristic structures as anticipations of as-yet-not-known solutions or content. This is one of two possible question frames. We could ask the opposite question. Will they test positive, not be safe, and contract disease? When we form hypotheses there is indeed a framing bias we should be aware of. The frame might bias selections that favor what the question implicitly looks for, here a positive or a negative test. It is the positive or negative connotation that creates this cognitive bias. "],["algorithmics-2-binomial-ups-and-downs.html", "Chapter 4 Algorithmics 2: binomial ups and downs 4.1 Anatomy of an algorithm 4.2 Ups and downs 4.3 Dispensing with the bag of beans 4.4 Great expectations 4.5 Then there were eleven 4.6 References and endnotes", " Chapter 4 Algorithmics 2: binomial ups and downs 4.1 Anatomy of an algorithm The anatomy of our algorithm for counting the ways is an example of a heuristic structure. We recall that such a structure attempts to anticipate that which is unknown. It houses the algorithm to help get to a solution, that is, the one hypothesis which is most consistent with the sampled and observed data. The scaffolding of the algorithm takes on othe role of a visualization, and metaphor, for an algorithm.9 Here is a visualization we can populate with the data flows, logic, and computations in the algorithm. First, and foremost, we conceive, think about, conjecture whether (coded a 1) or not (coded a 0) in a context an observation might (again a 1) or might not (again a 0) occur in a context, a population. We generate hypotheses (\\(h\\)) in the leftward red initiation of the structure. Within this sub-structure we also compute (impute if an argument and not a computation) an unobserved datum, \\(p\\), the proportion of potential observations that might occur. Having thought through all of the competing hypotheses, we move to the second, the blue sub-structure. There we observe, we sample 1s and 0s. These two numbers affirm as true (coded a 1) or do not affirm (0) the sampled existence of an observation. We count the number of 1s samples, the number of 0s samples. Then, and this is crucial, we ask how many ways can the number of observed 1s and 0s can possibly occur for any given hypothesis. We gather all of the ways. We perform a cross-cut of unobserved potential hypotheses with the ways observed data can occur according to a specific hypothesis. In the green sub-structure we then deduce the fractional contribution of the ways data is consistent with each hypothesis relative to the total number of ways the data can occur with respect to all hypotheses. We then search for the unobserved and hypothesized data, the hypothesis itself, which is most consistent with the observed sampled data. We label green sub-structure we label as the probability \\(Pr\\) that hypotheses \\(h\\) represent the population being observed, given (using the vinculum |) the observed, sampled, data \\(d\\). 4.2 Ups and downs Lets focus on one hypothesis, say, \\(p=0.25\\), the possibility that the population is 0.25 ups and thus 1-0.25 or 0.75 downs. Here is a hand-drawn view of this hypothesis for a sample of 3 ups and downs. Lots of activity can appear in graphs (nodes and edges). We start at one end, with zero samples, and move from left to right to 1 sample, which can be either up or down. As we move to two samples we start from the one sample nodes and then go up and down twice. We move to three samples starting with the two sample nodes and again go up or down. We end up with 4 nodes in the end with 3 sampled ups and downs. We have effectively mashed together all of the possible ways samples of size 3 can interact with this one hypothesis. The data are a sample of three: 2 ups and 1 down. We ask: how many ways can two ups and one down occur? Here is another hand-rendering of the hypothesis tree to answer this question. Either the left hand path can occur or the right hand path. Here \\(x=2\\) are the number of ups, \\(n=3\\) is the sample size, and \\(n-x=1\\) is the number of downs. It turns out that the probability of one of these paths, that is, that both 2 ups (blue) and 1 down (red) can occur is the product of the probabilities of 2 ups and the 1 down. \\[ \\begin{align} 0.25 \\times 0.25 \\times (1-0.25) &amp;= (0.25)^2 (0.75)^1 \\\\ &amp;= 0.046875 \\\\ &amp;\\approx 0.047 \\end{align} \\] We use this short-hand formula for one path to good use. \\[ p^x (1-p)^{n-x} = 0.047 \\] The tree instruct us that either 2 ups followed by 1 down can occur along 3 different paths. The probability that either \\(A\\) or \\(B\\) or \\(C\\) occurs is the sum of the probabilities, when the events are independent from one another, as they are here. The \\(\\vee\\) stands for the or. \\[ Pr( A \\vee B \\vee C) = Pr(A) + Pr(B) + Pr(C) \\] So we can now see that a path with probability \\(0.047\\) can happen any one of 3 ways. In this way we can write \\[ \\begin{align} Pr(up = 2 \\vee down = 1 \\mid p = 0.25) &amp;= 0.047 + 0.047 + 0.047 \\\\ &amp;= 3 ( 0.047) \\\\ &amp;= 0.141 \\end{align} \\] Another way to compute the number of paths so many ups and downs can take is to realize that these are combinations. The formula for finding the number of ways, without regard for the order in which they might occur of \\(n\\) items taken \\(x\\) at a time is \\[ _{n}C_{x} = {n \\choose x} = \\frac{n!}{x! \\, (n - x)!} \\] \\[ \\begin{align} _{5}C_{3} = {3 \\choose 2} &amp;= \\frac{3!}{2! \\, (3 - 2)!} \\\\ &amp;= \\frac{3 \\times 2 \\times 1}{(2 \\times 1)\\times 1} \\\\ &amp;= 3 \\end{align} \\] Splicing this formula together with the formula for the probability of a single path yields the probability that \\(x\\) ups occurs in \\(n\\) moves given that the probability of an up is \\(p\\). \\[ Pr(x \\mid n, p) = \\frac{n!}{x! \\, (n - x)!}p^x (1-p)^{n-x} \\] With our toy example we have this computation. \\[ \\begin{align} Pr(x = 2 \\mid n=3, p=0.25) &amp;= \\frac{3!}{2! \\, (3 - 2)!}(0.25)^2 (1-0.25)^{3-2} \\\\ &amp;= (3)(0.047) \\\\ &amp;= 0.141 \\end{align} \\] Lets get more precise. We have these conditions. Two possible event outcomes only in each run or scenario or branch on a path \\(p\\) = probability of an up event \\(n\\) = number of times outcomes might occur \\(x\\) = number of up events We use three assumptions: Each replication of the process is a combination of events that results in one of two possible outcomes (usually referred to as success\" or failure events). The probability of success is the same for each replication. The replications are independent, meaning here that a success in one replication does not influence the probability of success in another. Assumption three reveals itself as nodes in the hypothesis tree combining into one another. With all of these assumptions and definitions, we now have the binomial distribution probability that \\(x\\) up outcomes occur in \\(n\\) trials with \\(p\\) probability of an up outcome. This is a lot easier to compute when \\(n\\) gets much larger than 3. We did lose the green and red staircase for this example. But we also do not have to rely on the bag of beans approach to modeling. The unwieldliness of these large ways numbers would soon outrun at every larger values of \\(n\\) our computerss ability to count. The binomial algorithm, the formula we just miracled into existence, will solve the large number of ways problem. 4.3 Dispensing with the bag of beans We just reduced the counting model to a binomial probability calculation. Two are equivalent. We get the same probabilities both ways. We now add to our sophistication by allowing hypotheses to have different levels of influence, and recast the ways which data and hypotheses are consistent with a probabilistic definition. Here is a screenshot of an Excel rendering of the new, and improved, binomial model. We rid ourselves of the bag of beans and now realize that only the values of the hypothesized \\(p\\) proportions, now as probabilities of a single up event, characterize any hypothesis in this model. We generate the \\(p\\)-grid in column D by specifying the maximum value 1, the minimum value 0, and the number of intervals in the 11 node grid all computed in column K. We use the Excel built-in function =BINOM.DIST() in column F to calculate probabilities of seeing data \\(d\\) conditional on a hypothesis \\(h\\). The parameters of this function mirror the probability mass function derived above. We check the model by computing the value for \\(Pr( d:\\,up = 2,\\,n=3 \\mid h:\\,p=0.25 )=0.141\\) with the binomial distribution formula in cell F12. We have replaced the really large count the ways numbers with decimals. 4.4 Great expectations Indeed, we can use this model to aggregate our expectations of a binomial outcome. Those outcomes appear on a hypothesis by hypothesis basis. If we believe our model that \\(h_4:\\,p=0.75\\) is the model which is the most plausible, most consistent with the data, then we can model potential outcomes with the binomial model. The mathematical trick we use is to calculate expected value of outcomes as the weighted average of potential up outcomes. In the same way we can also compute the weighted average of squared deviations of up outcomes from the expected value. Then we can scale this squared-dimensioned variance by taking the square root to get the standard deviation of outcomes. Here is a set of fairly useful computations on our 5-node workhorse grid in Excel with formulae. We have computed all of the binomial quantities for the single hypothesis \\(p=0.25\\) and data \\(n=3\\) samples across all of the discrete possibilities for the number of ups and downs that we might see in the field. For each up outcome we calculate the contribution of that outcome, that deviation from expectation squared outcome, to get the expected outcome. We then gather them altogether by summing them up. These aggregations help us summarize the distribution we have worked so hard to compute and understand. There is a new kid on this block, skewness. This expectation can be positive or negative. One interpretation is that it measures the degree to which (weighted) average deviations favor conditions below or above the expected outcome. If negative we see lots more deviations below; if positive lots more above. We leave it to a context to understand this concretely and practically. By the way a zero skew is a symmetric distribution. We notice that we take the cubed deviations and scale the weighted average by the cube of the standard deviation, very clever indeed. We also notice that we piggyback the computation by using the weighted squared deviation, more cleverness than we can stand! Here is a graph of three binomial hypotheses. Which of these is negatively skewed, positively skewed. no skew at all? 4.5 Then there were eleven The usefulness of the approach to be rid of the bag of beans shows itself in expanding the grid to 11 nodes. Here is the result. Nothing has changed but for the granularity of the approximation of the resulting \\(Pr(h \\mid d)\\). In the language of the contingency table: \\(Pr(h)\\) is the marginal probability of a hypothesis, also known, and confusingly, the prior probability. The only thing that is prior is that it is a given, an assumption of the analysis. \\(Pr(d)\\) is the sum in the table and is the marginal probability of data, the ups in this case. \\(Pr(d \\mid h)Pr(h) = Pr(h \\wedge d)\\) is the joint probability both of \\(h\\) and \\(d\\), also known as, and confusingly, likelihood. We can try to add up the likelihoods. And if we do, they will not add up to 1, but they will all be between 0 and 1. Even so, we still cannot call these quantities probabilities. \\(Pr(h \\mid d) = Pr(d \\mid h)Pr(h)/Pr(d) = Pr(h \\wedge d)/Pr(d)\\) is the conditional probability of \\(h\\) given all of the \\(d\\), also confusingly known as the posterior probability. The only thing that is posterior is that it arrives after all of the hypothetico-deductive work we conducted to get us there. So much for nomenclature. We will stick with conditional probabilities, probabilities, joint probabilities, multiplication and addition and division. This will help us when we consider non-binary count and continuous data. Of course we must show the resulting plot and setup for the plot and calculation tables above. Here are the set up tables. The first table sets up the 11-node grid. The next table sets up the data needed for the presentation graphic. Is the distribution of the conditional probability of hypotheses given data positively or negatively skewed, or not at all and thus symmetric? 4.6 References and endnotes We define a heuristic notion as the concept of an as-yet-unknown content or solution, something to be discovered (Gr. heurisko). A heuristic structure is then an ordered set of heuristic notions, according to Lonergan (1970, p.392), that is, a method, insofar as structure is some sort of order. As such it is the procedure or method, not the content of the procedure or method. A solution for a heuristic structure can derive from approximations according to a criterion. Algorithms provide a quantification of some heuristics. Algorithms then form a subset of heuristic structures as anticipations of as-yet-not-known solutions or content. "],["algorithmics-3-playing-musical-raptors.html", "Chapter 5 Algorithmics 3: playing musical raptors 5.1 Is there more to life than binary? 5.2 Exploring what we do know 5.3 Whence the binomial generates the Poisson 5.4 Approximating Poisson 5.5 Zooming in for a closer look 5.6 Probability intervals 5.7 References and endnotes", " Chapter 5 Algorithmics 3: playing musical raptors 5.1 Is there more to life than binary? Yes indeed! Lets motivate ourselves with an example. We suppose that we observe raptors (birds we usually call hawks) off the Heldeberg Escarpment (423921N 740109W) southwest of Albany, NY in the John Boyd Thacher State Park. The raptor census helps us understand the health of the local environment. More raptors means more prey, and more ecological support for all concerned, including clean water and air. During 5 consecutive days we observe individual sightings or not. We want a record of the timestamp when we dont observe raptors as well, at least for time checks. Here is a sample of one days observations. When we sum the count column we calculate 13 raptor sightings. There are some curious aspects to this data collection effort. The non-sightings, coded as 0s, seem to occur at nearly random times during the day. Also the 11th hour is missing. Why? The observer did not record all the non-sightings because of a directive from her supervisor, just some time checks during the day. At 10:35AM the supervisor directed the observer to help the team set up a new observation blind. The observer did not return until 12:05PM. Were there raptors to be sighted during the 11th hour? We will never know. What about the other minutes without mention of a sighting? Logic should dictate that those minutes would receive a 0 code since we might assume that the 1s are reliably counted. And that is just the data collection phase! The 11th hour absence-with-leave would constitute an instance of measurement error. We just reduced binary 1s and 0s to the field of integers. We have a broad wing (BW) raptor counts for each of seven days along with the number of observational hours, cloud cover, and wind conditions. The hours are integer as are the BW counts. The weather conditions are binary data. We pose this question. In weekly January intervals, what number of raptors can we expect to see? First, and foremost, we conceive, think about, conjecture whether we might see anywhere from nearly 0 to nearly 100 raptors per day according to this very sparse data set. Our next step will describe the data. 5.2 Exploring what we do know Common sense would dictate that seeing 85 raptors does not seem very likely. Is it an outlier? Perhaps it lies outsider of a normal range of raptor sightings. But what is a normal range? We have not yet discussed a criterion to identify what is or is not an outlier. An operational definition ought at least to be based on observed data, experience with raptors and the local environment, and notions of rare, outlying, events. Also at the least a description and identification of an outlier should provide a strong enough contrast with those sightings that are not outliers. As always common sense will help us. We then set up a range of measures to describe the raptor data. Here is a panel of metrics that relate to the location of a tendency of the data to accumulate around a value, as well as the scale or range of the data that might deviate from a tendency, a trend, an expectation. We could also describe two shape metrics: one for skewness to measure preponderance of positive or negative deviations from trend; another for the thickness of the tail of the distribution of sightings. An important analytical task is to map the time series of sightings into a frequency distribution of the sightings. 5.2.1 Summarize the data We can aggregate the data into useful summaries that cover: Location is the tendency of the data to a central, or perhaps, another non-central value. Examples are the mean, median (50%tile), and mode (most frequently occurring datum). But we should not forget that other quantiles (also known as percentiles) such as the 25th and 75th, might be interesting locations to visit. Location metrics are described extensively here. **Scale* is the width of the data. The most obvious width is the distance in data dimensions (counts of birds here) from minimum to maximum known as the range. Other average distances include the standard deviation, mean absolute deviation, and interquartile range. Scale metrics are described extensively here. Shape measures the direction of the mass of frequencies, also known as skew, as well as the thickness of the tails of a distribution. Skew looks at the preponderance of negative and positive deviations. It does so by addup the cube of deviations. Thus a positive deviation will cube to a positive number, a negative deviation will cube to negative score. We raise deviations to a fourth power to calculate kurtosis, the thickness of the tails. This is the same as squaring the square and thus we can interpret kurtosis as akin to the standard deviation of the standard deviation. Shape metrics are described extensively here. Now we go to the calculation table. The first rule of spreadsheet engineering practice is to name ranges. Here we look at the vector of data named BW. We use the indirect(named_range) function to access values of the named range throughout this installment of Algorithmics. Mean and standard deviation (sd) are the usual suspects. Included also in the average absolute value of deviations of data from the mean, known as the Mean Absolute Deviation (mad). Skewness, kurtosis, and the count of data round out the summary. Are there any outliers? Tukeys fences can help us answer this question. As we can see in the cell formulae for C10 and C11, there is are lower (C10) and upper (C11) bounds or thresholds defined by 1.5 times the Interquartile Range (IQR). The IQR is a scale metric which is the difference between the 25th and 75th percentile. This range represents 50% of the data. We look for 1.5 times the IQT below the 25th percentile and 1.5 times the IQR above the 75th percentile for outlying data. We will discover in the next Algorithmics installment how to interpret the choice of the 1.5 threshold. So, no there are no outliers below the 25th percentile threshold as there are no counts of raptors that are negative, and by definition, the minimum count is far greater than the lower threshold. But there are outliers above the 75th percentile threshold as the max is greater than the upper threshold. Certainly values of greater than the threshold and less then or equal to the maximum number of sightings. We will use values from this table to begin to set out a grid to approximate \\(Pr(h \\mid d)\\). So what are our hypotheses? They will derive from a distribution that can use the integer sightings data we generated. This distribution is the Poisson distribution to which we now turn our rapt attention. 5.3 Whence the binomial generates the Poisson Suppose we have 20 raptor sightings per 9 hour observational day. This means 20/9 = 2.22 raptors let their presence be known to us per hour. If we model a success as a sighting we can calculate the probability of a sighting by hour using the binomial process. This means that most of the hours get zero sightings but some hours will get exactly 1 visit. However we observe that in some hours, and maybe minutes too, there are more than 1 visitor. The problem with the binomial approach is that it can not envisage more than one event in any unit of time or space. But suppose we then divide 1 hour into 60 minutes, and thus make the time unit smaller, say, the 60 minutes in an hour? Now 1 hour can contain multiple sightings. But our problem remains: any one minute will contain exactly one or no sightings. This gives us a clue as to how we might solve the problem of calculating the probability of multiple sightings in an hour. Furthermore it is possible that there are more than one sighting in even a minute. So then we divide the minute further into 60 seconds to begin to account for this set of events. It is even possible that more than one raptor sightings at the escarpment in a second, and so on. We divide the unit of time into smaller and smaller intervals. We can coerce the binomial process to handle multiple events by dividing a unit time into smaller units. By using smaller time intervals, we can make the original unit time contain more than one event. Mathematically, this means that \\(n\\), the number of trials, experiments, or observations approaches a very large number: \\(n \\rightarrow \\infty\\). But the number of events in the binomial process is \\(np\\), so as \\(n\\) gets big, then for any positive probability of a single success \\(p\\), \\(np\\) will explode. To handle this we let \\(p \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). Since we assume the rate of sightings per hour is fixed because otherwise, \\(np\\), which is the number of events, will blow up. Using the limit, the unit times are now infinitesimal. We no longer have to worry about more than one event occurring within the same unit time. And this is how we can derive the Poisson distribution. In the binomial distribution, the number of trials \\(n\\) should be known before hand. If you use the binomial, you cannot calculate the success probability only with the rate (i.e. 17 raptors/day, or hour, or week). You need more information: both \\(n\\) and \\(p\\). The Poisson Distribution, on the other hand, doesnt require us to know \\(n\\) or \\(p\\). We are assuming \\(n\\) is very large relative to \\(p\\). The Poisson distribution uses the intensity rate \\(\\lambda\\) which collapses \\(np\\) into a single parameter. In practical applications we usually only know this intensity rate. Lets start with the binomial probability distribution with \\(X\\) the population of sightings and \\(x\\) one instance of the number of sightings sampled from the population. \\[ P(X = x) = {n \\choose x} p^x (1-p)^{n-x} \\] Next we exploit the definition of \\(\\lambda = np\\) to calculate \\(p\\). \\[ p = \\frac{\\lambda}{n} \\] We insert this rendition of \\(p\\) into the binomial formula, and let \\(n\\) get ever larger, so that in the limit we have \\[ P(X = x) = lim_{n \\rightarrow \\infty} {n \\choose x} \\left(\\frac{\\lambda}{n}\\right)^x \\left(1-\\left(\\frac{\\lambda}{n}\\right)\\right)^{n-x} \\] Splitting up the \\((1-p)^{n-x}\\) term we see that \\[ P(X = x) = lim_{n \\rightarrow \\infty} {n \\choose x} \\left(\\frac{\\lambda}{n}\\right)^x \\left(1-\\left(\\frac{\\lambda}{n}\\right)\\right)^{n} \\left(1-\\left(\\frac{\\lambda}{n}\\right)\\right)^{-x} \\] It turns out that we have this amazing simplication. \\[ lim_{n \\rightarrow \\infty} \\left(1-\\left(\\frac{\\lambda}{n}\\right)\\right)^{n} = e^{-\\lambda} \\] Embedded here is the very definition of \\(e^{-\\lambda}\\) embedded in Jacob Bernoullis discovery of the transcendental number \\(e\\). As \\(p = \\lambda/n\\) gets very small with \\(n\\) getting very large for relatively increasingly very small \\(x\\), then this term approaches 1. \\[ lim_{n \\rightarrow \\infty}\\left(1-\\left(\\frac{\\lambda}{n}\\right)\\right)^{-x} = 1 \\] Finally, we can use this notion to think through the idea that \\({n \\choose x}\\) also approaches 1. \\[ \\begin{align} {n \\choose x}\\left(\\frac{\\lambda}{n}\\right)^x &amp;= \\frac{n!}{(n-x)! \\, x!}\\left(\\frac{\\lambda}{n}\\right)^x \\\\ &amp;= \\frac{n!}{(n-x)! \\, x!}\\left(\\frac{\\lambda^x}{n^x}\\right) \\\\ &amp;= \\frac{n!}{(n-x)! \\, n^x}\\left(\\frac{\\lambda^x}{x!}\\right) \\end{align} \\] But, \\[ \\begin{align} lim_{n \\rightarrow \\infty} {n \\choose x} &amp;= lim_{n \\rightarrow \\infty} \\frac{n!}{(n-x)! \\, \\, n^x} \\\\ &amp;= lim_{n \\rightarrow \\infty} \\frac {n(n-1)(n-2) \\dots (n-x+1)}{nn \\dots n} \\\\ &amp;= 1 \\end{align} \\] Again, we can use an amazing simplification. There are \\(x\\) terms in both the numerator and denominator. The numerator is ever only slightly smaller than the denominator when \\(n\\) is very large and the ratio of the two terms will converge eventually to equal each other since \\(n\\) is collosally greater than \\(x\\). Finally, when we assemble it all together what is left is the Poisson probability. We illustrate with \\(x=21\\) and \\(\\lambda=18.60\\). \\[ \\begin{align} Pr(X = x \\mid \\lambda) &amp;= e^{-\\lambda}\\left(\\frac{\\lambda^x}{x!}\\right) \\\\ &amp;= e^{-18.60}\\left(\\frac{18.60^{21}}{21!}\\right) \\\\ &amp;= 0.0747 \\end{align} \\] The first term in the exponent \\(e\\) is the probability that none of the \\(n\\) samples of data occur at rate \\(\\lambda\\), that is, \\(Pr(X=0 \\mid \\lambda)\\). The second term are the number of ways the probability that \\(x\\) sightings occur at rate \\(\\lambda\\). We have found our hypotheses finally. The only unobserved data in this probability statement reside in \\(\\lambda\\). 5.4 Approximating Poisson We use all of the algorithmic practice learned in building a binomial model to generate \\(Pr( h=\\lambda \\mid d=integer \\,\\, counts)\\). First we build a grid of \\(\\lambda\\)s. We start with the minimum and maximum from our study of the data and decide on how many nodes and thus intervals (nodes minus one). The first node is the minimum. The next nodes are incremented by the interval width. The last node should check out to be the maximum of the grid generator. We use equally likely \\(Pr(h)\\) for each hypothesized \\(\\lambda\\). Now we compute the probabilities of seeing each and everyone of the separate sightings counts in I9:K9 given each and every hypothesis for \\(\\lambda\\) in C10:C20. We of course use the hard fought for Poisson probability formula. We have a table of conditional Poisson probabilities to take in. The transpose (column for row) of the BW named range feeds each column of Poisson probabilities, one sighting datum = \\(x_i\\) for \\(i=1 \\cdots 7\\) in the sample, and one \\(\\lambda_j\\) for \\(j=1 to 11\\) hypotheses. So we compute in cell E10, holding the row absolute for the data and the column absolute for the hypothesis, the following formula. We illustrate with \\(x_4=21\\) and \\(\\lambda_3 = 18.60\\). \\[ \\begin{align} Pr(X = x_4 \\mid \\lambda_3) &amp;= e^{-\\lambda_4}\\left(\\frac{\\lambda_3^{x_1}}{x_4!}\\right) \\\\ &amp;= e^{-18.60}\\left(\\frac{18.60^{21}}{21!}\\right) \\\\ &amp;= 0.0747 \\end{align} \\] Now we have all of the calculations we need to feed the calculation of the conditional probability \\(Pr(h | d)\\). The goal continues to be to depict, using sampled, observed data, the uncertainty associated with possible answers to the question: what is the intensity of raptor sightings? We highlight the example computation of a conditional data probability. Our focus now is the relative frequency column. For each hypothesized \\(\\lambda_j\\) we next aggregate all of the Poisson conditional probabilities for all of the observed data \\(x_i\\). Since we observe both \\(x_1=13\\) and \\(x_2=3\\) and so on to \\(x_7=85\\) we calculate the conditional probability that we observe all of the data by multiplying the individual conditional data probabilities in column L. This number is often very small! In column M we multiply the conditional data probability \\(Pr(d \\mid h)\\) by the probability that the hypothesis \\(Pr(h)\\) occurs. The result is the numerator of Bayes Theorem. We then sum up the eleven \\(Pr(d \\mid h)Pr(h)\\) to yield the denominator \\(Pr(d)\\) in Bayes Theorem in cell M8. Column N yields the fruit of our labor. We calculate the ratio of \\(Pr(h \\mid d )Pr(h)\\) to \\(Pr(d)\\) to finally get \\(Pr(h \\mid d)\\). The maximum probability of a hypothesis given data is 0.97 and this finding points to \\(\\lambda=26.90\\) expected raptor sightings per day. 5.5 Zooming in for a closer look We notice that there is a high concentration of plausibility around \\(\\lambda=26.90\\). We can zoom into the range of \\(\\lambda\\)s around 26.90 to investigate the uncertainty associated with our best point estimate. If we set the maximum \\(\\lambda=45\\) and the minimum \\(\\lambda= 13\\) we get this new set of calculations. A plot will be more helpful. We see a strong peak at about 28. We need to remember that this is yet another grid approximation to the development of hypothesized \\(\\lambda\\) and their conditional probabilities. Nearly all of the probability around 28 ranges from 23 to 33 sightings per day, that is, \\(28 \\pm 5\\) sightings. 5.6 Probability intervals The cumulative relative frequency, also known more affectionately as the ogive curve, provides our entree to the world of making useful probability statements about the uncertainty of our hypotheses. Here is an example of a useful interrogative statement. We know there is error in measuring raptor sightings. What range of raptor sightings might we expect 99% of the time? This statement is useful because we have a concern about measurement error. It might also be useful if the consumer of the analysis requires an answer. Let us look at a range of hypothetical average sightings about the average sighting that is most plausible. Here is a concentrated range of hypotheses from a minimum of 13 per day to a maximum of 45 per day on a 11 node grid. We shade the range about the most plausible aaverge sightings perday \\(\\lambda=29.00\\) at a highest plausibility \\(Pr( h \\mid d)=0.6967\\). we search for the roughly equally dispersed range about this best hypothesis and find we only need to go down one and up one hypothesis. We find that the answer to the question is that about sightings on average per day occur between about 25 and 32 per day with 99% plausiblity. How do we get this calculation? The sum of the shaded \\(Pr(h \\mid d)\\) equals about 0.9897, close enough to the required level. We can also use the ogive, cumulative relative frequency of \\(Pr( h&lt; (\\lambda = 32.20) \\mid d)= 0.9916\\) and subtract \\(Pr( h&lt; (\\lambda = 22.60) \\mid d)= 0.0019\\) to get the same answer, 0.9897. We notice we have to step down one hypothesis to \\(\\lambda=22.60\\)$ to subtract the correct accumulated probability. We can also use an even finer grid to get a more precise answer, should our analysis, that is, the consumer of the analysis, require this extra time and expense. 5.7 References and endnotes Further information and more examples using the Poisson distribution can be found here. More detail with examples for outliers including Tukeys 1.5 outlier fences can be found here. "],["algorithmics-4-gaussian-blues.html", "Chapter 6 Algorithmics 4: Gaussian blues 6.1 While we wait for the other shoe to drop 6.2 Is there more to life than just counting? 6.3 Gauss, Gauss, where is Gauss? 6.4 Assume and simplify 6.5 How do we get there? 6.6 Grid lock 6.7 Onward we march 6.8 What does it all mean? 6.9 A provisional finding 6.10 Just one more thing 6.11 References and endnotes", " Chapter 6 Algorithmics 4: Gaussian blues 6.1 While we wait for the other shoe to drop Before we wade into yet another model and probability distribution to collide with data we will wander into a short aside, but definitely not a tangent. (???) deploy the parable of the Golem from Jewish folklore to talk about the mindless, though very animated and very useful role of models in science. Another theorist, (???) uses the Josef apeks native Czech word roboti to describe organic machines that end up annihilating humankind in his brother Karels play R.U.R (Rossums Universal Machines) in 1921. I like this depiction since the word and the play intersect with R. (???) and his influential frequentist hijacking in 1925 of the inverse probability approach known to Gauss, Poisson, and even Bernoulli. So are we building robotic golems? After all we are just building machines, some toy, some production, all not thinking, but sometimes acting like they are in our reverie that fantasizes a reification. Whatever the golem or robot does it is because we tell it to do so, and it will inexoribly and logically. Bernoullis binomial distribution, Poissons integer count distribution, and Gausss so-called normal, bell-shaped, distribution are such robots. They will do what we tell them to, even if we do not realize what it is exactly what we instructed them to do. But often we will interpret a robots abstractions as a real thing, which it emphatically is not. Thus it is with deep learning and AI, context-free grammars, digital journalism, algorithmic stock trading, driver-less cars, advertisments just for us. 6.2 Is there more to life than just counting? Yes indeed there is! In factor if we sub-divide integer counts of rocket launches, raptor sitings, hydro-dam failures into those murky areas of whats between 1 and 2 or 32 and 33, we arrive at continuous data. Is there an observational model we can deploy to guide our thinking about the plausibility of seeing a datum? Lets recall why we discuss any of these issues at all. Suppose we are trying to make wage policies, or perhaps, write a contract with employees in wages, among other issues. It is a known-unknown. We conduct a discussion with a few of our closest franalysts (friends who happen to be analysts). Here is the first set of marks we make on a board. POP is short for population. We have a population of wages out in the clouding of unknowing. It is a known-unknown with lots of experience with wages to back up the knowing. We take some samples from this cloud of unknowing about the known-unknown patterns of wages. Only five samples fit on the board. Someone comes up with the bright idea of depicting a range of wages from smallest to largest on the horizontal axis and the index number of each sample on the vertical axis. In this 2 dimensional field we plot the range of wages for each sample. We note some ideas like these are known but realize we most likely have not observed lots of other wage possibilities. Then someone mentions the scary word unknown. In any case the klatch of franalysts (we really have to find a better name!) decide to enumerate the arithmetic averages and simple range of wages for each of the five samples. This rather tidy plot and table summarizes the product of the brain storming session. The friendly, but often boisterous, and sometimes opinionated, analysts realize they need an observational model to help them select which of the five combinations of location (average) and scale (range) is the most plausible. They also realize that the wage data itself is not binary and is not integer valued. The analysts then realize they cant use the binomial or Poisson observational models. What are they to do? Someone, then all of them, had the insight to use all of the data at their disposal. They reasoned there are many possible ways in which location and scale can combine to represent the population whence they drew the samples. They decide to use both the, as yet unknown, mean and standard deviation of wages as their approach to the problem of making as objective a statement they can about the population of wages. They need an observational of the joint probability of observing wages and conjecturing mean and standard deviation of wages. 6.3 Gauss, Gauss, where is Gauss? Gauss did not have to really invent the normal distribution. We observe much physical, chemical, biological, psychological, social, economic, even financial behavior that appears on first glance to be Gaussian. Here are three examples from the business domain. Full-time equivalent employees Compound revenue growth Continuous stock returns and rates 6.3.1 Full time equivalent The Full time equivalent (FTE) measure helps us understand intensity of staffing against salary and benefits, among other issues. The calculation can be as simple as adding up all the employees, whether full or part-time, say a head count of 5 people, and the hours worked in a week by the 5 people, say 120 hours. If we assume a 40 hour week, then the full time equivalent number of employees per week in a 40 hour week would be 120 hours divided by a 40 hours/full-time work week to equal an FTE of 3. Is this Gaussian? Suppose we sum up a weeks worth of hours per day. Each days hours is again uniformly distributed, this time from 100 to 200 hours summed over a 5 day period. This is not the same necessarily as multiplying one days simulation times 5, or is it? hours &lt;- replicate( 10000, sum(runif(1, 15, 30) + runif(1, 15, 30) + runif(1, 15, 30) + runif(1, 15, 30) + runif(1, 15, 30))) summary(hours) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 80 106 112 112 119 145 fte &lt;- hours / 40 dens( fte , norm.comp=TRUE ) The average FTE is 2.75 people equivalent in this very symmetrical density. Again, why? Whatever the average value of the source distribution, here 113 hours, each sample from the source will be a deviation from this average. The summary shows a nearly equal quartile and minimum or maximum set of fluctuations from the mean. Adding up the deviations from the mean always equals zero algebraicly if we use the arithmetic mean. The range of the uniformly distributed hours is 75 hours per week while the simulations range is smaller at about 65 hours. The deviations are whittled down as they sum up. Where did they go? They began to offset one another. Large deviations offset large negative ones. The more terms the more ways these large movements offset one another one for one or in sums of smaller deviations that add up to the same large movements. The mostly ways to realize sums are those that aggregate around the mean. 6.3.2 Compound growth Suppose we have 1 quarter of growth in revenue. The quaterly rate of growth takes a $1 of revenue at the beginning of month 1 of the quarter, which grows at the first months rate \\(g_1\\) into \\(1+g_1\\). This amount grows into \\((1+g_1) + (1+g_1)g_2 = (1+g_1)(1+g_2)\\). This end of second month accumulated growth becomes \\((1+g_1)(1+g_2) + (1+g_1)(1+g_2)g_3 = (1+g_1)(1+g_2)(1+g_3)\\) by the end of the quarter and month 3. Is this Gaussian? Lets suppose that growth rates are uniformly distributed from -0.1 to 0.1. Then one path for quarterly (1 plus) might be \\((1+0.08)(1+(-0.02))(1+0.05)=\\) 1.11. Simulating growth 10000 times and viewing our work in the density plot shows the approximation to a theoretical normal distribution. growth &lt;- replicate( 10000 , prod( 1 + runif(12,-0.1,0.1) ) ) dens( growth , norm.comp=TRUE ) Not bad, but look at those tails! Yes, if we are willing to accept no wild swings on average in growth, then this model might help us understand patterns in revenue growth. Why Gaussian? It appears that growth will progress and decline, sometimes adding a lot or deducting a lot, but the cumulative impact will tail off as the month to month changes may even wipe each other out. As long as growth changes impact each successive month in ever smaller ways relative to the accumulative of previous months, then the impact will be Gaussian distributed. Isnt multiplication a shill for summation? 6.3.3 Log products  are 2x4 pine boards. Yes, and no. Our log products again turn into sums. Lets see how. The logarithm can be defined in terms of growth. \\[ g = log(e^g) \\] The role and function of a logarithms to a base \\(b\\) is just to return the exponent \\(x\\) of the exponential \\(b^x\\). Also since \\[ e^{g_1}e^{g_2}e^{g_3} = e^{g_1+g_2+g_3} \\] Then \\[ g_1+g_2+g_3 = log(e^{g_1+g_2+g_3})=log(e^{g_1} e^{g_2} e^{g_3}) \\] Products become sums with logarithms. And so it is with compound growth. \\[ log[(1+g_1)(1+g_2)(1+g_3)] = log(1+g_1)+log(1+g_2)+log(1+g_3) \\] Just another sum and also Gaussian? Lets see. growth &lt;- replicate( 10000 , log(prod( 1 + runif(12,-0.1,0.1)) ) ) dens( growth , norm.comp=TRUE ) Again large deviations from the mean of 0 will offset large negative deviations. Sums of small large deviations will offset large negative deviations and vice-versa. Whats left is an accumulation of the rest of the deviations. These become the most likely deviations large negatives with small positives and large positives with small negatives. If our data looks like a process in which large deviations offset small deviations in positive (greater than the mean) and negative (less than the mean), then a Gaussian distribution might be useful to approximate the behavior of the process. This model of log growth rates, and discounts, is the mathematical description of the percentage change of continuously compounded stock prices, revenue, cost, any financial statement item. 6.4 Assume and simplify We will build our first Gaussian model here and now. Our first simplifying assumption is Samples of data are independent of one another, and identically distributed too, also known as IID We take a deep breath and sigh, that cant be true! It is not, for it is true only in our mind and in the programming of our Gaussian robot. IID does not really exist, it is an artificial minds version of a can-opener that allows us to sort systematic and regularly occuring events from unsystematic movements in various outcomes. If systematic, then we have the job of describing how unsystematic deviations from the systematic might occur, how large they might be, how frequently they might occur, how lop-sided they make the distribution of our dreams. That is the character of the data we imagine and program into our robot. Our second assumption is that If all you know is the mean and standard deviation, use Gaussian. Less of an assumption than an implication of how we know or dont know about anything. Thats epistemology for you! This branch of philosophy asks a profound question, what is knowledge? The answer is well beyond what we are considering here, but which we use anyway, A variate will depend on another variate(s) only in a straight, so-called linear, way. From a distance, a twenty-one sided polygon, each side of which is a straight line, can look almost like a circle. Its an approximation, and if its close enough, it may work for our purposes. We usually start with intercept and slope. Later on in school we learned that slopes can change, and, voila a quadratic, a cubic, a whatever, is fomenting by our fervid imaginations. We trade off these shapes with the need to explain more about that dependent variable. There shalt be only one standard deviation of residuals. For otherwise we have a condition called heteroskedasticity, where residuals come from different distributions. These residuals represent the unsystematic side of the relationship and why we need to reason probablistically. This is the prominent reason why there might be a multiplicity of different Gaussian distributions hidden, latent, otherwise obfuscating the view. Different distributions always means, at least in our mind and in the programming of our robot, different behaviors. There shalt not be any meaningful relationships among explanatory variables, for otherwise we shall have confusion and be confounded. Independent sources and inputs mean clear results, usually. The contrary condition is called multicollinearity. This is another source of obfuscation that often arises in inflammatory oratory where the orator lists ten reasons why we should vote for his or her cause, and they all wind up being just one reason, and often not a good one either, and thus the bluster. 6.5 How do we get there? The friendly analysts with the wage problem know of a famous 2-parameter distribution, the Gaussian distribution. It is often referred to as the normal distribution.10 After running through the binomial and Poisson distributions, they dont really think there is anything simply normal about any distribution. They will simply call it the Gaussian in honor of Carl Friedrich Gauss who used this observational distribution to compute the time elapsed between observations of a ships position with a chronometer, effectively, longitudes and thus the minutes and seconds in a coordinate. With number of trials \\(n\\) and \\(p\\) probability of a single up movement in the binomial distribution then, the mean number of occurrences is \\(\\mu = np\\) and the standard deviation is \\(\\sqrt{np(1-p)}\\). Suppose we now perform an ever increasing number of trials \\(n\\), that is we let \\(n\\) get ever larger. In the limit as \\(n\\rightarrow \\infty\\) there is mathematics to prove that \\[ lim_{n\\rightarrow \\infty} {n \\choose{x}} p^x (1-p)^{n-x} \\approx \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{1}{2}z^2} \\] where, \\[ z = \\frac{x - \\mu}{\\sigma} \\] Whenever we see \\(\\pi\\) there is definitely a circle involved! In fact this model is very similar to the development of Ptolemys geocentric model of the movement of planets about the earth. We note well that this model worked extremely well for ancient astronomers who predicted seasons for agriculture and flood events. 6.6 Grid lock The analysts collect 12 average monthly observations of wages from the Current Population Survey of the U.S. Census Bureau. Since the wage rates (in $/hour) are averages and thus weighted sums, they believe that a reasonable observational model could be the Gaussian (normal) distribution. Here is the panel of data. We can work with the Gaussian distribution in a number of ways. As an observational model we will need to build a grid of potential hypotheses. Each hypothesis is a combination of a mean and a standard deviation. First we make a list of \\(\\mu\\)s and another list of \\(\\sigma\\)s, just like we do for \\(p\\) in the binomial model and \\(\\lambda\\) in the Poisson model of integer counts. As with equally spaced grids we calculate the width of each grid node interval using the maximum and minimum of the desired grid along with the number of intervals. We then make two lists, one for mu_h and the other for sigma_h. These are the separate conjectures for \\(\\mu\\) and \\(\\sigma\\). As always we make cells we will refer to frequently or that are otherwise important for the integrity of the calculations as named ranges. Now for the hard part. For each mu_h in the list we need to cycle through the entire list of sigma_hs, then go to the next mu_h where we cycle through the entire list of sigma_hs again. This will create a 2-dimensional grid of \\(5 \\times 5 = 25\\) \\(\\mu,\\,\\sigma\\) hypotheses. We can unpack the IF() statements in C5 and D5 by realizing that we must stay at the same mu_h only while we loop through the list of sigma_hs, otherwise move to the next mu_h. Cells C4 and D4 start the parade with the beginning entries of the mu_h and sigma_h lists. Cell C5 tests whether the previous M4 is the end of the sigma_h list by using the MAX() function. If true then the INDEX(..., MATCH()) retrieves the next mu_h, otherwise stay at the same mu_h in cell C4. At the same time, the IF() statement in M5 tests whether or not thesigma_hin D4 is the lastsigma_hin the list. If true, then go back to the beginning of thesigma_h, otherwise go to the nextsigma_h` in the list. With the 25 node grid we then proceed exactly as we did for the Poisson raptors. This time we use a different observational model, one fit for use with finding location, \\(\\mu\\) approximated by mu_h, and scale, \\(\\sigma\\) approximated by sigma_h. 6.7 Onward we march Our next stop on the magical mystery tour is the mashing together of observed data with the unobserved data of hypotheses, all 25 combinations of approximated \\(\\mu\\) and \\(\\sigma\\). These hypothetical parameters turn up in the Gaussian observational model thusly. \\[ Pr( x \\mid \\mu, \\, \\sigma) = \\frac {1}{\\sigma {\\sqrt {2\\pi }}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}} \\] Yes, it is a beast, but one that will serve our purposes. What are those purposes again? We simply want to find the probability of observing sampled wage data \\(x\\), given a conjecture about the mean \\(\\mu\\) and standard deviation \\(\\sigma\\) of the population of wages. Using this probability we seek to understand the uncertainty that surrounds a plausible choice of \\(\\mu\\) and \\(\\sigma\\). Here is our grid outfitted with the Gaussian observational model, rolled all the way through to the deduction of \\(Pr(h \\mid d) = Pr( \\mu, \\, \\sigma \\mid x)\\). We recall that we must multiply each of the \\(Pr( x_i \\mid \\mu_j,\\,\\sigma_j)\\), for each observed, sampled, wage \\(x_i\\) given just one of the hypothesized combinations of \\(\\mu_j\\) and \\(\\sigma_j\\). Here is the whole model at work. The cells which indicate the INDEX(..., MATCH()) functions inside of the IF() will be used to generate a flat, hierarchical grid. The grid in columns C and D jumps through the iterative steps of identifying \\(\\mu, \\, \\sigma\\) combinations, 25 in total. We construct a unique key in column W to help us process our analysis later. Columns E and F profess that each hypothesized \\(\\mu\\) and \\(\\sigma\\) is equally likely, an assumption we can change. Data from G4 through R4 comes directly from the data table using the TRANSPOSE() array function. Each column from G through R calculates the Gaussian distribution of the data in row 4 using the \\(\\mu, \\, \\sigma\\) combinations one row at a time. Cell M5 illustrates one of these calculations. In cell M32 we verify the calculation using the Excel NORM.DIST() function. We notice the use of holding a column constant and a row constant to build the table. There are 300 separate conditional probabilities of individual data items given \\(\\mu, \\, \\sigma\\) hypotheses. We calculate the both-and probabilities of observing all of the data given a \\(\\mu, \\, \\sigma\\) hypothesis in column S. We use the PRODUCT() function for this task in column T. Theprobability both of observing data and using the \\(\\mu, \\, \\sigma\\) hypotheses is the multiplication of the column S conditional probabilities times the probability that \\(\\mu\\) and \\(\\sigma\\) might actually represent the population from which we sampled the data. The marginal probability that the data is observed is the sum of column T in cell T3. We are not put off by the sometimes incredibly small numbers in these calculations! Because we remember that it is the overall contribution of these both-and probabilities that matters. We calculate the ratio of the probabilities in column T to the sum of probabilities in cell T3 altogether in column U. We have completed our analysis by starting with hypotheses, calculating the joint probabilities of data and hypotheses, and ultimately deducing the probabilities of \\(\\mu\\)s and \\(\\sigma\\)s given the data. 6.8 What does it all mean? Does our grid tell us anything useful? On its own it is not in a form easy to interpret. We have the raw \\(Pr(\\mu,\\,\\sigma \\mid wages)\\) in column U. We did build a key in column W above. Now is the time to put it to good use. We need to calculate the total probability of any particular \\(\\mu\\) or \\(\\sigma\\). Here is the ultimate grid that relates each hypothesized \\(\\mu\\) with each hypothesized \\(\\sigma\\). The link between them is the probability both of \\(\\mu\\) and \\(\\sigma\\), that is, \\(Pr(\\mu,\\,\\sigma \\mid wages)\\) in column U. The marginal probability of \\(Pr( \\mu = 12.75)\\) is the highest density in the I column. We calculate it realizing that this probability must take into account any of the ways in which \\(\\mu=12.75\\) interacts jointly with each of the hypothesized \\(\\sigma\\)s. The key word in the last sentence is the indefinite pronoun any. This pronoun denotes an either-or proposition: either \\(\\sigma=5\\) or 7.5 or, , 15. Either-or situatons have probabilities that add up and thus the SUM() in cell I5. Similarly the marginal probability of \\(Pr(\\sigma = 10 )\\) is the highest density for the hypthesized \\(\\sigma\\)s. This probability is also the sum of the either-or probabilities of \\(\\sigma = 10\\) interacting jointly with any of the hypothesized \\(\\mu\\)s. We often refer to this calculation as integrating out, in this case, the \\(\\mu\\)s for each \\(\\sigma\\), and vice-versa for integrating out the \\(\\sigma\\)s for each \\(\\mu\\). 6.9 A provisional finding Finally, one reasonable answer after all of this work is this. An average wage of approximately $12.75, with standard deviation of $10.00, is most consistent with the data. We must caveat this finding by noting that we used an extremely coarse grid of only 5 nodes. This underlines the approximation assumptions we deployed throughout this analysis. 6.10 Just one more thing That sort of heading is almost an example of a outlier. We ran across two in the raptor-Poisson model. There seems to be one here in the wage data, maybe. How might we detect outliers? Since we take a probabilistic view of everything as humans, seemingly always hedging our bets, we might think about an outlier as far out into the tail of a distribution of data. 6.10.1 One way One heuristic, a rule of thumb (due to (???)), for finding outliers uses quartiles of the data: The first quartile \\(Q1\\) is a data point which is \\(\\geq 1/4\\) of the data starting from the first data point. The second quartile \\(Q2\\) or the median data point which is \\(\\geq 1/2\\) of the data. The third quartile \\(Q3\\) is a data point which is \\(\\geq 3/4\\) of the data starting from the first data point. From the first and third quartile we compute a measure of the scale, or width, of the data called the interquartile range (IQR), \\(Q3  Q1\\). Tukeys rule states that outliers are values more than 1.5 times the interquartile range from the quartiles either below: \\(Q1  1.5IQR\\), or above: \\(Q3 + 1.5IQR\\). This heuristic rule can apply just as well to observed data as to unobserved hypotheses (with some computational modifications). Here is a summary of the wage data. The INDIRECT() function illustrates the generic function usage where only the named range is referenced. This feature will allow us to create lists of choices so that users may interact with the model more easily. The quartiles are enabled by the PERCENTILE(). The IQR is just the distance between the 75%ile and the 25%ile, representing the middle 50% of the data. The sample wage distribution exhibits a preponderance of positive deviations of the data from the mean, thus a large positive skew. The high kurtosis indicates high probabilities of rare events. We also know that one of the events is a greater than $45/hour wage. We calculate Tukeys fences with their 1.5 threshold number of IQRs above and below the 75%ile and 25%ile, respectively, in cells B18 and B19. We review the data and find that the $46.96/hour wage rate is well beyond the upper fences. It is a Tukey outlier. There are no lower fence outliers, just as there are no negative wage rates. 6.10.2 Or the other Another, probabilistic, approach would ask how plausible is each data point relative to the center of the distribution. This is much the same idea as Tukey who uses the middle 50% IQR as a benchmark. For example we can compute the \\(Pr( \\mu,\\,\\sigma \\mid d = 46.92)\\) distribution. This appears in a long format grid as the following. We can interpret this beast of a grid in the same as we did above. Here is the view. We have a \\(\\mu\\) by \\(\\sigma\\) table again, but set just for the pleasure of the one data point $46.92. The most plausible \\(\\mu=12.50\\) with \\(\\sigma=10\\). We notice here that the most plausible \\(\\mu_{46.92}=45\\) as close as this grid will go and meet the one data point 46.92, with the smallest \\(\\sigma=5\\) on the grid  all much in agreement with commonsense. To the right of the \\(\\mu\\) by \\(\\sigma\\) table we calculate the deviation of \\(\\mu\\) grid nodes about the most plausible \\(\\mu=12.75\\) for the whole data set. We scale these deviations by the robust Mean Absolute Deviation (MAD) metric. The result is the number of MAD deviations a proposed \\(\\mu\\) is from the highest density \\(mu=12.75\\) for the whole data set. Next to the MAD distances we calculate the cumulative probability distribution for \\(\\mu\\)s relative to the one data point 46.92. We now have a more principled and nuanced approach to distances from a maximum plausibility grid node applied to the entire data set. Taking each data point on its own, like 46.92, We see that it is \\(1.00-0.01=0.99\\) less plausible to observe an average of \\(\\mu=45\\), near where the one data point 46.92 at 4.89 MADs from the maximum plausible \\(\\mu=12.75\\), again for the whole data set. Yes, indeed we can call 46.92 an outlier if we want to define an outlier as a data point that is that implausible. Implausible or not, this outpost of a data point imparts information about the population from which it was sampled. It should therefore not be ignored. 6.11 References and endnotes Renowned and reviled, Karl Pearson started to use the term normal for the Gaussian distribution. He also discussed the existence of anti-matter with Einstein. "],["part-three-rubber-meets-the-road.html", "Part Three  Rubber meets the road", " Part Three  Rubber meets the road Samples meet the population Credibility meets confidence A death knell for the null? "],["gausss-robots-again.html", "Chapter 7 Gausss robots again 7.1 An auspicious result 7.2 Tale of two populations 7.3 Education is the key 7.4 Sample until we drop 7.5 Results, results, we want results! 7.6 Yet another rocky road we have traveled", " Chapter 7 Gausss robots again 7.1 An auspicious result The Gaussian (also known as the so-called normal distribution) is famous and important because many natural phenomena are thus distributed. Along with heights and weights, we discussed FTE, compound growth, and asset returns all distributed according to this bell shaped curve. Gaussian robots continue their merry quest to describe. Will they help us explain too? Well see. But the Gaussian distribution, is important for another reason: the distributions of means are themselves normally distributed. We demonstrated this idea in our introduction to where the Gaussian distribution can possibly come from: there is was in the calculation of Full Time Equivalent workers in an organization. The FTE calculation involved a sum of hours (randomly sampled) divided by a standard work week number of hours, the constant 40 in that case. These are then the means of hours samples. The sampling was done with the uniform distribution  nothing very informative at all. In fact we can demonstrate that the population never need be Gaussian distributed for its sampled means to be Gaussian distributed. 7.2 Tale of two populations Our claim is that we can take any distribution of outcomes, sample repeatedly (with replacement), then sum the samples and divide by the number of sampled outcomes, we will always generate a Gaussian distribution. This claim is known as the Central Limit Theorem. So heres an acid test. It is the tale of two vastly different populations we splice together. It has two modes, two scales, and two locations. One is skewed, the other is not much so. One has thick tales (do we mean tails?), the other thin. As advertised we see a two mode distribution. The population mean and standard deviation are \\(\\mu=55.58\\) and \\(\\sigma=24.69\\). If the outcomes are wage rates for two genders, one for each of the two populations, then it appears we see two very different labor markets at work. The CLT will take care of that issue! Lets remember our claim. We need to sample repeatedly from the two mode population we just conjured up. Then we calculate the sampled means, standard deviations, also the skewness and kurtosis. We can also calculate median and other quantiles. We will play with these another time. We will also experiment with multiple sample sizes from this very large population of 100,000 outcomes as well as trial runs of the simulation. ## List of 1 ## $ strip.text.y:List of 11 ## ..$ family : NULL ## ..$ face : NULL ## ..$ colour : NULL ## ..$ size : num 5 ## ..$ hjust : NULL ## ..$ vjust : NULL ## ..$ angle : NULL ## ..$ lineheight : NULL ## ..$ margin : NULL ## ..$ debug : NULL ## ..$ inherit.blank: logi FALSE ## ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;element_text&quot; &quot;element&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;theme&quot; &quot;gg&quot; ## - attr(*, &quot;complete&quot;)= logi FALSE ## - attr(*, &quot;validate&quot;)= logi TRUE What do we see? First, we need sample sizes and only a relatively few number of trials to move the sampled means away from their original distribution into the Gaussian shape. Second, if our data and hypotheses do not allow for large sample sizes and lots of repetitive sampling, then the CLT will not work its asymptotic magic. This second condition is important because very little data and even fewer experimental runs and trials are the norm for most human decision making. They do work fine in the laboratory where controlled conditions can be met and lots of repetition occur to ensure asymptotic results. 7.3 Education is the key We ask the question: Educational attainment levels continue to rise. Will earnings rise too? To begin to answer this question we sample 120 monthly average years of education along with average USD wages per hour. We then EDA them and boxplot them (two new verbs for us to consider?). ## age gender earnings education ## Min. :29.0 female:4 Min. : 9 Min. :12.0 ## 1st Qu.:29.0 male :8 1st Qu.:14 1st Qu.:12.0 ## Median :29.5 Median :14 Median :13.0 ## Mean :29.5 Mean :16 Mean :13.2 ## 3rd Qu.:30.0 3rd Qu.:17 3rd Qu.:13.0 ## Max. :30.0 Max. :35 Max. :18.0 We can hover over the boxplots of two groups in the data by gender. The hover reveals minimum, maximum, quartiles and Tukeys fences. There appear to be no female outliers in this sample of data. There is one male outlier, and it is not a lower fence wage rate. We perform the same visual analysis for the years of education. This time we note the limits of years of educational attainment for both of the identified genders. The maximum is 16 years, equivalent to 4 years of post-secondary school education for both genders. Males in this sample have an observation at the 8th grade level. To explore one step further we visualize the potential relationship between wage and education here. ## $title ## ## $subtitle ## [1] &quot;Exploring wages and education&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;labels&quot; Both wages and education are highly skewed with wages to the right and education somewhat to the left. A scatter plot in the lower left hand quadrant shows us some relationship. The correlation calculation is small, but positive. Here is a calculation table for correlation. These are the descriptive calculations for mean, standard deviation, and a new measure we will call correlation. The standard deviations are the, get ready, the square root of the average sum of squared deviations of an observation about its arithmetic mean. The arithmetic sampled mean \\(\\overline{w}\\) for a sample size of \\(n=12\\) of \\(w_i\\), for \\(i=1 \\cdots 12\\) observations, sampled from a population is \\[ \\begin{align} \\overline{w} &amp;= \\frac{\\Sigma_{i=1}^n w_i}{n} \\\\ &amp;= \\frac{192.43}{12} \\\\ &amp;= 16.04 \\end{align} \\] We pronounce \\(\\overline{w}\\) as wbar. And so we have the standard deviation as $$ \\[\\begin{align} s_{w} &amp;= \\sqrt{ \\frac{\\Sigma_{i=1}^n (w_i - \\overline{w} )^2 }{n-1} } \\\\ &amp;= \\sqrt{ \\frac{650.28}{12-1} } \\\\ &amp;= 7.69 \\end{align}\\] $$ The sampled standard deviation is \\(s_w=7.69\\) for this wage, \\(w\\), sample. Similarly we gather mean, \\(\\overline{e}=13.67\\), and standard deviation \\(s_e=2.67\\) for education observations \\(e\\). The standard deviations are arranged as the diagonals of a matrix. Then there are the diagonals of this matrix which record the relationship, maybe better, association between wage and education, and exactly the same, education and wage. The correlation \\(r_{we}\\) is a number between -1 and 1 with the interpretation that a perfect, straight-line relationship can be completely negative, -1, or completely positive, +1. There is zero in the middle which registers no relationship at all. Here is the formula, \\[ \\begin{align} r_{we} &amp;= \\frac{\\Sigma_{i=1}^n (w_i - \\overline{w} )(e_i - \\overline{e} ) / (n-1) }{s_w \\, s_e} \\\\ &amp;= \\frac{5.529}{7.89 \\times 2.67} \\\\ &amp;= 0.27 \\end{align} \\] Again we have results as previously advertised and in the pairs scatterplot from above. 7.4 Sample until we drop Lets run 10,000 samples each of size 12 from the population of potential wage and education observations. Then lets calculate the sampled mean, standard deviation of each sample and correlation between education and wage. The first thing we do is build a simple sampler using Excels =RANDBETWEEN() applied to the entire population of wages, education levels, age, and gender. The maximum and minimum indices of the population data set are used (as named ranges) in the =RANDBETWEEN() function. Visual Basic for Applications (VBA code) directs the drawing of samples, the calculation of sampled means, standard deviations for wage and correlations between wage and education. The button is a form control found ine Developer ribbon under Insert. The button references the cmdMonteCarlo_click() subroutine. This routine effectively just automates our recalculation of the spreadsheet. Each time we recalculate, say, by pressing function key F9, we deaw a new sample, calculated sampled statistics and record the result. When we click the button, the subroutine takes over function key F9 and produces 10,000 simulation runs by sampling the population 10,000 times 12 observations at a time. 7.5 Results, results, we want results! In a separate eda worksheet, we can view our handiwork in tables. Here we have summary statistics and a grid that feeds a table of intervals, midpoints, frequencies, relative frequencies, and cumulative relative frequencies. We may, under some strictures, view the relative frequencies as conditional probabilities of conjectured midpoints, here illustrated with simulated correlations, given the population of data we sampled repeatedly. For emphasis we add a Gaussian interpretation of the conjectured midpoint correlations using the simulated mean and standard deviation from the summary table. A separate worksheet sets up the plot of the frequency table. We will use an abline to visually indicate the simulated unobserved parameter with the maximum relative frequency as the \\(Pr(h \\mid d)\\), that is, the probability of conjectured, hypothesized, simulated parameter midpoint given the sampled population of data. At last here is the correlation plot. We see something of a bell-shaped body in blue dots. The ogive overshoots but otherwise tracks the simulated cumulative relative frequency. There is an odd bit of a kink around zero simulated correlation. We may need to investigate this further. Finally, the story of the simulated sampled means unfolds here. This plot and accompanying table appear in a separate results worksheet. The consumer of all of this intelligence product can choose a variable from the following list. Again the simulated sampled means tracks the Gaussian ogive, but does over shoot it. Sampling error we might say? Perhaps. But repeated runs will find that the sampled means will most consistently aggregrate about the mean of the population. The mean is not biased. Standard deviations and correlations will be another story for another day. 7.6 Yet another rocky road we have traveled We have worked hard and hope, nay, expect that some of this made some modicum of sense. Can we use these ideas to build on our models of data consistency with hypotheses? Yes, we can, and will. We will continue to plumb the depths of the ultimate geocentric model, the Gaussian distribution. Everything we have done here is combletely consistent with a Gaussian observational model. One of the many reasons this is so mathematically is that the Gaussian model, with its robotic programming, is a quadratic polynomial by nature. Such mathematics describes well any phenomenon and behavior whose deviations from the trend are symmetric and concentrate in that quadratic peak we just glimpsed around the dashed lines in the plot. "],["gausss-robots-go-rogue.html", "Chapter 8 Gausss robots go rogue 8.1 Spreadsheets? Really? 8.2 An auspicious result again? 8.3 The most uninformative distribution 8.4 Simulate until morale improves! 8.5 Is it true that Gauss is in the house again? 8.6 And again? 8.7 The Association 8.8 A tale of coir 8.9 Endnotes", " Chapter 8 Gausss robots go rogue 8.1 Spreadsheets? Really? Yes, emphatically! George Gilder says we should waste transistors (that is chips).11 Gilder makes the fairly obvious point that we must use transistors (lots of them in an integrated circuit) or go out of business. They are ubiquitous. And arrived everywhere in a very short amount of time to boot. If you do not use them you lose control of your cost structure. Anything you build will be too expensive, too heavy, too big, too slow, too lacking in quality. The same idea goes with Michael Schragge builds on Gilders ironic hyperbole about transistors and analogizes that we should waste simulations.12 If we do not so-called waste prototyping, rapid development, simulating potential problems, solutions, we will also go out of business. We must simulate until we drop! The alternative is that we will miss the one opportunity to improve or the one error that eliminates us. Of course the point he makes is that it iss not a waste, rather we should never shy away from working the problem, simulating the art of the possible. So what is the value added of a prototype, which is simply a working model? It is about information, and information is a surprise, a deviation from a trend. Schragge believes that testing a hypothesis just gets us to the point of saying we seem, in probability that is, to have a trend going on here. In the world of growth, opportunity, error and ignorance, having a trend is barely the beginning of our journey. It is the deviation from the trend that matters. Are we still talking about spreadhseets? Schragge quotes Louis Pasteur: Chance favors the prepared mind. Here the prepared mind is a product of simulations, the rapidly developed prototypes, Fleming used agar and discovered penicillin  completely unexpected! Dan Bricklin developed the spreadsheet IBMDOS/Apple IIe program Visicalc.13 As a complete surprise this product was able to be used by millions of people to rapidly simulate other products and services. Steve Jobs credited Visicalc with the success of the Apple IIe and the Macintosh in 1985. IBM credited it with the success of the PC. Now people had a reason to buy the PC. Using Visicalc we were able 40 years ago to build practical, plottable, usable option pricing models which transparently allowed us to visualize the calculations directly. Financial analysts built interactive pro forma balance sheet, income statements, and cash flow statements fed from managers expectations, scenarios, and expert knowledge of markets. These models literally paid for themselves in days, not years. The main criterion for innovation success has always been the customers payback, not the investors. How long did it take for the customer to recoup her investment? Thats the innovation criterion. The spreadsheet is a sophisticated scratchpad some have used to be a production ready system. But what is the most important message? A working prototype should be a sandbox where everyone is willing to get in and play. It has at least to be durable enough to get to the first slate of useful comments and suggestions for further improvement. Development continues! Rick Lamers recently open sourced his Grid Studio spreadsheet product with deep integration with Python. Yes, lets continue to play. 8.2 An auspicious result again? Instead of nearly 3,000 observations about wages and educational attainment, suppose we only believed that the minimum and maximum wage rate and years of education are credible. This is about the least amount of information we could possible cull from our knowledge of labor markets. If we simulate samples from these markets a lot of times (10,000 lets say) will be get our Gaussian auspicious results again? To play in this sandbox, lets think a bit about what having just maximum and minimum data means. Here is that dataset. It really does not get simpler than this! Our thought experiment, as we waste Gilder transistors over this, is to consider wages and years of educational attainment to be equally likely within the maximum and minimum intervals in our truly sparse data set. This is an example of the uniform distribution. Here is a sketch of the distribution. There are two parameters only: \\(a\\) is the minimum and \\(b\\) is the maximum of the outcomes \\(x=X\\) on the horizontal axis. The probability that any outcome occurs between \\(a\\) and \\(b\\) is just this, \\[ Pr(x \\mid a, \\, b) = \\frac{1}{b-a} \\] 8.3 The most uninformative distribution Suppose we have no idea about the shape of the distribution of our stock portfolio. We do know that the value of the portfolio can range from $10,000 to $15,000 in a 52 week period. What is mean and standard deviation you can expect in this situation? We use the uniform distribution to model our diffuse beliefs. For this problem \\(a = 2.14\\), \\(b = 97.5\\). The population mean of the uniform distribution is \\[ \\mu = \\frac{(a+b)}{2} = \\frac{2.14 + 97.5}{2} = 49.82 \\] The population standard deviation is \\[ \\sigma = \\frac{(b-a)}{\\sqrt{12}} = \\frac{(97.5 - 2.14)}{3.46} = 27.53 \\] Why? Press this button to flex your calculus-based analytical muscles! show / hide We start with the probability distribution function of the uniform distribution. \\[ \\begin{equation} f(x) = \\begin{cases} \\frac{1}{b-a}, &amp; a \\le x \\le b \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\end{equation} \\] where \\(x\\) is a number between \\(a\\) and \\(b\\), \\(a \\le x \\le b\\). This equation states simply that in an experiment, say call some brokers, there is an \\(f(x)\\) probability of getting a value \\(x\\). The \\(f(x\\) are all the same and constant. This is about as uninformative a distribution as we can imagine. What is the expected value? It is just the weighted average of \\(x\\) from \\(x=a\\) to \\(x=b\\) where the weights are all given by \\(f(x)\\). Since \\(x\\) is on the real line, we use the integral to calculate the weighted average. We should expect the answer to be the simple average of \\(a\\) and \\(b\\). \\[ \\begin{align} \\mu &amp;= \\int_a^b xf(x)dx \\\\ &amp;= \\int_a^b x \\left(\\frac{1}{b-a}\\right) dx \\\\ &amp;= \\left(\\frac{1}{b-a}\\right) \\int_a^b x\\, dx \\\\ &amp;= \\left(\\frac{1}{b-a}\\right) \\left. \\frac{x^2}{2} \\right|_{x=a}^{x=b} \\\\ &amp;= \\left(\\frac{1}{b-a}\\right)\\left( \\frac{b^2 - a^2}{2}\\right) \\\\ &amp;= \\frac{a+b}{2} \\end{align} \\] We used the division of \\(b^2-a^2\\) by \\(b-a\\) to get \\(b+a\\), for \\((b-a)(b+a)=b^2-a^2\\). The result aligns with our intuition. How about the variance and standard deviation. We use the result that the variance is \\[ E(x-\\mu)^2 = E(x^2) - \\mu^2 \\] Here the \\(E(y)\\) is the expectation of \\(y\\), that is the weighted average computed in the same way as we did for \\(\\mu\\). Intuitively this measure should most likely include the range of the distribution, that is, \\(b-a\\). In fact we can show that the standard deviation is \\[ \\sigma = \\frac{b-a}{\\sqrt{12}} \\] So it is the range scaled by \\(\\sqrt{12}\\). Where on earth did 12 come from? \\[ \\begin{align} E(x^2) &amp;= \\int_a^b x^2f(x)dx \\\\ &amp;= \\int_a^b x^2 \\left(\\frac{1}{b-a}\\right) dx \\\\ &amp;= \\left(\\frac{1}{b-a}\\right) \\int_a^b x^2\\, dx \\\\ &amp;= \\left(\\frac{1}{b-a}\\right) \\left. \\frac{x^3}{3} \\right|_{x=a}^{x=b} \\\\ &amp;= \\left(\\frac{1}{b-a}\\right)\\left( \\frac{b^3 - a^3}{3}\\right) \\\\ \\end{align} \\] We then combine this result with \\(\\mu^2\\) use our cleverness in combining and simplifying polynomials. \\[ \\begin{align} Var(x) =\\sigma^2 &amp;= E(x^2) - [E(x)]^2 \\\\ &amp;= \\left(\\frac{1}{b-a}\\right)\\left( \\frac{b^3 - a^3}{3}\\right) - \\frac{(b+a)^2}{4} \\\\ &amp;= \\left(\\frac{1}{b-a}\\right)\\left[\\frac{4b^3 - 4a^3}{12} - \\frac{3(b-a)(b+a)^2}{12}\\right] \\\\ &amp;=\\frac{(b-a)^2}{12} \\end{align} \\] 8.4 Simulate until morale improves! Do we notice a change from the previous model of last week. We compare column F for the two models. Indeed we made an error: we failed to model (and its an easy model) the sample index from 1 to 12. We missed a sample. Last week there were only 11 samples taken 10,000 times. Well, this week we adjust the model and move on. An improvement here would be to calculate and simulate education means and standard deviations. This is a good exercise and all it takes is to put the cursor into cell L3 and insert two columns using the Home &gt; Insert feature. Then do the same at cell Q3. We then fill in the relevant formulas and check the name manager to be sure the ranges are intact. Then we rerun the model 8.5 Is it true that Gauss is in the house again? It definitely appears to be so. A near 3.00 mesokurtotic relative frequency distribution with near zero skewness match with a mean that is almost exactly the median all point to a Gaussian distribution. 8.6 And again? This ones a little rugged do we think? Yes, and probably due to sampling error with square root calculationg. We recall that the standard deviation is the square root of the variance, that the variance is the average of the sum of squared deviations of wage from its arithmetic mean. But all in all it looks Gaussian for the same reasons as the mean. 8.7 The Association Correlation is not Causation True or False? In our simple model, \\(E \\rightarrow W\\) where \\(E\\), education, is represented and measured by years of education starting with first grade, and \\(W\\), is the median USD per hour wage rate. This is causation. There is an argument of antecedent probability that a highly plausible correlation supports the conjecture (here again, theory, hypothesis) that education causes wages. But by the application of the mind projection fallacy, theory is not all of reality. It can be refuted (we remember, maybe with some trepidation the logical consistency of modus tollens). In any case, we might conjecture on the conjecture that correlation is least certain of all findings. 8.8 A tale of coir Here is a round of work steps we can perform to repurpose our model with coir data to help solve a procurement question. 8.8.1 Business Situation We imagine we work for an organization which manufactures activated carbon for filtration systems. We reduce coconut fiber (coir) to carbon using a chemical process to make activated carbon. We are about to contact 12 vendors for price quotes. Here is a quickly rendered picture of the organizations supply chain from vendors \\(V\\) to customers \\(C\\). The flow of tasks ranges from vendor \\(V\\) through procurement \\(P\\), manufacturing \\(M\\), distribution \\(D\\), all driven by the customer \\(C\\). Political and international relations, regulators, and the Board of Directors \\(BoD\\) govern the management of the processes and relationships. Infrastucture supports the processes. 8.8.2 Business Questions What is a reasonable range of coir prices we might use to write a contract for coir delivery with a potential vendor? What prices would be considered too high (potential price gouging) ot too low (potential evidence of poor quality coir)? Both of these questions bear on our organizations tolerance for risk. This tolerance is buried in the Delegation of Authority (DoA) policy understanding between the organizations governing board and the managers the board hires. The DoA helps to align manager actions with organizational preferences and requirements. 8.8.3 Data We visit the coconut community trade organization site to get a range of possible coir prices. There are two series:. The price difference (basis) is due to insurance and freight charges in the distance between the two countries. Apparently the direction of trade is from west to east. Here is a graph taken from the site. It only shows recent history through 2018 unfortunately. A task will be to get more recent data. But for now this will work for us. Our manufacturing facility is in Southeast Asia, so we would be taking delivery of coir closer to Indonesia than to Sri Lanka, thus we choose the Indonesian FOB price series for analysis. We keep in mind the International Commercial Terms of trade and the definition of FOB. Here is are our findings. The map shows the direction of trade as west to east. Indonesian prices are higher than Sri Lankan prices by a range of USD 130 to 170 per MT. This is called physical price basis and is due to freight charges and the differential between the countries and their supply and demand for coir. 8.8.4 Analysis We will use the uniform distribution to model 12 vendor price quote responses. Why? Mainly because we are agnostic about the shape of the distribution of possible prices. The only thing we know is a maximum and minimum of prices. Our tolerance for error is set by corporate policy at 5%. This means that we are looking for the plausibility of prices that center around the most probable price plus or minus \\(100 \\times (1.00 - 0.05)/2 =47.5\\)%. We will call this the *high density probability interval**. We simulate 12 vendor price responses many times to generate a distribution of mean prices across the 12 vendors. This move recalls Schrages waste simulations which we will interpret as: we will not sign a contract until we exhaust the possibilities of price movements, or ourselves, whichever comes first. 8.8.5 Results What is our level of credibility regarding the Indonesian range of coir prices? What low price range is consistent with this data? Why should we be wary? What low price range is consistent with this data? Again, do we have any reason to be wary? What if we were to model the difference (basis) between Indonesia and Sri Lanka? How different are the two prices? We will come back to this question in future rounds! 8.9 Endnotes See Gilders comments here. He goes on to a further idea: build billions of 1-chip interconnected systems (our mobile phones that are really computers) and waste chips that way instead of manufacturing billion chip data centers. According to Moores law we will eventually get to a near zero-cost chip. Here is a taste of Schrages points of view. He compiled the wasting prototyping paradigm into this book a couple of decades ago. Here is a summary of his work. His innovation with Visicalc was to transform 20 hours of work into 15 minutes, almost of play at the time. Visicalc first ran on the Apple IIe. Dan is working on a web-based WikiCalc these days. "],["credible-interval-training.html", "Chapter 9 Credible interval training? 9.1 Imagine this 9.2 Try this on for size 9.3 What about the sampled standard deviation? 9.4 Probability intervals 1: known population standard deviation 9.5 Our first procedure emerges 9.6 Probability intervals 2: on to the unknown standard deviation 9.7 Our second procedure 9.8 Exercises", " Chapter 9 Credible interval training? 9.1 Imagine this Your team job is to handle the 579 current client billings in your teams book of business. You only can contact 10 clients in the short time between now and when you must estimate the range of billings for a revenue forecast for your teams managing director. Specifically, What is the expected level of billings? What is the range of billings we might expect? What is a team member to do? This time around we will try mightily to do these things. Understand the reason for estimating with confidence interval Calculate credibility / probability intervals for population proportions and means Interpret a probability interval Know the meaning of margin of error and its use Compute sample sizes for different experimental setting Know when and how to use z- and t-scores and probability intervals to estimate the population mean Compute sample sizes for estimating the population mean Yes, it seems a very tall order. Actually, all of these items are in our tool box already. Now we use them, interpret the results, mold them to the problem at hand. 9.2 Try this on for size What is a team member to do? Experiment! Thats what. Suppose there are only 10 billings. What if we take samples of 4 billings? There are \\(_{10}C_4 = 210\\) combinations of samples. We might enumerate them all. Calculate means for each sample. Calculate the mean of the sampled means. Compare with the mean of the population of the five experimental billings. Below is the distribution the random variable, sample means. Table 9.1: Billing data: 10 observations mean std_dev median skewness kurtosis 139 37.4 138 0.103 2.04 Table 9.1: Sample Means: N = 10 , n = 4 , Samples = 210 mean std_dev median skewness kurtosis 140 17.2 139 0.095 2.84 Some observations are likely in order. The sampled mean distribution is Approximately symmetric Defined on + to - infinity Almost mesokurtic Mean of sample means is equal to the population mean Standard deviation of sample means is about half of the population standard deviation That last observation is the square root of one-fourth. The others indicate a normal distribution is at work underneath all of this calculational machinery. Are there any recommendations out there? 9.3 What about the sampled standard deviation? We supposed we had a experimental population of 10 billings. We just pulled several samples of sample size 4 from this population. We just found out that the mean of the sample means is the same as the population mean. This the same as saying that the point estimate of the mean of the sample means is unbiased. The samples are all pulled from a population with population mean \\(\\mu = 138\\) and has a population standard deviation of \\(\\sigma = 35.33\\). All of this indicates that each and every draw of each of the 4 sampled billings comes from a population distributed with a \\(\\mu = 138\\) and a population standard deviation of \\(\\sigma = 35.33\\). Lets get a little more precise using the logic of algebra. We know that each sample is a draw of 4 billings \\(X = \\{X_1, X_2, X_3, X_4\\}\\), where 1, 2, 3 and 4 are simply any four indiscriminate draws from the experimental population of 10 billings. Now we take just three samples each with 4 draws. We calculate the mean of the sampled means of each sample here. \\[ \\overline{X} = \\frac{1}{3}(\\overline{X}_1+\\overline{X}_2+\\overline{X}_3) \\] We then calculate he variance (square of the standard deviation) of the sampled mean. \\[ \\sigma_{\\overline{X}}^2 = \\left( \\frac{1}{3} \\right)^2 (\\sigma_{\\overline{X}_1}^2 + \\sigma_{\\overline{X}_2}^2 + \\sigma_{\\overline{X}_3}^2) \\] We use a sleight of the algebraic hand to get this result. Here it is below in a separate section. The variance (square of the standard deviation) of the independently drawn (no intersection!) sum of the samples themselves is \\[ \\sigma_{(\\overline{X}_1+\\overline{X}_2+\\overline{X}_3)}^2 = \\sigma_{\\overline{X}_1}^2 + \\sigma_{\\overline{X}_2}^2 + \\sigma_{\\overline{X}_3}^2 \\] But \\[ \\sigma_{\\overline{X}_1}^2 + \\sigma_{\\overline{X}_2}^2 + \\sigma_{\\overline{X}_3}^2 = 3\\sigma^2$ \\] Which is three times the square of the population standard deviation. So that, \\[ \\sigma_{\\overline{X}}^2 = \\left( \\frac{1}{4} \\right)^2 4\\sigma^2 = \\frac{\\sigma^2}{4} \\] and for any sample size \\(n\\), we have the standard deviation of the sampled means as \\[ \\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}} \\] For our experiment all of this indicates that the distribution of the sample means is ` 1. Approximately normally distributed with 2. Mean = population mean \\(\\mu = 138\\), and 3. Standard deviation \\(\\sigma_{\\overline{X}} = \\frac{35.33}{\\sqrt{3}} = 20.398\\) This is very convenient result indeed, a veritable short-cut. 9.3.1 Heres the promised derivation Lets derive the above formula. We define variance: the expectation of the squared deviation of a random variable from its mean. We denote variance by ^{2}, \\(s^{2}\\) or \\(Var(X)\\). From this definition of Variance, we can write the following equation. \\[ Var(X) = E[(X - E[X])^2] \\] Since we have to find the variance of the mean of samples, lets replace the random variable X in the above equation with its mean, \\(\\overline{X}\\). \\[ Var(\\overline{X}) = E[(\\overline{X} - E[\\overline{X}])^2] \\] We know that we can calculate the arithmetic mean this way. \\[\\overline{X}= \\frac{\\sum_{i=1}^n{X_i}}{n}\\] we can expand the variance definition into this next expression. \\[ Var(\\overline{X}) = E\\left[\\left(\\frac{\\sum_{i=1}^n{X_i}}{n} - E\\left[\\frac{\\sum_{i=1}^n{X_i}}{n}\\right]\\right)^2\\right] \\] We then factor the \\(\\frac{1}{n}\\) from the above equation. \\[ \\begin{align} Var(\\overline{X}) &amp;= E\\left[\\frac{1}{n^2}\\left(\\sum_{i=1}^n{X_i} - E[\\sum_{i=1}^n{X_i}]\\right)^2\\right] \\\\ &amp;= \\frac{1}{n^2}E\\left[\\left(\\sum_{i=1}^n{X_i} - E[\\sum_{i=1}^n{X_i}]\\right)^2\\right] \\end{align} \\] We expand \\(E[\\sum_{i=1}^n{X_i}]\\) into this expression. \\[ E[{X_1} + {X_2} + {X_3} + ...... + {X_n}] \\] Expectation is just an average so that the average of a sum is the sum of the averages. \\[ E[{X_1}] + E[{X_2}] + E[{X_3}] + ...... + E[{X_n}] \\] Since all the samples in the distribution are random; also known as IID (Independent and Identically Distributed), the mean of each of them is the same. Therefore we can write the above equation as: \\[ E[\\overline{X}_1] + E[\\overline{X}_2] + ...... + E[\\overline{X}_n] = \\sum_{i=1}^nE[\\overline{X}_i] \\] We use this summation to get this result. \\[ \\begin{align} Var(\\overline{X}) &amp;= \\frac{1}{n^2}E[(\\sum_{i=1}^n{X_i} - \\sum_{i=1}^nE[\\overline{X}])^2] \\\\ &amp;= \\frac{1}{n^2}\\sum_{i=1}^nE[({X_i} - E[\\overline{X}])^2] \\\\ &amp;= \\frac{1}{n^2}[E({X_1} - E[\\overline{X}])^2 + E({X_2} - E[\\overline{X}])^2 + ...... + E({X_n} - E[\\overline{X}])^2] \\end{align} \\] We used the idea that the expectation (the average after all) of an expectation (of an average) is just the average. Math! The variance of the all the random variables} {X_1}, {X_2}{X_n} give us this progression. \\[ \\begin{align} Var(\\overline{X}) &amp;= \\frac{1}{n^2}[nE({X} - E[\\overline{X}])^2] \\\\ &amp;= \\frac{1}{n^2}(n\\sigma^2) \\\\ &amp;= \\frac{\\sigma^2}{n} \\end{align} \\] And finally we have this nugget. \\[ \\sigma_{\\overline{X}} = \\sqrt{Var(\\overline{X})} = \\frac{\\sigma}{\\sqrt{n}} \\] Now we return to our regularly sponsored program. 9.4 Probability intervals 1: known population standard deviation Our first forecasting exercise is upon us. If we know the population standard deviation we use the Normal sample means distribution to help us think about \"confidence. Out of all of the possible average billings, What is a range of expected billings such that the MD would still possibly believe that she has 95% consistency with the data? If the population standard deviation is known, then we can estimate expected billings such that \\(\\mu\\) is somewhere between a lower bound \\(\\operatorname{L}\\) and an upper bound \\(\\operatorname{U}\\) \\[ \\operatorname{L} \\leq \\mu \\leq \\operatorname{U} \\] Our beliefs will be a probabilistic calculation of the lower and upper bounds. Suppose our required level of plausiblity is 95%. We have two tails which add up to the maximum probability of error, which we will call the \\(\\alpha\\) significance level. In turn \\(alpha\\) equals one minus the confidence level, which is \\(1- \\alpha = 0.95\\). For the two tail interval, calculate \\(1 - confidence = \\alpha = 1- 0.95 = 0.05\\), so that \\((1-\\alpha) / 2\\) for the amount of alpha in each tail. Heres what we can do next. For \\(1- \\alpha = 95\\%\\) there is \\(\\alpha / 2 = 0.05/2 = 0.025\\) in each tail. The upper tail for the \\(\\alpha\\) consistency level begins at \\(1 - 0.025 = 0.975\\) cumulative probability or \\(97.5\\%\\). The lower tail for the \\(\\alpha\\) consistency level ends at \\(0.025\\) cumulative probability or \\(2.5\\%\\). 9.5 Our first procedure emerges We may have a procedure we can follow. We will base lower and upper bounds using the \\(z\\) score. Start with the \\(z\\) score and solve for the population mean \\(\\mu\\) and remembering that \\(z\\) can take on plus and minus values: \\[ z = \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\] \\[ \\mu = \\bar X \\pm z \\sigma / \\sqrt{n} \\] If the population standard deviation \\(\\sigma\\) is known then our belief about the size of the population mean \\(\\mu\\) may be represented by the normal distribution of sample means. Suppose we desire a alpha 95% consistency of our conjectures with the data about the size of the population mean. Remember that in our experiment the sample size \\(n = 3\\).Then calculate \\(\\operatorname{L} = \\overline{X} - z_{0.025}\\sigma / \\sqrt{n}\\), where \\(z_{0.025} =\\) NORM.INV(0.025,0,1) = -1.96, so that \\[ \\overline{X} - z_{0.025}\\sigma/\\sqrt{n} = 138.13 + (-1.96)(35.33 / \\sqrt{4}) = 138.13 - 34.6 = 104 \\] \\(\\operatorname{U} = \\overline{X} + z_{0.975}\\sigma/\\sqrt{n}\\), where \\(z_{0.975} =\\) NORM.INV(0.975,0,1) = 1.96, so that \\[ \\overline{X} + z_{0.975}\\sigma/\\sqrt{n} = 138.13 + (1.96)(35.33/ \\sqrt{4}) = 138.13 + 34.6 = 173 \\] Thus we have 95% consistency that the expected billings \\(\\mu\\) lie in the interval \\[ 104 \\leq \\mu \\leq 173 \\] For language and interpretation purposes, literally from day one of our investigations, we can also say that it is 95% plausible, indeed credible, to believe that the population mean lies in this interval. And that is all we can say with this model. 9.6 Probability intervals 2: on to the unknown standard deviation Lets now suppose we do not know the population standard deviation. Instead of sampling from a known distribution like a uniform distribution, we sample from a population whose standard deviation we can only guess. Now the sampled standard deviation is also a random variable, like the sampled mean before it. In practice this is nearly always the case as we usually sample behavior in the raw, without help from friendly distributions. What do we do now? We have a can-opener. It is the Students t-distribution. We use the Students t distribution to correct for consistencies that are, well, not so consistent due to the increased uncertainty introduced by a thoroughly unknown, but sampled, standard deviation. Heres a plot of the Students t overlaid with the normal distribution. What do we notice? The normal (red) distribution is more pinched in than the Students t (kurtosis? right!). Students t (blue) distribution has thicker tails than the normal distribution. Otherwise, both are symmetric. Lets check tail thickness: in Excel we can use =T.INV(2.5%,3) which returns -4.30, and where the degrees of freedom \\(df\\) of our 4 sample billings are \\(df = n - k = 4 - 1 = 3\\). We lose one degree of freedom when we calculate the sample mean. Thus for the t distribution it takes 4.30 standard deviations below the mean to hit the 2.5% level of cumulative probability. It only took 1.96 standard deviations on the normal distribution. That it took fewer standard deviations for the normal than for the t distribution to hit the 2.5% level of cumulative probability means that the t distribution is thicker tailed than the normal. 9.6.1 By the way, who is Student? There is a brewery in Dublin, Ireland, whose slogan is Guiness is Good for You. W. S. Gosset (1876-1937) was a modest, well-liked Englishman who was a brewer and agricultural statistician for the famous Guinness brewing company in Dublin. Guiness insisted that its employees keep their work secret, so he published the distribution under the pseudonym Student in 1908. This was one of the first results in modern small-sample statistics. 9.7 Our second procedure Again a procedure that follows the known population \\(\\sigma\\), but instead of using the z score and the normal (Gaussian of course) distribution, we use the thicker tailed Students t-distribution. We start by basing lower and upper bounds using the \\(t\\) score and solve for the population mean \\(\\mu\\) and remembering that \\(t\\) can take on plus and minus values: \\[ t = \\frac{\\overline{X} - \\mu}{\\hat s / \\sqrt{n}} \\] \\[ \\mu = \\overline{X} \\pm t \\hat s / \\sqrt{n} \\] If the population standard deviation \\(\\sigma\\) is not known then our belief about the size of the population mean \\(\\mu\\) may be represented by the Students t distribution of sample means. Suppose we desire a 95% level of plausibility, of consistency of the data with this observational model, the Students t-distribution about the size of the population mean. This means we have a \\((1 - 0.95)/2 = 0.025\\) \\(\\alpha\\) probability of error in consistency of the data with this model in mind. Remember that in our experiment the sample size \\(n = 4\\). Instead of the population standard deviation \\(\\sigma\\), we use the sample standard error \\(\\hat s\\), where the hat over the variable emphasizes the sampled character of this whole computation. Suppose \\(s = 37.23\\). Given an 95% consistency level and 3 degrees of freedom we have these computations. \\[ \\ell = \\overline{X} - |t_{0.025}| \\hat s / \\sqrt{n} \\] where \\(t_{0.025} =\\) T.INV(0.025,3) = -3.18, and we take the absolute value of \\(t_{0.025}\\) since the \\(\\alpha\\) significance rate 2.5% is symmetrically positioned on the t distribution in each tail. We then have \\[ \\operatorname{L} = 138.13 - (3.18)(35.33 / \\sqrt{4}) = 82.11 \\] We have at the other end of the distribution \\[ \\operatorname{U} = \\overline{X} + t_{0.975}\\hat s / sqrt{n} \\] where \\(t_{0.975} =\\) T.INV(0.975,2) = 3.18, so that \\[ \\overline{X} + t_{0.975}\\sigma / \\sqrt{n} = \\bar X + (3.18)(35.33/ \\sqrt{4}) = 194.55 \\] Thus we believe our deductions that the data are 95% consistent with expected billings \\(\\mu\\) that are somewhere in this interval: \\[ 82.11 \\leq \\mu \\leq 194.55 \\] We see a much wider an interval than if we knew the population standard deviation! 9.8 Exercises The Hiatus retail outlet takes a random sample of 25 customers from a segment population of 1,000 with a mean average transaction size of $80 normally distributed with a known population standard deviation of $20 per transaction. Find The 90% confidence interval for transaction size, and The 95% confidence interval for transaction size, and The 99% confidence interval for transaction size. What do these results indicate for management? show / hide \\(\\mu\\) is between 70 and 90 with 90% confidence, between 68 and 92 with 95% confidence, and between 65 and 95 with 99% confidence. A compensation analyst for an investment bank wants to estimate the mean hourly wages of several hundred employees in the first 5 pay bands plus or minus within plus or minus $20. Management wants a 99% confidence level for the analysis. Assume that the population standard deviation is known to be $40 and that hourly wages are normally distributed. Find the minimum sample size required for this analysis. show / hide 27 EXTRA: Find the confidence intervals for 1 if the population is not known and the sample standard deviation is $23 per transaction. Find the minimum sample size required for 2 if the population is not known and a sample standard deviation is $34. "],["hypothetically-speaking.html", "Chapter 10 Hypothetically Speaking 10.1 Imagine this 10.2 Can we be wrong? 10.3 Yet another way 10.4 Exercises", " Chapter 10 Hypothetically Speaking 10.1 Imagine this Our team is about to contract for coir. We make activated carbon for filtration systems. We have a bet going on: half the team believes that coir prices are low, the other half high. Both sides have stated what they think is a low versus a high price based on procurement experience and trades in the market for coir, FOB Indonesia. We formulate these two hypotheses. \\[ \\begin{align} H_{high}:&amp;\\,\\, \\mu=b=330.\\, with \\, \\operatorname{Pr}(H_{high}) = p \\\\ H_{low}:&amp;\\,\\, \\mu=a=230, \\, with \\, \\operatorname{Pr}(H_{low}) = 1-p \\end{align} \\] Each hypothesized mean coir price can occur with the Jakob Bernoulli jump probabilities \\(p\\) and \\(1-p\\). In previous episodes, all we did was flip the up-down machine a number of times to get at the binomial probability. This time we know the magnitudes of the jump. The standard deviation \\(\\sigma=30\\) betrays the riskiness of this market and the size of the jump as well. What we are really trying to do? Suppose we observe a price of \\(y=290\\). We ask is this a high regime price or a low one. Under \\(H_{low}\\), prices average USD 230/mt so that observed prices \\(y=Y\\) will be distributed as \\(Y \\mid H_{low}N(230, 30^2)\\). If we believe this then we have this \\(\\operatorname{Pr}(data = y \\mid hypothesis = H_{low})\\). \\[ \\operatorname{Pr}(y \\mid H_{low})= \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\left(\\frac{(y-230)^2}{2\\sigma^2}\\right)} \\] Yes, the Gaussian robot is back. We have our two observational models of coir prices. Which one is more plausible? Which one is more consistent with the observation of coir prices, really of one coir price USD 290/mt. Here is the second one model at stake when prices average a high level of $330/mt. \\[ \\operatorname{Pr}(y \\mid H_{high})= \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\left(\\frac{(y-330)^2}{2\\sigma^2}\\right)} \\] For both \\(\\sigma=30\\). Will that \\(\\sqrt{2\\pi}\\) hang around? We will see soon enough. For each hypothesized average coir price and probability of that hypothesis even occurring, it behooves us to pick the hypothesis that is most plausible. We have a both-and statement looming in the mist. We look at how plausible it is that both the event \\(y=290\\) occurs and the belief that \\(H_{low}\\) occur together. We know that this means we must multiply the two probabilities. For the joint distribution of data \\(y\\) and \\(H_{low}\\) we have this relation. \\[ \\begin{align} \\operatorname{Pr}( (y=290) \\wedge (H_{low}:\\, \\mu=230) ) &amp;= \\operatorname{Pr}(y \\mid H_{low}) \\operatorname{Pr}(H_{low}:\\, \\mu=230) \\\\ &amp;= \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{(y-230)^2}{2\\sigma^2}\\right)}\\right) \\times (1-p) \\end{align} \\] Cutting to the chase we calculate this high regime version of the both-and probability. \\[ \\begin{align} \\operatorname{Pr}( (y=290) \\wedge (H_{high}:\\, \\mu=330) ) &amp;= \\operatorname{Pr}(y \\mid H_{high}) \\operatorname{Pr}(H_{high}:\\, \\mu=230) \\\\ &amp;= \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{(y-330)^2}{2\\sigma^2}\\right)}\\right) \\times p \\end{align} \\] As ominous as all of this looks all we want to find out is if \\(H_{high}\\) is more plausible than \\(H_{low}\\) then this simple relationhip must be true. \\[ \\begin{align} \\operatorname{Pr}( (y=290) \\wedge (H_{high}:\\, \\mu=330) ) &amp;\\geq \\operatorname{Pr}( (y=290) \\wedge (H_{low}:\\, \\mu=230) ) \\\\ \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{(y-330)^2}{2\\sigma^2}\\right)}\\right) \\times p &amp;\\geq \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{(y-230)^2}{2\\sigma^2}\\right)}\\right) \\times (1-p) \\end{align} \\] As bestly as the formulas look, we do know how to calculate then! In Excel each term in the large parenthesis is just NORM.DIST(y, \\(\\mu\\), \\(\\sigma^2\\), FALSE ), where FALSE calculates the probability mass. We get this result after a short bit of time inside Excel. \\(H_{high}\\):The left-hand side is NORM.DIST(290, 330, $30^2$, FALSE )*0.5 = 0.0055 * 0.5000 = 0.0027. \\(H_{low}\\): The right-hand side is NORM.DIST(290, 230, $30^2$, FALSE )*0.5 = 0.0018 * 0.5000 = 0.0009 The high:low odds ratio is 0.0027:0009 = 3:1 in favor of the high price regime for coir prices observed at a level of $290/mt. What is the breakeven, that is even odds of 1:1, threshold price? It turns out to be in this simple case of agnostic, indifference to one or the other hypothesis, that it is the simple average of the two hypothesized means, \\((330+230)/2=280\\). Any observed price above this threshold favors a high price regime, and otherwise classifies the observed price as a low price. In the next section we take our algebraic life into our hands and deduce, using our armory of albegraic tools and vast experience, to verify our results here. After that we implement the model into an Excel worksheet and continue our quest for more knowledge. 10.1.1 For those who really want to, or even need to We can bow to the formulae and balance and reduce to a, perhaps, surprisingly simple result, algebraicly. We will also do all of this with a low price mean of \\(a\\) and a high price mean of \\(b\\). In this way we will have a general formula for any binary decision with Gaussian noise in it. We start with the question we posed in the last section. What threshold favors a high price regime? The answer we propose is the hypothesis whose odds ratio is greater than 1, alternatively whose joint probability of observed data and hypothesis is the greater. We start right off with the big beasts of burden Gaussian inequalities. \\[ \\begin{align} \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{(y-b)^2}{2\\sigma^2}\\right)}\\right) \\times p &amp;\\geq \\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\left(\\frac{(y-a)^2}{2\\sigma^2}\\right)}\\right) \\times (1-p) \\\\ exp{\\left[-\\left(\\frac{(y-b)^2}{2\\sigma^2}\\right)\\right]} &amp;\\geq exp{\\left[-\\left(\\frac{(y-a)^2}{2\\sigma^2}\\right)\\right]} \\times \\left(\\frac{1-p}{p}\\right) \\\\ exp{\\left[-\\left(\\frac{(y-b)^2}{2\\sigma^2}\\right) + \\left(\\frac{(y-a)^2}{2\\sigma^2}\\right)\\right]} &amp;\\geq \\left(\\frac{1-p}{p}\\right) \\\\ \\frac{(y-a)^2 - (y-b)^2}{2\\sigma^2} &amp;\\geq log\\left(\\frac{1-p}{p}\\right) \\end{align} \\] As is the story with anything Gaussian we end up with a quadratic term. We used these algebraic moves to get the last inequality. We also save some notional angst with \\(log_e() = log()\\) throughout, sometimes \\(ln()\\) is used as well. Multiply both sides by \\(\\sigma\\sqrt{2\\pi}\\) and anything multiplied by 1 is itself. For base \\(e\\), or any base for that matter, \\(e^{-x}/e^{-y}=e^{-x}e^{y}=e^{-x+y}\\) and the fact that \\(1/e^{z}= e^{-z}\\). Again for base \\(log_e (e^x) = x\\) and whatever we do to one side of the inequality we must do to the other (if there are no negative multiplications or divisions involved). Lets stop there! On the right is the logarithm of the odds in favor of \\(H_{low}:\\, \\mu = a\\) versus \\(H_{high}:\\, \\mu = b\\) flipping the ratio on the left-hand side. Lets not be too misled by the use of the \\(log()\\) function. After all logs are the same function (although to base 10) that we measure decibels and wonder how loud the music can go before we lose our hearing! We have seen odds ratios before. Here again to overdo the point we use the logarithm to the base \\(e=2.712\\ldots\\). The \\(log()\\) function just scores the ratios. On the left-hand side we need to reduce the numerator to this polynomial. \\[ \\begin{align} \\frac{(y-a)^2 - (y-b)^2}{2\\sigma^2} &amp;= \\frac{(y^2 - 2ay + a^2) - (y^2 - 2by + b^2)}{2\\sigma^2} \\\\ &amp;= \\frac{(2b-2a)y + (a^2 - b^2)}{2\\sigma^2} \\\\ &amp;= \\frac{2(b-a)y + (a-b)(a+b)}{2\\sigma^2} \\\\ &amp;= \\frac{2(b-a)y - (b-a)(a+b)}{2\\sigma^2} \\\\ &amp;= \\frac{2(b-a)\\left(y - \\frac{a+b}{2}\\right)}{2\\sigma^2} \\\\ &amp;= \\left(\\frac{b-a}{\\sigma^2}\\right)\\left(y - \\frac{a+b}{2}\\right) \\end{align} \\] The algebraic insight here was to realize that \\((a^2-b^2)=(a-b)(a+b)=-(b-a)(a+b)\\) a trick in much use by quadratic afficionados. The \\(b-a\\) term is just the range between the low and high hypothesized means. The \\((a+b)/2\\) term is the arithmetic average of the two hypothesized means. Doesnt this look suspiciously like some sort of uniform distribution magic? No, not magic, but the interplay of Gaussian, and the other distributions, with the uniform distribution building block. Its really like Legos(tm). We drop this result into the seeming quagmire of our derivation next where We then multiply both sides by \\(\\frac{\\sigma^2}{b-a}\\) and add \\(\\frac{a+b}{2}\\) to get much simpler result, almost a gem. \\[ \\begin{align} \\left(\\frac{b-a}{\\sigma^2}\\right)\\left(y - \\frac{a+b}{2}\\right) &amp;\\geq log\\left(\\frac{1-p}{p}\\right) \\\\ \\left(y - \\frac{a+b}{2}\\right) &amp;\\geq \\left(\\frac{\\sigma^2}{b-a}\\right) log\\left(\\frac{1-p}{p}\\right) \\\\ y &amp;\\geq \\frac{a+b}{2} + \\left(\\frac{\\sigma^2}{b-a}\\right) log\\left(\\frac{1-p}{p}\\right) \\end{align} \\] Theres that arithmetic average at the head of the line. It is followed by an important correction for the strength of our convictions about one or the other price regime. If we favor the high price regime then we subtract from the breakeven. This has the effect of curtailing the range of the low price regime while expanding the primacy of the high price regime. We illustrate this here. The table of varying \\(\\operatorname{Pr}(H_{high}=p)\\) yields the log odds ratio \\(log((1-p/p))\\) to the measured on the secondary y-axis to the right. As we assign greater plausibility to the high regime hypothesis, we also reduce the odds of the low regime hypotheses. When we reduce the low regime hypothesis below the break even \\(p=0.5\\) level, we begin to subtract from the breakeven threshold, \\(\\theta=280\\). The opposite happens when we assign greater credibility to the high regime hypothesis. To belabor the point further, here are some numbers we shouold into the threshold formula! We know that \\(b=330\\), \\(a=230\\), \\(\\sigma=30\\) and \\(p=0.5\\). That the \\(\\operatorname{Pr}(H_{high})=\\operatorname{Pr}(H_{low})\\) eliminates that potentially nasty looking, but very helpful we will see, \\(log((1-p)/p) = log(0.5/0.5) = log(1) = 0\\) term. All we are left with, in the equal \\(sigma\\) case we have here is this very simple decision rule. \\[ \\begin{align} y &amp;\\geq \\frac{a+b}{2} \\\\ &amp;\\geq \\frac{230+330}{2} \\\\ &amp;\\geq 280 \\end{align} \\] We did it and it probably did not take too many years off our lives as well. What does this inequality say to us? We choose the low price regime whenever an observed price, or an average of observed prices falls below USD 280/mt net of a correction for the dispersion of prices, \\(\\sigma\\), and taking into account experience with the occurrence of low versus high price regimes, \\(p\\). In fact, if we are agnostic, ignorant, or just do not care about whether one or the other hypothesis has ever existed in the field, then we would set \\(p=0.5\\). The logarithm of the ratio of \\(1-p\\) to \\(p\\) is then conveniently zero. We are left with \\(y \\leq 280\\). Any price greater than 280 is consistent with the high price regime centered at USD b/mt. We have thus derived our first statistical decision rule. We will therefore statistically classify any observed coir price as a high price if we observe that the price is greater than (or equal to) USD280/mt. Otherwise we classify the observed price as a low price. Done! This is our first foray into a machine learning model, the binary classification model. Yes, to learn is to infer. Here we inferred with an abundance of probability. We now venture into the deeper end of the pool by considering a numerical implementation of this model in Excel. We will ask questions like what would happen if the \\(\\sigma\\)s are different between the two classes of prices? What happens to the threshold if we vary the probabilities that the hypotheses are reasonable in the first place? What would happen if we used the Poisson or binomial or Students t distributions instead, and why? More to come with a graph or two and certainly a bunch of tables to ponder. 10.1.2 Finally an excel screenshot We can build a simple two model (two means, two standard deviation) selection criterion. Yes, you read it right, two standard deviations. In our exhausting reliving of elementary algebra memories we only used one standard deviation. We can rederive the result algebraicly for two or more means and standard deviations should we desire to. It is indeed a good exercise like solving the London Times cross-word puzzle, in ink. Here is our one standard for two models example. We first notice that the plausibility split lands on the \\(H_low\\) hypothesis, even when both hypotheses are equally plausible. In this case the observed price \\(y=270\\) is below the decision threshold \\(280 - (30^2/100)log(0.5/0.5)=280\\) so that some common sense reigns. If we were to observe \\(y=290\\) exactly the opposite occurs and we land squarely in the high price regime. The odds clearly no longer favor the low price regime. When we set the observed price to \\(y=280\\) we see this result. The odds are even. The decision maker would, statistically only speaking, be indifferent to high or low regimes. We discover an insight here: even odds (1:1) mean both regimes are equally plausible so that the decision threshold is the one value that makes the odds even. Our next experiment is to set the low price \\(\\sigma_{low}=0.15\\), a phenomenon often observed in commodity markets. Not much activity happens at low price levels due sometimes to a relative lack of arbitrage bid-ask price widths. We use our insight about even odds to find the decision threshold in this practical situation. We can use Data &gt; what if &gt; Goal Seek to find the price we would have to observe to be the decision threshold. Mathematically what is happening here is a one variable optimization. \\[ min_{y} |OR - 1.00| \\\\ such\\,\\,that \\\\ p = 0.5 \\] We choose \\(y\\) to minimize the absolute value of difference between the odds ratio \\(OR\\) and 1.00. There are several implicit constraints embedded in the odds ratio, not the least of which is the calculation of the numerator and denominator of the odds ratio. We focus on the important \\(p=0.5\\), the so-called uninformative indifference to the two hypotheses. By pressing OK we find this not so surprising result. The decision threshold is lower than the equal \\(\\sigma\\) case by over $13. It makes sense since the high price regime is spread out more than the low price regime. One more thought experiment presents itself. This time we use the low \\(\\sigma\\) threshold of \\(y=266.38\\) with equal \\(\\sigma=30\\) for each regime. This allows us to find the probability \\(p\\) of ever observing the low price regime. We minimizes the absolute difference between the odds ratio and 1.00 under the constraint that the decision threshold is 266.38. So many words that collapse into this mathematical expression. \\[ min_{p} |OR - 1.00| \\\\ such\\,\\,that \\\\ y = 266.38 \\] In this model we discover complementary roles for \\(\\sigma\\) and \\(p\\). A low value of \\(\\sigma_{low}=15\\) carries the same level of information about the low price regime hypothesis that a low value of \\(p = \\operatorname{Pr}(H_{low})=0.18\\) carries. They are equivalent measures. In this model the claim that the low price regime is less risky, less variable, less noisy than the high price regime is that same thing as saying that we find the low price regime less credible than the high price regime in general. We are indeed biased to the high price regime in our decison making. We would, on average and in expectation, even prefer the high price regime to the low price regime if \\(\\operatorname{Pr}(H)\\) has any meaning at all. Decision preferences are right at the edge of what we can expect this model to do for us. 10.2 Can we be wrong? One more concern. We might want to know what is the probability that the low regime price hypothesis is true even though we chose the high regime, for example if we observe a price, say of 290. this is the statement \\(y \\mid H_{low} &gt; 266.38\\) for the mixed standard deviation / high price preference data. This table and graph zooms into the critical intersection of the high and low price regime distributions. The tail we are concerned with is the blue-hatched nearly triangular shaped area to the right of the dotted line and below the blue low regime normal density curve. We should also note that the low and high regime densities cross each other at the threshold and where the odds ratio is exactly 1. This tail is also the probability of the high regime price given a low regime decision. It is the probability we might be wrong about choosing low versus high. Other have called this a sort of p-value, but its not quite that. We can calculate the tails cumulative probability as the area under the low price regime probability density curve in the tail beyond the threshold 266.38. This expression will compute the amount for us: 1 - NORM.DIST( 266.38, 230, 30, TRUE ) = 0.0074, where TRUE means we are to compute the cumulative probability up to 266.38. There is only a less than 1% probability that the low regime is consistent with the observation of prices above the threshold. We will probably use this threshold in our decision making. We will also use the idea that if we observe a price below the threshold that price is indeed low, with about 99% probability. The opposite red-hatched triangular tail is the area to the left of the dotted threshold line at 266.38 and, this time, underneath the high price distribution. We can calculate, and interpret this area too as the cumulative probability of seeing a price below 266.38 given a high price regime point of view: `NORM.DIST( 266.38, 330, 30, TRUE ) = 0.014, This may or may not be too high for the risk intolerant among the analysts and decision makers. Or, they may take this information into consideration in making a contract for high priced coir. 10.3 Yet another way Conventional hypothesis testing reaches back about 100 years. The goal then was to supply researchers with relatively easy to compute and apply tests to what has been called significance of a finding. There is a whole theory of knowledge, cognitional operations, logic, epistemology, and methodology behind this approach, and just a little bit of controversy over the past decade as well. Up to this point we have used what we might call a superset of techniques a subset of which is the conventional hypothesis testing, and confidence (really  plausibility) interval estimations embedded in the methodology. Both techniques are are the same when the probability of any one hypothesis is the same as any other hypothesis. The results may be interpreted differently, but in the end our supposition is that decision makers hanker for some measure of the plausiblity of a claim, a conjecture, a hypothesis. We have done this the entire course of our investigation into probabilistic reasoning. Here are the mechanics of a binary hypothesis testing method using conventional techniques without regard to explicit measure of the probability of a hypothesis. 10.3.1 Population standard deviation known As with confidence and credibility intervals, we sample repeatedly in our experiments from a population. We calculate sampled means and we know these are Gaussian, normally, distribution is a mean of the sampled means equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size. Hypotheses face off with data. There are two hypotheses in the 1928 approach by Neyman and Pearson. First there is noise only. It might be Gaussian, or Poisson generated, but it is the status quo. This is called the null hypothesis \\(H_0\\), which we believe is true. Against this hypothesis is the speculative, alternative hypothesis \\(H_A\\) or \\(H_1\\), which we are trying to refute, since it is speculative after all. The benefit of the doubt is given to the null hypothesis in this approach. Two errors are possible. Here we build more into the tails of the overlapping distributions we experienced before. Type I error, also known as a false positive is when we reject a null hypothesis when it is actually true. This is the error of accepting an alternative hypothesis (the real hypothesis of interest) when the results can be attributed to chance. We think we are observing a difference in the two hypothesis when in truth there is none, probably so. Type II error, also known as a false negative occurs when the error of not rejecting a null hypothesis when the alternative hypothesis is the true state of nature. We fail to accept an alternative hypothesis when we dont have adequate power. We are fail to observe a difference when in truth there is one. Null is true Null is not true Reject Null OK False Negative Keep Null False Positive OK Type I Error: the False Negative means the researcher did not retrieve relevant information. Type II Error: the False Positive means that the research retrieved irrelevant information. How can the company control for error? Lets be more specific. How can the company ensure that the processed coconut price is as accurate as possible? 10.3.2 Control is probability Here is what the company does: Management makes an assumption and forms a hypothesis about the average price of processed coconuts found in searches of contracts and other market documents. This is a precise statement about a specific metric. Here the metric is the average price of processed coconut per metric ton, \\(\\mu\\), Suppose this target level is USD 1000/mt. The null hypothesis (\\(H_0\\)) is that the population metric equals a target value \\(\\mu_0\\) or \\(H_0: \\mu = \\mu_0\\). Suppose that \\(H_0: \\mu = 1000\\). This is business as usual. Those who favor this hypothesis would hope that the only difference is prices is random. The alternative hypothesis (\\(H_1\\)) is that the population metric does not equal (or is just greater or less than) the target value. Thus we would have \\(H_1: \\mu \\neq 1000\\). These speculators think that prices will rise or possibly fall. Their detractors scowl at the thought. Corporate policy sets a degree of confidence in accepting as true the assumption or hypothesis about the metric. The company determines that 95% of the time \\(\\mu = 1000\\). This means there is an \\(\\alpha =\\) 5% significance that the company would be willing to be wrong about rejecting that \\(H_0: \\mu = 1000\\) is true. Under the null hypothesis it is probable that above or below a mean value of 1000 there is an error of \\(\\alpha = 0.05\\) in total, or \\(\\alpha / 2 = 0.025\\) above and \\(\\alpha / 2 = 0.025\\) below the mean. This practically means that management is willing to have only a 1 in 20 chance of being wrong about their view of the business, that is about the null hypothesis. Because management expresses the alternative hypothesis, \\(H_1: \\mu \\neq 1000\\), as not equal then this translates into a two-tailed test of the null hypothesis. What if management expressed the alternative hypothesis as \\(H_1 &gt; 1000\\)? show / hide If \\(H_1: \\mu &gt; 1000\\), then management in effect specifies a one-tailed test. This means that management believes that under the null hypotheses \\(H_0:\\, \\mu = 0\\), that the distribution of documents per day for which the null hypothesis is probably true extends from the current price all the way up to the 95%tile of occurrences of the level of prices. The region of the distribution beyond the 95%tile is 5% of the time and represents the highest range of prices. Similarly for the \\(H_1:\\,\\mu&lt;1000\\) case. 10.3.3 On to the unknown Lets suppose we do not know the population standard deviation. Now the sampled standard deviation is also a random variable, like the sampled mean. In practice this is nearly always the case. What do we do now? Use the Students t distribution to correct for small sample errors as well as the idea that sampled standard deviations are random processes themselves, just like the sampled means used to compute the sampled standard deviations. Quite a web we are weaving! Heres a plot (again) of the Students t overlaid with the normal distribution. What do we notice? Normal is more pinched in than t (kurtosis? right!) t has thicker tails than normal Lets check that: in Excel use =T.INV(2.5%,3) which returns -3.18, and where the degrees of freedom \\(df\\) of our 4 sample prices from our work in confidence intervals is \\(df = n - k = 4 - 1 = 3\\). Here \\(n\\) is the sample size of 4 randomly sampled prices and \\(k\\) is the number of estimators we are building, just one in this case \\(\\mu\\). Thus for the t distribution it takes 3.18 standard deviations below the mean to hit the 2.5% level of cumulative probability. It would only take 1.96 standard deviations on the normal distribution. There are \\(k=3\\) degrees of freedom because it only takes 3 out of the 4 sampled prices to get the fourth sampled price (we do this by using 1 estimator, the mean we calculated). That it took fewer standard deviations for the normal than for the t distribution to hit the 2.5% level of cumulative probability means that the t distribution is thicker tailed than the normal. 10.3.4 On with our story When management does not know the population standard deviation, the analyst must use the Students t distribution to correct for small sample sizes. As this is almost always the case for hypothesis testing, management has decreed that the Student-t distribution will be used for hypothesis testing. CONTINUED  management decides on regions of the distribution for acceptance that the null hypothesis is probably true and for rejection of the null hypothesis as well. This picture tells those and about 900+ more words. Suppose management can take a random sample of \\(n = 100\\) prices. An analyst then computes the sample average \\(\\overline{X} = 980\\) of prices (USD/mt, that is) with a standard deviation of \\(s = 80\\), meant to represent the very unknown population \\(\\sigma\\). They then compute the \\(t\\) score, just like the z-score for the normal distribution: \\[ t = \\frac{\\overline{X} - \\mu_0}{s / \\sqrt{n}} = \\frac{980 - 1000}{80 / \\sqrt{100}} = -2.5 \\] and compare this value with the the acceptance region of the null hypotheses \\(H_0\\). So, what is this value? For a sample size of \\(n = 100\\) and \\(k = 1\\) estimator (\\(\\overline{X}\\)), the degrees of freedom \\(df = n - k = 100 - 1\\). Under a Students t distribution with 99 \\(df\\), and using Excels =T.INV(0.025, 99), the region is bounded by t scores between \\(-1.98\\) and \\(+1.98\\). The computed t score is -2.5 and falls in the rejection region of the null hypothesis. The analyst can report that it is 95% plausible that the data rejects the null hypothesis that the market is holding steady at USD 1000/mt. Another way of reporting this might be that there is a 5% probability that the data is compatible with the null hypothesis. 10.4 Exercises An electric car manufacturer buys aluminum alloy sheets of 0.05 of an inch in thickness. Thick sheets are too heavy and thin sheets imbalance the axle loads on icy and rainy road surfaces. The purchasing officer along with a manufacturing engineer samples 100 sheets of a shipment before accepting it and calculates an average of 0.048 inches in thickness with a standard deviation of 0.01 of an inch. At a 5% level of being wrong, or in error, should the purchasing officer accept the shipment? What is the probability that the purchasing officer is wrong about rejecting the null hypothesis? A real estate developer is comparing construction wages in two markets. In New York, using a random sample of 150 workers, the average daily wage is $1,800 with a standard deviation of $500 per day. In Los Angeles, for the same skills and experience, a random sample of 125 workers yields a daily wage average of $1,700 per day with a standard deviation of $450 per day. Is there a significant difference in wage levels between the two cities at the 5% level? This is the same thing as asking if there are two distinct sub-markets at work. What is the probability of being wrong about rejecting the null hypothesis if we were to use this approach? "],["part-four-the-test-of-a-relationship.html", "Part Four  The Test of a Relationship", " Part Four  The Test of a Relationship Waiting in line  for what? Does education matter? Wealth and terrain in Africa "],["relationships-put-to-the-test.html", "Chapter 11 Relationships Put to the Test 11.1 Its not so hard to imagine this 11.2 The maths! The maths! 11.3 Does education matter? 11.4 Back to the business at hand 11.5 Does it really matter? 11.6 References and endnotes", " Chapter 11 Relationships Put to the Test 11.1 Its not so hard to imagine this There are lines everywhere! At the store, in cafeterias, waiting for buses, trains, our friends to pick us up, the line formed by the process of doing homework, baking a cake, getting an insight. All of this takes time. Waiting times exploit our sense of satisfaction and accomplishment. We desire short waiting lines especially when we do not prefer to be in the line in the first place. The opposite happens when we listen to deeply moving music, regard a dramatically poignant painting: we want the moment to last. Our team is about to assist the Colleges administration of COVID-19 vaccinations on campus. Organizers have tasked us with analyzing the results of the duration of time students, faculty, and staff stand in line waiting for a vaccination. The times have been observed in morning and afternoon shifts. A way of thinking about waiting times is this model. \\[ \\mu_{i} = \\alpha_{i} + \\beta_{i}A_i \\] where \\(\\mu_i\\) is the average waiting time in minutes at vaccination station \\(i\\), \\(\\alpha_i\\) is the average morning waiting time, \\(\\beta_i\\) is the average difference in morning and afternoon waiting times, and \\(A_i\\) is a zero/one indicator of whether we are in the afternoon shift, 1, or present ourselves to the morning shift, 0. THe model seems simple enough to understand, and possibly communicate to others. Waiting times, we hypothesize, depend upon a single factor, whether the test occurs in the morning, or in the afternoon. If \\(A_i=0\\), then we observe a waiting time average in the morning only. The average waiting time is this. \\[ \\begin{align} \\mu_{i} \\mid (A_i=0) &amp;= \\alpha_{i} + \\beta_{i}(0) \\\\ \\mu_{i} \\mid (A_i=0) &amp;= \\alpha_{i} \\end{align} \\] Only the intercept \\(\\alpha_i\\) matters in this conditional expression. Given that we observe waiting times in the afternoon, then \\(A_i = 1\\). \\[ \\begin{align} \\mu_{i} \\mid (A_i=1) &amp;= \\alpha_{i} + \\beta_{i}(1) \\\\ \\mu_{i} \\mid (A_i=1) &amp;= \\alpha_{i} + \\beta_{i} \\end{align} \\] This allows us to add the average waiting time differential for the afternoon \\(\\beta_i\\) for each station \\(i\\) to the morning average waiting time \\(\\alpha_i\\). Here is the management problem we might face. If we observe a waiting time of 5 minutes, should we label this a morning-style of waiting? By style, we now abstract from chronological notions of morning and afternoon. We release ourselves from the idea of a clock. We now have two possible regimes: one that looks like a typical chronological morning, the other that mimics an afternoon. We now focus on the problem of deciding, probabilistically speaking of course, whether a relatively high wait time mimics a morning session or an afternoon session. The implications might influence staffing, scheduling, availability of vaccines on hand, size of facility, effect of weather on waiting lines, and so on, and so forth. We will focus on a single vaccination station to work out the kinks of our model, and of our thinking. All of this allows us to specify these mutually exclusive hypotheses, at least logically so. \\[ \\begin{align} H_{PM}:&amp;\\,\\, \\mu_{PM} = \\alpha + \\beta, \\, &amp;&amp;&amp;with \\, Pr(H_{PM}) &amp;= p \\\\ H_{AM}:&amp;\\,\\, \\mu_{AM} = \\alpha, \\, &amp;&amp;&amp;with \\, Pr(H_{AM}) &amp;= 1-p \\end{align} \\] While logically mutual exclusivity exists, we can imagine that distributions of \\(\\mu_{PM}\\) and \\(\\mu_{AM}\\) might overlap in probability. The hypotheses are two classifications of a waiting time. There are two of them and thus we often use the term binary classification to describe what we are to do next. We believe also that the \\(PM\\) regime is \\(p\\) probable, so that the \\(AM\\) shift must be \\(1-p\\) probable.14 Lets insert data into this otherwise very theoretical story. Here is an Excel rendering of these hypotheses and the (Bayesian) binary classification model. The cross-over of the two distributions is fairly high up the frequency axis. The probability to the left of the 1:1 threshold 3.1188 under the AM distribution (blue) curve much larger than the probability under the red PM distribution curve. Any observations of waiting times greater than 3.1188 would most probably be AM shifts, and any less are PM timings. So ends our initial foray into conditioning a variate and building hypotheses. The condition is of the simplest kind, 0 or 1. The expected values of waiting times depend on the rules of the conditioning conveyed by an intercept and slope. The model naturally yields the two hypotheses. We now wonder what might happen if the conditioning was more like the wages \\(W\\) and educational attainment \\(E\\) model. We hypothesize that wages depend on educational level. We will use a straightline model again. \\[ \\mu_W = \\alpha + \\beta E \\] We retrieve wages \\(W\\) as normally distributed with mean \\(\\mu_W\\) and \\(\\sigma_W\\). \\[ W \\sim \\operatorname{N}(\\alpha + \\beta E ,\\, \\sigma_W) \\] We believe there is some sort of dependency, at least an association between \\(W\\) and \\(E\\). We might measure this with correlation \\(\\rho\\). We might also wonder what comes of \\(\\alpha\\) and \\(\\beta\\) in the face of \\(\\rho\\). We might as well throw in \\(\\mu_E\\) and \\(\\sigma_E\\) while we are at it. Thats our next job. 11.2 The maths! The maths! We can fuss about all we want about the maths, but they are impervious to our feelings. They remain. We can stay, or go. If we stay, and spend the time in active pursuit (just like a waiting time, waiting for insight), we might achieve a learning apogee. We suppose that we will stay awhile, for the time being. Now let us dig into our model of waiting times. Our first stop is a set of tools we will need for the excavation. In what follows we use \\(Y\\) as the wage, the metric we want to generate from its mean and standard deviation. We conjecture that \\(Y\\) depends on \\(X\\), the level of educational attainment through the conditional mean of \\(Y \\mid X\\), just like we did with vaccination waiting times. 11.2.1 What did we all expect? We define expectations as aggregations of two kinds of information. One is the information provided by an array of outcomes \\(Y_i\\) for \\(i=1 \\ldots N\\), where \\(i\\) indexes \\(N\\) outcomes. The other is the array of probabilities assigned to each outcome \\(\\pi_i = f_i/N\\), where \\(f_i\\) is the long-run frequency of occurrence of outcome \\(i\\). The one property that we must enforce is that the probabilities sum to 1. This ensures we have a complete picture of all of the probability contributions, as weights, of each outcome. The aggregation is then this expression for the expectation \\(E\\) of outcomes \\(Y\\). \\[ \\begin{align} \\operatorname{E}Y &amp;= \\sum_{i}^{N} \\left( \\frac{f_i}{N}\\right) Y_i \\\\ &amp;= \\sum_{i}^{N} \\pi_i Y_i \\end{align} \\] In this way we can say that \\(\\operatorname{E}\\) operates on \\(Y\\) where to operate means to aggregate several outcomes \\(X_i\\) into one number (or possibly function) by multiplying probability weights times outcomes and then summing the products. Thats really two operations combined into the expectation operation. And so goes the maths! Using this idea of an operator \\(\\operatorname{E}Y\\) means we define the aggregation as this expression. \\[ \\begin{align} \\operatorname{E} = \\sum_{i}^{N} \\pi_i \\times \\end{align} \\] Here are some of the algebraic rules of the road when we use this highly condensed short-hand notation. \\[ \\begin{align} Y &amp;= \\alpha\\,X \\\\ \\operatorname{E}Y &amp;= \\operatorname{E}[\\alpha\\,X] \\\\ &amp;= \\sum_{i}^{N}[\\pi_i\\,(\\alpha\\,X_i) ] \\\\ &amp;= \\pi_1\\,\\alpha\\,X_1 + \\ldots \\pi_N\\,\\alpha\\,X_N \\\\ &amp;= \\alpha \\, (\\pi_1\\,X_1 + \\ldots \\pi_N\\,X_N) \\\\ &amp;= \\alpha\\,\\sum_{i}^{N}[\\pi_i\\,(X_i) ] \\\\ &amp;= \\alpha\\,\\operatorname{E}X \\end{align} \\] This means that we can take the constant \\(\\alpha\\) outside of the expectation operator. All we did, step by step on the logical staircase, is to use the definition of the operator and then manipulate it algebraicly to deduce an equivalent expression. If \\(X_1=1, \\ldots, X_N=1\\), and the sum of probabilities \\(\\sum_{i}^N \\, \\pi_i = 1\\), then we can deduce this expression. \\[ \\begin{align} Y &amp;= \\alpha\\,X \\\\ \\operatorname{E}Y &amp;= \\operatorname{E}[\\alpha\\,X] \\\\ &amp;= \\sum_{i}^{N}[\\pi_i\\,\\dot (\\alpha\\,X_i) ] \\\\ &amp;= \\pi_1\\,\\alpha\\,(1) + \\ldots \\pi_N\\,\\alpha\\,(1) \\\\ &amp;= \\alpha \\, (\\pi_1\\,(1) + \\ldots \\pi_N\\,(1) \\\\ &amp;= \\alpha\\,\\sum_{i}^{N}[\\pi_i (1)] \\\\ &amp;= \\alpha\\,\\operatorname{E}1 \\\\ &amp;= \\alpha \\end{align} \\] This may have been immediately clear to some of us before the 7 step deduction, but we might find it reassuring that the deduction verifies, and perhaps validates, our initial conjecture. We also discover another relationship. \\[ \\operatorname{E}1 = 1 \\] In algebra we call this the identity operator. For any number or variable, or even another expectation, \\(\\alpha\\), then this is true. \\[ \\begin{align} \\alpha \\, \\operatorname{E}1 &amp;= \\alpha\\, 1 \\\\ &amp;= \\alpha \\end{align} \\] Yes, this is identity under a multiplication. Is there a zero? Yes, \\(\\operatorname{E}0 = 0\\), the identity operator under addition. Anything added to \\(\\operatorname{E}0=0\\) just returns itself. What is the expectation of a sum of variables \\(X\\) and \\(Y\\)? \\[ \\begin{align} Z &amp;= X+Y \\\\ \\operatorname{E}Z &amp;= \\operatorname{E}[X + Y] \\\\ &amp;= \\sum_{i}^{N}[\\pi_i\\,(\\,X_i + Y_i) ] \\\\ &amp;= \\pi_1\\,\\,(X_1 + Y_1) + \\ldots \\pi_N\\,(X_N+Y_N) \\\\ &amp;= (\\pi_1\\,X_1 + \\ldots \\pi_1\\,X_N) + (\\pi_N\\,Y_N + \\ldots \\pi_N\\,Y_N) \\\\ &amp;= \\sum_{i}^{N}[\\pi_i\\,(X_i) ] + \\sum_{i}^{N}[\\pi_i\\,(Y_i) ] \\\\ &amp;= \\operatorname{E}X + \\operatorname{E}Y \\end{align} \\] The expectation of a sum of outcome variables is the sum of the expectations of each variable. We just examined a sum of two variables, so it behooves us to look at the product of two variables. \\[ \\begin{align} Z &amp;= XY \\\\ \\operatorname{E}Z &amp;= \\operatorname{E}[XY] \\\\ &amp;= \\sum_{i}^{N}[\\pi_i\\,(\\,X_i\\,Y_i) ] \\\\ &amp;= \\pi_1\\,X_1 \\, Y_1 + \\ldots \\pi_N\\,X_N\\,Y_N) \\\\ &amp;= \\operatorname{E}XY \\end{align} \\] Alas, we have reduced this operation to its simplest expression already. If \\(Y=X\\), going through the same steps as above we find this out. \\[ \\begin{align} if\\,\\,Z &amp;= XY \\\\ and \\\\ Y &amp;= X \\\\ then \\\\ Z&amp;= XX\\\\ \\operatorname{E}Z &amp;= \\operatorname{E}[XX] \\\\ &amp;= \\sum_{i}^{N}[\\pi_i\\,(\\,X_i\\,X_i) ] \\\\ &amp;= \\pi_1\\,X_1 \\, X_1 + \\ldots \\pi_N\\,X_N\\,X_N) \\\\ &amp;= \\pi_1\\,X_1^2 + \\ldots \\pi_N\\,X_N^2 \\\\ &amp;= \\operatorname{E}X^2 \\end{align} \\] It turns out that we can take an expression like this, \\(Y=\\alpha + \\beta\\,X\\), multiply it by \\(X\\) and, then operate on it with \\(\\operatorname{E} = \\sum_{i}^{N} \\pi_i \\times\\) with the tools we now possess. \\[ \\begin{align} Y &amp;=\\alpha + \\beta\\,X \\\\ XY &amp;= \\alpha\\,X + \\beta\\,XX \\\\ XY &amp;= \\alpha\\,X + \\beta\\,X^2 \\\\ \\operatorname{E}XY &amp;= \\operatorname{E}[\\alpha\\,X + \\beta\\,X^2] \\\\ &amp;= \\operatorname{E}[\\alpha\\,X] + \\operatorname{E}[\\beta\\,X^2] \\\\ &amp;= \\alpha\\,\\operatorname{E}[X] + \\beta\\,\\operatorname{E}[X^2] \\end{align} \\] This will be very useful indeed. We usually will call \\(\\operatorname{E}X = \\mu_X\\) in honor of the mean of the population of all possible realizations of \\(X\\). We already know this as the weighted average of \\(X\\) outcomes, where the weights are probabilities, all of which add up to 1. What about \\(\\operatorname{E}X^2\\)? To ponder this we consider the calculation of another very familiar metric, the square of the standard deviation, which has been dubbed the variance. We start with the definition and use all of the new tricks up our sleeves. We define variance as the probability weighted average of squared deviations of outcomes from the expected outcome. We will need the remembrance of things in our algebraic past that look like this. \\[ \\begin{align} (a + b)^2 &amp;= (a + b)(a + b) \\\\ &amp;= a^2 + 2ab + b^2 \\end{align} \\] In what follows \\(a = X\\) and \\(b = -\\operatorname{E}X\\). We will also need to remember that \\(-2b^2 + b^2 = -b^2\\). \\[ \\begin{align} define \\\\ \\sigma_X^2 &amp;= Var(X) \\\\ then \\\\ Var(X) &amp;= \\operatorname{E}(X - \\operatorname{E}X)^2 \\\\ &amp;= \\operatorname{E}(X^2 - 2X\\operatorname{E}X + \\operatorname{E}X^2) \\\\ &amp;= \\operatorname{E}X^2 - \\operatorname{E}[2X\\operatorname{E}X] + \\operatorname{E}[\\operatorname{E}X^2] \\\\ &amp;= \\operatorname{E}X^2 - 2(\\operatorname{E}X)^2 + (\\operatorname{E}X)^2 \\\\ &amp;= \\operatorname{E}X^2 - (\\operatorname{E}X)^2 \\\\ &amp;= \\operatorname{E}X^2 - \\mu_X^2 \\\\ thus \\\\ \\sigma_{X}^2 &amp;= \\operatorname{E}X^2 - \\mu_X^2 \\\\ rearranging \\\\ \\operatorname{E}X^2 &amp;= \\sigma_{X}^2 + \\mu_X^2 \\end{align} \\] Lets now move on to the piece de resistance , \\(\\operatorname{E}XY\\). We start with the definition of covariance, for this is where an \\(XY\\) product resides. \\[ \\begin{align} define \\\\ \\sigma_{XY} &amp;= Cov(X, Y) \\\\ then \\\\ Cov(X, Y) &amp;= \\operatorname{E}(X - \\operatorname{E}X)(Y - \\operatorname{E}Y) \\\\ &amp;= \\operatorname{E}(XY - X\\operatorname{E}Y - Y\\operatorname{E}X + \\operatorname{E}X\\,\\operatorname{E}Y) \\\\ &amp;= \\operatorname{E}(XY - \\operatorname{E}X\\,\\operatorname{E}Y - \\operatorname{E}Y\\,\\operatorname{E}X + \\operatorname{E}X\\,\\operatorname{E}Y) \\\\ &amp;= \\operatorname{E}XY - 2\\operatorname{E}X\\,\\operatorname{E}Y + \\operatorname{E}\\,X[\\operatorname{E}Y \\\\ &amp;= \\operatorname{E}XY - \\operatorname{E}X\\,\\operatorname{E}Y \\\\ thus \\\\ \\sigma_{XY} &amp;= \\operatorname{E}XY - \\mu_X\\mu_Y \\\\ rearranging \\\\ \\operatorname{E}XY &amp;= \\sigma_{XY} + \\mu_X\\mu_Y \\end{align} \\] Now we can go to work on our model with one more stop: solving a simultaneous equation. This tool too will come in handy. We suppose we have the following two equations in \\(a\\) and \\(b\\). We will use the row-column convention of subscripts. Thus coefficient \\(c_{12}\\) will be in row 1, column 2 of a matrix. First the two equations. \\[ \\begin{align} c_{11}a + c_{12}b &amp;= d_1 \\\\ c_{21}a + c_{22}b &amp;= d_2 \\end{align} \\] In matrix form this is a very tidy arrangement like this. \\[ \\begin{align} \\begin{bmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} &amp;= \\begin{bmatrix} d_1 \\\\ d_2 \\end{bmatrix} \\\\ \\mathrm{C}\\mathrm{a} &amp;= \\mathrm{d} \\end{align} \\] Very tidy indeed! We might remember that a unique solution exists only if (or is it if and only if?) when the determinant of the matrix \\(\\mathrm{C}\\) is not zero. If it is, then the solution \\(\\mathrm{a}=\\mathrm{C}^{-1}d\\) does not exist and the model is singular. In what we will do below we will compose our coeffients of means, standard deviations and correlations. Some combinations of these aggregations, constants, will prove to yield a zero determinant, and a singular model results. The determinant \\(\\det{mathrm{C}}\\) is \\[ \\det{\\mathrm{C}} = c_{11}c_{22}-c_{12}c_{21} \\] The solution proceeds in two sweeps, one for each of \\(a\\) and \\(b\\). In the first sweep we replace the first, the \\(a\\) column, in \\(\\mathrm{C}\\) with the column vector \\(d\\). We find the determinant of this new \\(\\mathrm{C}_a\\) matrix and divide by \\(\\det{\\mathrm{C}}\\). Here we go. \\[ \\begin{align} original \\, \\, &amp;\\mathrm{C} \\\\ \\begin{bmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\end{bmatrix} \\\\ swap\\,\\, out\\,\\, &amp;first\\,\\, column \\\\ \\mathrm{C}_a &amp;= \\begin{bmatrix} d_{1} &amp; c_{12} \\\\ d_{2} &amp; c_{22} \\end{bmatrix} \\\\ then \\\\ a &amp;= \\frac{\\det{\\mathrm{C_a}}}{\\det{\\mathrm{C}}} \\\\ &amp;= \\frac{d_1c_{22}-d_2c_{12}}{c_{11}c_{22}-c_{12}c_{21}} \\end{align} \\] Now the second sweep in all its glory. \\[ \\begin{align} original \\, \\, &amp;\\mathrm{C} \\\\ \\begin{bmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\end{bmatrix} \\\\ swap\\,\\, out\\,\\, &amp;first\\,\\, column \\\\ \\mathrm{C}_b &amp;= \\begin{bmatrix} c_{11} &amp; d_{1} \\\\ c_{21} &amp; d_2 \\end{bmatrix} \\\\ then \\\\ b &amp;= \\frac{\\det{\\mathrm{C_b}}}{\\det{\\mathrm{C}}} \\\\ &amp;= \\frac{c_{11}d_2-c_{21}d_1}{c_{11}c_{22}-c_{12}c_{21}} \\end{align} \\] Very much a formula for the ages. 11.2.2 Walking the straight line Here is our model where both \\(Y\\) and \\(X\\) have some distribution with \\(\\pi\\) probabilities for each. Here we use \\(\\pi\\) as the Greek letter for \\(p\\), not as the \\(\\pi\\) of circle fame. Both \\(Y\\) and \\(X\\) are what we will very loosely call random variables, because they have outcomes with associated probabilities of occurrence. \\[ Y = \\alpha + \\beta\\, X \\] We now ask the question, what is \\(\\operatorname{E(Y \\mid X=x)=\\mu_{Y \\mid X}}\\)? What, on weighted average, can we expect \\(Y\\) to be? First of all, this must be true. \\[ \\begin{align} if \\\\ \\operatorname{E}(Y \\mid X=x) &amp;= \\mu_{Y \\mid X} \\\\ then \\\\ \\mu_{Y \\mid X} &amp;= \\operatorname{E}(\\alpha + \\beta\\, X) \\\\ &amp;= \\operatorname{E}\\alpha (1) + \\operatorname{E}(\\beta\\,X) \\\\ &amp;= \\alpha\\,\\operatorname{E}1 + \\beta\\,\\operatorname{E}X \\\\ &amp;= \\alpha\\,(1) + \\beta\\,\\mu_X \\\\ &amp;= \\alpha + \\beta\\,\\mu_X \\end{align} \\] Result one is in hand, \\(\\mu_{Y \\mid X}= \\alpha + \\beta\\,\\mu_X\\) is a true statement according to our many deductions. By the way the statement \\(\\mu_{Y \\mid X} = \\alpha\\,\\operatorname{E}1 + \\beta\\,\\operatorname{E}X\\) is an example of the distributive property of multiplication over addition. Now for our second trick we multiply \\(Y\\) by \\(X\\) to get a second result and a second true statement. We will condense \\(Y \\mid X = Y\\) to save whats left of our eyesight. We remember all of our hard work above, especially this inventory of results. \\[ \\begin{align} \\operatorname{E}Y &amp;= \\mu_{Y} \\\\ \\operatorname{E}X &amp;= \\mu_{X} \\\\ \\operatorname{E}X^2 &amp;= \\sigma_{X}^2 + \\mu_X^2 \\\\ \\operatorname{E}XY &amp;= \\sigma_{XY} + \\mu_X\\mu_Y \\end{align} \\] Using this inventory more than a few times we get these results. \\[ \\begin{align} Y &amp;= \\alpha + \\beta\\, X \\\\ then \\\\ XY &amp;= \\alpha\\,X + \\beta\\, XX \\\\ &amp;= &amp;= \\alpha\\,X + \\beta\\, X^2 \\\\ so\\,\\,that \\\\ \\operatorname{E}XY &amp;= \\operatorname{E}(\\alpha\\,X + \\beta\\, X^2) \\\\ &amp;= \\operatorname{E}\\alpha\\,X + \\operatorname{E}\\beta\\,X^2 \\\\ &amp;= \\alpha\\,\\operatorname{E}X + \\beta\\,\\operatorname{E}X^2 \\\\ &amp;= \\alpha\\,\\mu_X + \\beta\\,(\\sigma_X^2 + \\mu_X^2) \\\\ but\\,\\,we\\,\\,know\\,\\,that \\\\ \\operatorname{E}XY &amp;= \\sigma_{XY} + \\mu_X\\mu_Y \\\\ thus,\\,\\, again \\\\ \\sigma_{XY} + \\mu_X\\mu_Y &amp;= \\alpha\\,\\mu_X + \\beta\\,(\\sigma_X^2 + \\mu_X^2) \\end{align} \\] We now have two equations in two, as yet to be determined, unknowns. They are unobserved data, \\(\\alpha\\) and \\(\\beta\\). Both equations are true, and true jointly. This means we can stack one on top of the other as a simultaneous equation system and, we hope this time, solve them for unique values of \\(\\alpha\\) and \\(\\beta\\). Yes, we demand a formula! Here are the two equations with \\(\\alpha\\) and \\(\\beta\\) terms on the left-hand side and constant terms, the expectations are all constant aggregations, on the right-hand side of the equation. We also commutes the terms so that our unknowns are pre-multiplied by coefficients. \\[ \\begin{align} \\alpha + \\mu_X\\,\\beta &amp;= \\mu_Y \\\\ \\mu_X\\,\\alpha + (\\sigma_X^2 + \\mu_X^2)\\,\\beta &amp;= \\sigma_{XY} + \\mu_X\\mu_Y \\end{align} \\] The matrix representation will help us easily match coefficients with our simultaneous equation model, way above as we replicate below. \\[ \\begin{align} \\begin{bmatrix} c_{11} &amp; c_{12} \\\\ c_{21} &amp; c_{22} \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} &amp;= \\begin{bmatrix} d_1 \\\\ d_2 \\end{bmatrix} \\\\ \\mathrm{C}\\mathrm{a} &amp;= \\mathrm{d} \\end{align} \\] Our simultaneous equations of expected values for the linear model \\(Y=\\alpha+\\beta X\\) yields this structure. \\[ \\begin{align} \\alpha + \\mu_X\\,\\beta &amp;= \\mu_Y \\\\ \\mu_X\\,\\alpha + (\\sigma_X^2 + \\mu_X^2)\\,\\beta &amp;= \\sigma_{XY} + \\mu_X\\mu_Y \\\\ becomes \\\\ \\begin{bmatrix} 1 &amp; \\mu_X \\\\ \\mu_X &amp; \\sigma_X^2 + \\mu_X^2 \\end{bmatrix} \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} &amp;= \\begin{bmatrix} \\mu_Y \\\\ \\sigma_{XY} + \\mu_X\\mu_Y \\end{bmatrix} \\\\ \\mathrm{C}\\mathrm{a} &amp;= \\mathrm{d} \\end{align} \\] We can solve for the unobserved, unknown, and otherwise conjectured (we might smell a hypothesis brewing here) \\(\\alpha\\) and \\(\\beta\\) using our trusty determinant solutions. \\[ \\begin{align} \\alpha &amp;= \\frac{\\mu_Y(\\sigma_x^2 + \\mu_X^2) - \\mu_X(\\sigma_{XY} + \\mu_X\\mu_Y)}{\\sigma_X^2 + \\mu_X^2 - \\mu_x^2} \\\\ &amp;= \\frac{\\mu_Y\\sigma_X^2 - \\mu_X\\sigma_{XY}}{\\sigma_X^2} \\\\ &amp;= \\mu_Y - \\mu_X\\frac{\\sigma_{XY}}{\\sigma_X^2}\\\\ and\\,\\, then \\\\ \\beta &amp;= \\frac{\\det{\\mathrm{C_{\\beta}}}}{\\det{\\mathrm{C}}} \\\\ &amp;= \\frac{c_{11}d_2-c_{21}d_1}{c_{11}c_{22}-c_{12}c_{21}} \\\\ &amp;= \\frac{\\sigma_{XY} + \\mu_X\\mu_Y - \\mu_X\\mu_Y}{\\sigma_X^2 + \\mu_X^2 - \\mu_x^2} \\\\ &amp;= \\frac{\\sigma_{XY}}{\\sigma_X^2} \\end{align} \\] Yeow! All that work to get at this simplification all due to the wonderful result that \\(\\alpha\\) has \\(\\beta = \\sigma_{XY}/\\sigma_X^2\\) in it. \\[ \\begin{align} \\operatorname{E}(Y \\mid X) &amp;= \\alpha + \\beta\\,X \\\\ \\operatorname{E}(Y \\mid X) &amp;= \\left(\\mu_Y - \\mu_X\\frac{\\sigma_{XY}}{\\sigma_X^2}\\right) + \\frac{\\sigma_{XY}}{\\sigma_X^2}\\,X \\\\ rearranging\\,\\,terms\\\\ \\operatorname{E}(Y \\mid X) &amp;= \\mu_Y + \\frac{\\sigma_{XY}}{\\sigma_X^2}(X - \\mu_X) \\end{align} \\] The second formulation is also the basis for the vaunted Capital Asset Pricing Model in finance, where \\(Y\\) is the return on a security (stock, bond, etc.) and \\(X\\) is the return on a market index (e.g., S&amp;P 500). We have, yes, one more stop, before we drop. The definition of correlation is here. \\[ \\rho = \\frac{\\sigma_{XY}}{\\sigma_X\\,\\sigma_Y} \\] We can use this to rearrange the deck chairs on this Titanic of a beast of gnarly maths (all algebra! and barely a faint odor of calculus?). \\[ \\begin{align} if \\\\ \\rho &amp;= \\frac{\\sigma_{XY}}{\\sigma_X\\,\\sigma_Y} \\\\ then \\\\ \\sigma_{XY} &amp;= \\rho\\,\\sigma_X\\,\\sigma_Y\\\\ thus \\\\ \\beta &amp;= \\frac{\\sigma_{XY}}{\\sigma_X^2} \\\\ &amp;= \\frac{\\rho\\,\\sigma_X\\,\\sigma_Y}{\\sigma_X^2} \\\\ &amp;= \\frac{\\rho\\,\\sigma_Y}{\\sigma_X} \\end{align} \\] We need numbers, fast. But we should hold on. One more calculation to make. After we have the mean, but what about the conditional standard deviation? 11.2.3 A short variance diatribe Here we take the standard deviation as given, perhaps at our peril. The variance of waiting times is \\[ Var(Y \\mid X) = (1-\\rho^2)\\sigma_Y^2 \\] How do we get this? A lot easier than the preceding. There are no simultaneous equations to worry about. Heres the algebra. \\[ \\begin{align} Var(Y \\mid X) &amp;= \\operatorname{E}[Y - \\operatorname{E}(Y \\mid X)]^2 \\\\ &amp;= \\operatorname{E}[Y - (\\mu_Y + \\frac{\\sigma_{XY}}{\\sigma_X^2}(X - \\mu_X))]^2 \\\\ &amp;= \\operatorname{E}[(Y- \\mu_Y) + \\frac{\\sigma_{XY}}{\\sigma_X^2}(X - \\mu_X)]^2 \\\\ &amp;= \\operatorname{E}[(Y- \\mu_Y)^2 + \\frac{\\rho_{XY}^2\\sigma_Y^2}{\\sigma_X^2}(X - \\mu_X)^2 - 2\\frac{\\rho_{XY}\\sigma_Y}{\\sigma_X}(Y- \\mu_Y)(X - \\mu_X)] \\\\ &amp;= \\operatorname{E}[(Y- \\mu_Y)^2] + \\frac{\\rho_{XY}^2\\sigma_Y^2}{\\sigma_X^2}\\operatorname{E}[(X - \\mu_X)^2] - 2\\frac{\\rho_{XY}\\sigma_Y}{\\sigma_X}\\operatorname{E}[(Y- \\mu_Y)(X - \\mu_X)] \\\\ &amp;= \\sigma_Y^2 + \\frac{\\rho_{XY}^2\\sigma_Y^2}{\\sigma_X^2} \\sigma_X^2 - 2\\frac{\\rho_{XY}\\sigma_Y}{\\sigma_X}(\\rho_{XY}\\sigma_X\\,\\sigma_Y) \\\\ &amp;= \\sigma_Y^2 - \\rho_{XY}^2\\sigma_Y^2 \\\\ &amp;= (1 - \\rho_{XY}^2)\\sigma_Y^2 \\end{align} \\] Done! If the joint distribution of \\(X\\) and \\(Y\\) is Gaussian, then we can generate \\(Y \\mid X \\sim \\operatorname{N}( \\alpha + \\beta X, (1 - \\rho_{XY}^2)\\sigma_Y^2)\\). Now we can infer \\(Y\\) behavior. 11.3 Does education matter? Finally, some numbers? Suppose this is the data and an Excel trendline through the scatter plot for wages and education from way long ago. Excel calculates an intercept \\(\\alpha = -35.47\\), and slope \\(\\beta=3.834\\). Here we calculate intercepts and slopes based on the expectation of wages conditional on education level. The calculations align exactly with Excels view of the universe. 11.4 Back to the business at hand We used all of that math to understand the ins and outs of conditional expectations. The condition influences at least the expectation, it also influences the conditional standard deviation. Here we take the standard deviation as given, perhaps at our peril. The variance of waiting times is \\[ Var(Y \\mid X) = (1-\\rho^2)\\sigma_Y^2 \\] Heres the question before us: if we spend more years being educated, do we have a higher wage? We can mold thi into two hypotheses to put a point on it. How do wages compare between 18 years of schooling (4 years post-secondary education) and 16 years (secondary education)? \\[ \\begin{align} H_{college}:&amp;\\,\\, \\mu_{college} &amp;= \\alpha + \\beta (16), \\, &amp;&amp;&amp;with \\, Pr(H_{college}) &amp;= p \\\\ H_{high school}:&amp;\\,\\, \\mu_{highschool} &amp;= \\alpha + \\beta (12), \\, &amp;&amp;&amp;with \\, Pr(H_{highschool}) &amp;= 1-p \\end{align} \\] Lets compute some parameters. It is now a matter of depositing these values into our hypothesis testing model, the one we used with waiting times, modified for wages and education. We have a perfectly symmetrical solution for the equal probability, even odds, experiment. Consistent with this sample only, 16 years of education seems to work for a wage less than $18.20/hour. Any wage greater than that is consistent with 18 years of education, in this sample, and probably so. Oh, and yes education matters, at least financially, and in this sample, probably so. 11.5 Does it really matter? Here is another hypothesis test. Suppose we are still wary of all of the math and its interpretation and even the data and anecdotal, personal, experience. The skeptic says educational attainment does not matter. The critical thinker says, lets use data to help us understand whether the skeptics claims are true, probably. Here are binary hypotheses for us to consider. We whittle the skeptic down to a level of educational attainment the skeptic can live with, at least for this test. The level is \\(E=12\\) years for a yes answer. The skeptic also agrees to the same sample we used before and no means \\(\\beta=0\\). For the skeptic, and the model, this means that \\(E\\) has no impact, no relationship with wages. \\[ \\begin{align} H_{no}:\\,\\, \\mu_{no} &amp;= \\alpha, \\, &amp;&amp;with \\, Pr(H_{no}) = p \\\\ H_{yes}:\\,\\, \\mu_{yes} &amp;= \\alpha + \\beta (12), \\, &amp;&amp;with \\, Pr(H_{yes}) = 1-p \\end{align} \\] The \\(mu_{no}\\) and \\(\\mu_{yes}\\) are the results of two different ways, two different conjectures, two different models of behavior. We build no and yes into our computational model in the next round. The problem is that in this sample, the intercept \\(\\alpha &lt; 0\\). 11.6 References and endnotes We could do very well to check out this [Excel implementation of an entropic, almost Bayesian, approach to classification.[(https://pubsonline.informs.org/doi/10.1287/ited.1100.0060) What is entropy? From thermodynamic irregularities to the delivery of anaesthesia, entropy can measure chaos. It has a lot to do with the \\(log((1-p)/p)\\) portion of the 1:1 odds threshold we discussed. "],["the-journey-continues.html", "Chapter 12 The journey continues 12.1 Backing up 12.2 Fences and neighbors 12.3 Binomial raptors. 12.4 Managing relationships", " Chapter 12 The journey continues 12.1 Backing up We continue to learn. At this point we are at the end of a portion of our journey, and at the beginning of a new segment. We review all of the most important aspects of our previous work. It is but a stepping stone to future efforts. Here is both a summary and a compendium of ways in which we approach probabilistic reasoning. The last segment will answer a question we asked before: does it matter? In what follows we should try to answer the questions given our hard earned knowledge about probabilistic reasoning, and, perhaps, a peak at previous work. If we get too stymied, start beating bodily parts against immovable objects, then, we might press the button where all will be revealed. 12.2 Fences and neighbors We built two fence analyses, each with lower and upper bounds. Both use some aspect of all of the probability we have strived so hard to learn. Tukey Outliers. Here we used raw percentiles, calculated a scale, a range, from 25% to 75% and called it an Interquartile Range (IQR). Using this as our measure of how wide the data seems to be we construct upper and lower bounds. Any data beyond those bounds we label outliers. Credibility Intervals. These are really probability intervals, even more correctly, intervals within which we are probably sure that the data is compatible with our model of the data. The model of the data we bury in hypothetical data called conjectures. We parameterize those conjectures with means and standard deviations (Gaussian  so-called normal), arrival rates (Poisson), proportions (binomial). We still get upper and lower bounds. Lets use this set of visual and tabular results about coir prices, FOB Indonesia, from week 10. Even though we ran 10,000 draws from the coir price urn, we sampled only 12 prices at a time. 12.2.1 Tukeys fences. Construct Tukeys fences. What outliers can we detect in this data (if any). show solution One heuristic, a rule of thumb (see week 6 for a refresher), for finding outliers uses quartiles of the data: The first quartile \\(Q1\\) is a data point which is \\(\\geq 1/4\\) of the data starting from the first data point. The second quartile \\(Q2\\) or the median data point which is \\(\\geq 1/2\\) of the data. The third quartile \\(Q3\\) is a data point which is \\(\\geq 3/4\\) of the data starting from the first data point. From the first and third quartile we compute a measure of the scale, or width, of the data called the interquartile range (IQR), \\(Q3  Q1\\). Tukeys rule states that outliers are values more than 1.5 times the interquartile range from the quartiles either below: \\(Q1  1.5IQR = 311 - 1.5\\,(18.17) = 283.75\\), or above: \\(Q3 + 1.5\\,IQR = 329.17 + 1.5\\,(18.17) = 356.43\\). Thus we have *no outliers** in this range of coir prices \\(C\\). \\[ 283.75 \\leq C \\leq 356.43 \\] We review the data summary and find that the maximum coir price of $368.17/metric ton is well beyond the upper fence. However, the minimum coir price of $269.92/metric is well within the range of the lower fence. There is at least one Tukey outlier beyond the upper fence. There are no lower fence outliers. 12.2.2 Credibility intervals. Construct 89% credibility intervals using the information in this figure. What are the 89% lower and upper bounds for coir prices? show solution If the population standard deviation is known, then we can estimate expected billings such that \\(\\mu\\) is somewhere between a lower bound \\(\\operatorname{L}\\) and an upper bound \\(\\operatorname{U}\\). We use material from weeks 10 and 11 to solve for the interval. \\[ \\operatorname{L} \\leq \\mu \\leq \\operatorname{U} \\] Our beliefs will be a probabilistic calculation of the lower and upper bounds. Suppose our required level of plausibility is 89%. We have two tails which add up to the maximum probability of error, which we will call the \\(\\alpha\\) significance level. In turn \\(alpha\\) equals one minus the confidence level, which is \\(1- \\alpha = 0.89\\). For the two tail interval, calculate \\(1 - confidence = \\alpha = 1- 0.89 = 0.11\\), so that \\((1-\\alpha) / 2 = 0.11 / 2 = 0.055\\) for the amount of alpha in each of the two tails. We may have a procedure we can follow. We will base lower and upper bounds using the \\(z\\) score. Start with the \\(z\\) score and solve for the population mean \\(\\mu\\) and remembering that \\(z\\) can take on plus and minus values: \\[ z = \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\] \\[ \\mu = \\bar X \\pm z \\sigma / \\sqrt{n} \\] If the population standard deviation \\(\\sigma\\) is known then our belief about the size of the population mean \\(\\mu\\) may be represented by the normal distribution of sample means. Suppose we desire a alpha 95% consistency of our conjectures with the data about the size of the population mean. Remember that in our experiment the sample size \\(n = 3\\).Then calculate The tail has \\(0.055\\) area of probability up to the lower bound, so that we then can say, \\(\\operatorname{L} = \\overline{X} - z_{0.055}\\sigma / \\sqrt{n}\\), where \\(z_{0.055} =\\) NORM.INV(0.055,0,1) = -1.60 (rounded!), so that \\[ \\overline{X} - z_{0.055}\\sigma/\\sqrt{n} = 319.96 + (-1.60)(13.93 / \\sqrt{12}) = 319.96 - 6.43 = 313.53 \\] We start with \\(0.055\\) probability in the lower tail and add \\(0.89\\) of the body of the distribution to get the area of probability of \\(0.945\\) by the time we reach the upper bound. We have then \\(\\operatorname{U} = \\overline{X} + z_{0.945}\\sigma/\\sqrt{n}\\), where \\(z_{0.945} =\\) NORM.INV(0.945,0,1) = 1.60, so that \\[ \\overline{X} + z_{0.945}\\sigma/\\sqrt{n} = = 319.96 + (1.60)(13.93 / \\sqrt{12}) = 319.96 + 6.43 = 326.39 \\] Thus we have 94% consistency that the expected coir price \\(\\mu\\) lies in the interval \\[ 314 \\leq \\mu \\leq 326 \\] For language and interpretation purposes, literally from day one of our investigations, we can also say that it is 89% plausible, indeed credible, to believe that the population mean lies in this interval. And that is all we can say with this model and its results. 12.3 Binomial raptors. Raptors are particularly good indicators of environmental health because they inhabit most ecosystem types, occupy large home ranges, feed at the top of the food web, and are highly sensitive to chemical contamination and other human disturbance. They are also easy to tally when they congregate during migration. Thats why we are standing in the wind, on cloudy and clear days, nearly every day of the year here on the Heldeberg Escarpment (423921N 740109W) southwest of Albany, NY in the John Boyd Thacher State Park. We have a clear view of the confluence of the Mohawk and Hudson Rivers and their watersheds. 12.3.1 Cloudy or clear. Here is data on weather for several recent days of raptor sightings (mostly broad-winged hawks, but there are some turkey vultures out there  so we hear) during an annual migration. How much more likely is it to be cloudy if it is windy? show solution How much more likely? These are the odds. The odds are the ratio (\\(OR\\)) of two conditional probabilities. \\[ OR = \\frac{\\operatorname{Pr}(cloudy \\mid windy)}{\\operatorname{Pr}(not \\,cloudy \\mid windy)} \\] First, lets do some counting. With this small data set, we could have just as easily done this by hand. But we do have the COUNTIFS() tool at our disposal. By the way, what is the very first we do in a spreadsheet with data? The two conditional probabilities are calculated along the one cut of data, \\(windy = yes\\) a logical statement. This data fans across just two conjectures, \\(cloudy = yes\\) and \\(cloudy = no\\). Their ratio is just the odds ratio \\(OR\\) of 3:2. The answer is when it is windy is is one and a half times as likely to be cloudy than not. 12.3.2 Binomial sightings. Some one of us is standing at the edge of the escarpment. The observer looks up. What is the probability of seeing a raptor and how plausible is this claim? What is the proportion of times the observer will sight a raptor once, and what is the probability that this claim is true? Same question, put differently. Suppose the observer looks up six (6) times and sights raptors twice (2). We will use a grid of five (5) equally spaced proportions. We will assume that each conjecture is equally probably. show solution This question tests our ability to identify the right model. The event is look up and sight, a binary outcome, yes a rapter is sighted, no it is not. Binary events require the use of a binomial model. We set up 5 hypotheses about the proportion \\(p\\) of sightings \\(up\\) in this model. For each conjectured proportion \\(p\\) we deduce the probability both of the hypothesis and the binomial event of sighting 2 raptors in 5 tries. The number of tries is independent of the number of hypotheses. We build the following model. According to this approximation, the probability of a single sighting, \\(p\\), is most likely, most compatible with the binomial data of \\(n=6\\), \\(x=up=2\\), of \\(p=0.25\\). The probability of seeing this proportion conditional on the binomial data is \\(\\operatorname{Pr}(p=0.25 \\mid n=6,\\,x=2)= 0.53\\). 12.3.3 Poisson raptors. Cloudy, windy, craning the neck with binoculars into the wide horizon of the sky  but what is the average number of sightings on a given day? We observe sightings of 20, 18, 14, and 10 on four days. To answer this question we will assume a 5 node grid with minimum of 9 and maximum of 21 for hypothesized average sightings. Each conjecture is equally likely. show solution Since the number of sightings is integer date, we gravitate to the Poisson observational distribution. We do this remembering that we derived the Poisson from the count, or frequency if we want, of the number of binomial events, thus integers for observations. The probability that we observe a number of sightings \\(x\\) at an average rate \\(\\lambda\\), here per day, is with example \\(x=21\\) and \\(\\lambda=18.60\\), \\[ \\begin{align} Pr(X = x \\mid \\lambda) &amp;= e^{-\\lambda}\\left(\\frac{\\lambda^x}{x!}\\right) \\\\ &amp;= e^{-18.60}\\left(\\frac{18.60^{21}}{21!}\\right) \\\\ &amp;= 0.0747 \\end{align} \\] We drop this formula directly into the spreadsheet grid approximation. Given our assumptions and approximating grid, we match the highest level of plausibility of a conjectured \\(\\lambda\\) with its mate on the \\(\\lambda\\) grid. The answer is in hypothesis number 3 where \\(\\lambda=15\\) with probability of \\(0.59\\). 12.3.4 Poisson expectations. Given our analysis in the previous question, how many sightings might we expect in excess of the average most likely sighting? show solution We interpret the phrase in excess of the average most likely sighting as all \\(\\lambda\\)s such that \\(\\lambda &gt; 15\\). We thus have two inclusion outcomes only 18, with probability 0.28, and 21, with probability 0.03. The expected value of these two outcomes must employ normalized probabilities. The key to this answer is in column Q where we normalize the contributions of 0.29 and 0.03. Our expectations are nearly the same as the overall average of 18. So, anyone expecting more sightings, and maybe expecting more resources to manage extra sightings, might be disappointed! 12.4 Managing relationships Our sights roam to just the African continent countries bordering the Bay of Guinea. We focus on this area because they share a more common geography, oceanography, and geological evolution. They also have in common the transmigration of enslaved people across the Atlantic to the Western Hemisphere over several centuries. Here is the data. We built two models of relationships. Waiting time. But that was also coffee and bees! It could have been pre- and post-launch of IGAUNOGOHOME. It could be snowing or not snowing. It could be cloudy and windy. It is the basic model of an intervention. Does the intervention matter? Education matters. This model is the expectational version of the regression model. We try to understand how the expectation of a variable, wages, can be explained, predicted or simply how it is dependent on education. Here we use some new data about African continent country gross domestic product per capita and a measure of terrain ruggedness. We ask does terrain matter to the development of the wealth of a nation? If it does, to what extent? 12.4.1 Drawing the line What is the average impact of the ruggedness index on gross domestic product per capita? show solution Here is the model with \\(Y\\), the dependent variable gross domestic product per capita, and \\(X\\), the independent variable terrain ruggedness index. \\[ \\begin{align} \\operatorname{E}(Y \\mid X) &amp;= \\alpha + \\beta\\,X \\\\ \\operatorname{E}(Y \\mid X) &amp;= \\left(\\mu_Y - \\mu_X\\frac{\\rho\\,\\sigma_X \\, \\sigma_Y}{\\sigma_X^2}\\right) + \\frac{\\rho\\,\\sigma_X \\, \\sigma_Y}{\\sigma_X^2}\\,X \\\\ \\sigma_{Y \\mid X} &amp;= \\sqrt{1-\\rho^2}\\, \\sigma_Y \\end{align} \\] If the joint distribution of \\(X\\) and \\(Y\\) is Gaussian (yes, normal), then we can generate \\(Y \\mid X \\sim \\operatorname{N}( \\alpha + \\beta X, (1 - \\rho_{XY}^2)\\sigma_Y^2)\\). Now we can infer \\(Y\\) behavior. There is a strong enough correlation between the average ruggedness and GDP per capita to be be interested in this potential predictor. This scatterplot depicts the work in the expectational computations above. We also see that the square of correlation, \\(R^2\\), is 0.23. We interpret correlation through \\(R^2\\) as the percentage of variation in \\(G\\) explained, predicted, by a country feature, here, \\(R\\). Accodingly ruggedness probably explains about 23% of the variation in gross domestic product per capita. 12.4.2 Does it matter? Some might insist that ruggedness has nothing to do with prosperity. Maybe so. What is the uncertainty we face if we believe that ruggedness accounts, in some part, for the propsperity of a particular country? Let that country be Cameroon. What is the probability that we decide that ruggedness does influence Cameroons gross domestic product per capita, but in reality, not our mind, it really does not? show solution We need to pick out the ruggedeness, gdp per capita coordinates from the data for Cameroon. They are displayed below. A list box helps us choose any country in this data set. The odds are in favor of a dependence of gdp per capital on the terrain index for Cameroon. Also the uncertainty is smaller, 0.4028, if we choose \\(H_2:\\, \\beta \\neq 0\\), than if we choose to ignore ruggedness as an influential feature \\(H_1:\\, \\beta = 0\\), with higher uncertainty, 0.5971. "],["references.html", "References", " References "]]
